{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "Homework1_Scaffold.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manisha2297/DataAndBases/blob/master/Homework1_Scaffold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1LPcWO2eHvY"
      },
      "source": [
        "![](https://github.com/PadmajaVB/UnivAI_AI-3/blob/main/fig/univ.png?raw=1)\n",
        "\n",
        "# AI-3: Language Models\n",
        "## Homework 1: Embeddings and Language Models\n",
        "\n",
        "**AI3 Cohort 1**<br/>\n",
        "**Univ.AI**<br/>\n",
        "**Instructor**: Pavlos Protopapas<br />\n",
        "**Maximum Score**: 100\n",
        "\n",
        "<hr style=\"height:2.4pt\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h0qj0bGeHvn"
      },
      "source": [
        "#RUN THIS CELL \n",
        "import requests\n",
        "from IPython.core.display import HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JejHEQwzeHvo"
      },
      "source": [
        "# Import necessary libraries\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import dot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, SimpleRNN\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPMYtVTzeHvo"
      },
      "source": [
        "### INSTRUCTIONS\n",
        "\n",
        "\n",
        "- This homework is a jupyter notebook. Download and work on it on your local machine.\n",
        "\n",
        "- This homework should be submitted in pairs.\n",
        "\n",
        "- Ensure you and your partner together have submitted the homework only once. Multiple submissions of the same work will be penalised and will cost you 2 points.\n",
        "\n",
        "- Please restart the kernel and run the entire notebook again before you submit.\n",
        "\n",
        "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit. \n",
        "\n",
        "- To submit the homework, either one of you upload the working notebook on edStem and click the submit button on the bottom right corner.\n",
        "\n",
        "- Submit the homework well before the given deadline. Submissions after the deadline will not be graded.\n",
        "\n",
        "- We have tried to include all the libraries you may need to do the assignment in the imports statement at the top of this notebook. We strongly suggest that you use those and not others as we may not be familiar with them.\n",
        "\n",
        "- Comment your code well. This would help the graders in case there is any issue with the notebook while running. It is important to remember that the graders will not troubleshoot your code. \n",
        "\n",
        "- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**. \n",
        "\n",
        "- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print()` function that includes a reference to the calculated value, **not hardcoded**. For example: \n",
        "```\n",
        "print(f'The R^2 is {R:.4f}')\n",
        "```\n",
        "- Your plots should include clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is not a descriptive title; \"95 % confidence interval of coefficients of polynomial degree 5\" is).\n",
        "\n",
        "- **Ensure you make appropraite plots for all the questions it is applicable to, regardless of it being explicitly asked for.**\n",
        "\n",
        "<hr style=\"height:2pt\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke0XQlxXeHvp"
      },
      "source": [
        "### Names of the people who worked on this homework together\n",
        "#### Manisha R and Padmaja V Bhagwat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqEwLDl_eHvp"
      },
      "source": [
        "### **DATASET ACCESS**\n",
        "\n",
        "**Please note that all the datasets used in this homework are available to you on edStem. You will find it in the resources tab (on the top right) next to your lessons tab. Additionally, some datasets have been provided in a form that will allow you to access it directly on google colab by uncommenting and running some cells.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iPpuILbeHvp"
      },
      "source": [
        "### **HOMEWORK QUIZ**\n",
        "\n",
        "**For each part of the homework, there is an associated quiz on edStem. You are required to attempt that after completing each section of this homework. Please note that the quiz is one attempt only.**\n",
        "\n",
        "\n",
        "![](https://github.com/PadmajaVB/UnivAI_AI-3/blob/main/fig/one_attempt.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc6P9IjIeHvp"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "## **PART 1 [35 points]: Language Modelling using ngrams**\n",
        "<br />    \n",
        "\n",
        "In the first part of the homework, you are expected to build a language model based on bigrams. You will develop your own sub-word tokenization to analyze dissaster messages from multiple natural disasters dataset. All the sentences are translated into english.\n",
        "\n",
        "\n",
        "You have been tasked to develop a language model to complete messages that for some reason arrive incomplete to a radio station. Given the delicate situation, you will have to be extra careful. Each word in the sentence convey a lot of information, and improper handling of the data can mean harm to someone. \n",
        "    \n",
        "    \n",
        "</div>\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noT0TNOleHvq"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "## **PART 1: Questions**\n",
        "<br />\n",
        "\n",
        "### **1.1 [5 points] PREPROCESS THE DATASET**\n",
        "<br />\n",
        "\n",
        "**1.1.1** - Read in the dataset `disaster_response_messages_training.csv` and select only the column \"message\".\n",
        "<br /><br />\n",
        "\n",
        "**1.1.2** - Define a function `clean_data` that takes the data frame as input, converts the characters to lower case and removes any special characters that you might consider irrelevant,  adds the start token `<s>` and the end token `</s>` to every sentence (row) in the data frame and returns the processed data frame. \n",
        "<br /><br />\n",
        "\n",
        "\n",
        "**1.1.3** - Split the dataset into train and test sets. The proportion should be 0.95 and 0.05, respectively. You will create the language model based on the train set and validate your results on the test set.\n",
        "<br /><br />    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "### **1.2 [8 points] TOKENIZE AND COUNT**\n",
        "<br />\n",
        "In this section, you will create three different tokenizers that you will build LM based on. The tokenization functions must divide the text into tokens, count their frequency and return a dictionary with a mapping of token to number.\n",
        "    \n",
        "**1.2.1** - Create your own tokenization function ('tokenizer_1') based on whitespace. Set the vocabulary size to 1000, including the `<UNK>` token for out of the vocabulary (OOV) words. \n",
        "<br /><br />\n",
        "\n",
        "**1.2.2** - Create a second tokenization function ('tokenizer_2') based on whitespace, but do not limit the vocabulary size.\n",
        "<br /><br />\n",
        "\n",
        "**1.2.3** - Create a third tokenization function ('tokenizer_3') based on sub-words. You have to define a set of common sub-words in the English language, for example, the subtokens _ing_ and _n't_.\n",
        "    \n",
        "In this example, the sentence \"_It is raining outside_\" would be tokenized as [_It_, _is_, _rain_, _ing_, _outside_ ].\n",
        "<br /><br />\n",
        "    \n",
        "    \n",
        "    \n",
        "### **1.3 [6 points] CONSTRUCTING BIGRAMS**\n",
        "<br />\n",
        "\n",
        "**1.3.1** - Using each of the tokenizer functions you created, split each sentence into tokens in their numerical representation. \n",
        "<br /><br />\n",
        "\n",
        "**1.3.2** - Count the bigrams in the dataset for each tokenizer and divide them by the total number of bigrams. This will give you the probability of each bigram.\n",
        "<br /><br />\n",
        "    \n",
        "    \n",
        "    \n",
        "### **1.4 [8 points] PREDICTING THE NEXT WORDS**\n",
        "<br />\n",
        "\n",
        "**1.4.1** - Simulate the incomplete messages dividing each sentence of the **test** set into two. For this, split each sentence in a 3:1 ratio. The first $75\\%$ of a sentence will represent the correct message, and the last $25\\%$ will convey the missing information. You will not give this 25% to your model, it is kept hidden. This 25% will only be used to evaluate the predictions of your language model.\n",
        "    \n",
        "For example in the sentence: *\"I will go out on a vacation, now that my semester ended.\"*\n",
        "\n",
        "The first 75% will be *\"I will go out on a vacation, now\"*\n",
        "\n",
        "The last 25% will be *\"that my semeter ended\"*\n",
        "\n",
        "Your aim is to predict the last part by giving your model the first \"part\" of the sentence.\n",
        "\n",
        "\n",
        "Note that in an n-gram language model, only the last $n-1$ words are used to make a prediction. For example, for the above sentence, if you are using bigrams, the input to your model would only be \"now\" and you are expected to predict \"that\".\n",
        "    \n",
        "<br /><br />    \n",
        "    \n",
        "**1.4.2** - Given 5 sentences from the previous question (test set), predict the next word. \n",
        "Append this predicted word to the input sequence and predict the next one. Repeat this process until you reach the 10th token or the end of a sentence. Compare your results qualitatively with the original sentences. Do the results make sense wrt the context and semantics?\n",
        "\n",
        "Repeat this for all the models built using different tokenization techniques.\n",
        "<br /><br />\n",
        "\n",
        "**1.4.3** - Repeat the same exercise, for all 3 models, but this time, the next token will be sampled from a distribution given by the bigram frequency. Compare and comment on the results?\n",
        "\n",
        "\n",
        "*Hint:* In a model of two bigrams with frequencies 0.7 and 0.3, a deterministic prediction will only predict the first bigram. Sampling from a distribution, will enable the model to predict the second bigram with a probability of 0.3. In this way we can still predict infrequent tokens. \n",
        "<br /><br />\n",
        "    \n",
        "\n",
        "### **1.5 [5 points] EVALUATE THE LANGUAGE MODELS**\n",
        "<br />\n",
        "\n",
        "    \n",
        "**1.5.1** - For each of the models built using different tokenization techniques, compute the average perplexity in the test set (part 1.1.3). Perform smoothing on the bigram models. Based on the perplexity, which model is better?\n",
        "<br /><br />\n",
        "\n",
        "**1.5.2** - Given the perplexities, which model do you think is better? Why do you think so? Does this reflect the quality of the prediction as seen in part 1.4? \n",
        " What is the effect of UNK words?\n",
        "\n",
        "<br /><br />\n",
        "\n",
        "### **1.6 [3 points] HOMEWORK QUIZ**\n",
        "<br />\n",
        "After attempting this part of the homework, answer the questions on edStem. All the questions depend on this part of the homework and you will not be able to answer them without attempting this part.\n",
        "\n",
        "<br /><br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncCnPL2Be6Bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1018c643-16ea-46ff-f7f4-42207eb48af3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll4Mrj7feHvt"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "## **PART 1: Solutions**\n",
        "    \n",
        "### **1.1 [5 points] PREPROCESS THE DATASET**\n",
        "<br />\n",
        "\n",
        "**1.1.1** - Read in the dataset `disaster_response_messages_training.csv` and select only the column \"message\".\n",
        "<br /><br />\n",
        "\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUGkyUoTeHvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "463b4929-b73d-4665-cacd-d5b961829de8"
      },
      "source": [
        "# Your code here\n",
        "raw_data = pd.read_csv('/content/drive/MyDrive/UnivAI/Univ AI 3/Homework 1 - Part 1 Dataset.csv')\n",
        "raw_data = raw_data[['message']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZUBqYDfLOc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "99b53bf8-d768-41cd-9ca6-40a273480eec"
      },
      "source": [
        "raw_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Weather update - a cold front from Cuba that c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Is the Hurricane over or is it not over</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>says: west side of Haiti, rest of the country ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Information about the National Palace-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Storm at sacred heart of jesus</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message\n",
              "0  Weather update - a cold front from Cuba that c...\n",
              "1            Is the Hurricane over or is it not over\n",
              "2  says: west side of Haiti, rest of the country ...\n",
              "3             Information about the National Palace-\n",
              "4                     Storm at sacred heart of jesus"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMQD-CRWeHvu"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "**1.1.2** - Define a function `clean_data` that takes the data frame as input, converts the characters to lower case and removes any special characters that you might consider irrelevant,  adds the start token `<s>` and the end token `</s>` to every sentence (row) in the data frame and returns the processed data frame. \n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va7CNTlaiQmw"
      },
      "source": [
        "def preprocess_text(text):\n",
        "  clean_text = text.lower()\n",
        "  # remove special characters - basically anything that is not a letter or a space\n",
        "  clean_text = re.sub(r'[^a-z0-9\\s]+','', clean_text)\n",
        "  clean_text = '<s> '+clean_text+' </s>'\n",
        "  return clean_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E7jcvkOeHvu"
      },
      "source": [
        "# Your code here\n",
        "def clean_data_1(dataframe):\n",
        "  dataframe['message'] = dataframe['message'].apply(preprocess_text) \n",
        "  return dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-89piHxei8RQ"
      },
      "source": [
        "# clean the data\n",
        "df = clean_data_1(raw_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncJbbmVRjnFI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "35d52a4b-e031-4ce4-9c1b-a5a9a39d5644"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;s&gt; weather update  a cold front from cuba tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;s&gt; is the hurricane over or is it not over &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;s&gt; says west side of haiti rest of the countr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;s&gt; information about the national palace &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;s&gt; storm at sacred heart of jesus &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message\n",
              "0  <s> weather update  a cold front from cuba tha...\n",
              "1   <s> is the hurricane over or is it not over </s>\n",
              "2  <s> says west side of haiti rest of the countr...\n",
              "3     <s> information about the national palace </s>\n",
              "4            <s> storm at sacred heart of jesus </s>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbJsxkICeHvu"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.1.3** - Split the dataset into train and test sets. The proportion should be 0.95 and 0.05, respectively. You will create the language model based on the train set and validate your results on the test set.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7MGHw32eHvv"
      },
      "source": [
        "# Your code here\n",
        "index = np.unique(df.index)\n",
        "train_index, val_index = train_test_split(index, train_size=0.95, random_state=66)\n",
        "\n",
        "df_train = df.loc[train_index]\n",
        "df_val = df.loc[val_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8GQEOQNeHvv"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **1.2 [8 points] TOKENIZE AND COUNT**\n",
        "<br />\n",
        "In this section, you will create three different tokenizers and build an LM based on each one of them. The tokenization functions must divide the text into tokens, count their frequency and return a dictionary with a mapping of token to number.\n",
        "\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6rUkeBZeHvv"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.2.1** - Create your own tokenization function ('tokenizer_1') based on whitespace. Set the vocabulary size to 1000, including the `<UNK>` token for out of the vocabulary (OOV) words. \n",
        "<br /><br />\n",
        "    \n",
        "</div> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "779oD91IeHvv"
      },
      "source": [
        "# Fill in to complete this function \n",
        "def tokenizer_1(text_corpus, vocabulary_size):\n",
        "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(Counter(text_corpus.split()).most_common(vocabulary_size-1))\n",
        "    \n",
        "    dictionary={}\n",
        "    # For all words in count, assign a token (you can use a for loop) \n",
        "    for i, tup in enumerate(count):\n",
        "        dictionary[tup[0]] = i\n",
        "        \n",
        "    # Make a new list of tokens associated with words    \n",
        "    data = []\n",
        "    # Initialize a counter for 'UNK' values \n",
        "    unk_count = 0\n",
        "    \n",
        "    # For all words in corpus, find the associated token, and append to \n",
        "    # the 'data' variable defined above\n",
        "    for word in text_corpus:\n",
        "        if word in dictionary:\n",
        "            token = dictionary[word]\n",
        "        # If word is not in dictionary, it is 'out of vocabulary'\n",
        "        # So we need to assign it the zero token and\n",
        "        # update the count of the 'UNK' token\n",
        "        else:\n",
        "            token = 0  \n",
        "            unk_count += 1\n",
        "            \n",
        "        # Append token to data \n",
        "        data.append(token)\n",
        "        \n",
        "    # We can now set the count of 'UNK' tokens in the corpus\n",
        "    count[0][1] = unk_count\n",
        "    \n",
        "    # A reverse dictionary takes you from tokens to words\n",
        "    # Eg. if dictionary['Ignacio'] == 44\n",
        "    # reverse_dictionary[44] == 'Ignacio'\n",
        "    reversed_dictionary = {v:k for k,v in dictionary.items()}\n",
        "    \n",
        "    return data, count, dictionary, reversed_dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ktr_kTBeHvw"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.2.2** - Create a second tokenization function ('tokenizer_2') based on whitespace, but do not limit the vocabulary size.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLGPaXj3eHvw"
      },
      "source": [
        "# Fill in to complete this function \n",
        "def tokenizer_2(text_corpus):\n",
        "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
        "    count = []\n",
        "    count.extend(Counter(text_corpus.split()).most_common())\n",
        "    \n",
        "    dictionary={}\n",
        "    for i, tup in enumerate(count):\n",
        "        dictionary[tup[0]] = i\n",
        "        \n",
        "    # Make a new list of tokens associated with words    \n",
        "    data = []\n",
        "    for word in text_corpus:\n",
        "        if word in dictionary:\n",
        "            token = dictionary[word]\n",
        "        else:\n",
        "            token = 0  \n",
        "            \n",
        "        # Append token to data \n",
        "        data.append(token)\n",
        "\n",
        "    dictionary['UNK']=len(text_corpus.split())\n",
        "    reversed_dictionary = {v:k for k,v in dictionary.items()}\n",
        "    \n",
        "    return data, count, dictionary, reversed_dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwE4UjhNeHvw"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.2.3** - Create a third tokenization function ('tokenizer_3') based on sub-words. You have to define a set of common sub-words in the English language, for example, the subtokens _ing_ and _n't_.\n",
        "    \n",
        "In this example, the sentence \"_It is raining outside_\" would be tokenized as [_It_, _is_, _rain_, _ing_, _outside_ ].\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKgmLPEykLWE"
      },
      "source": [
        "def sub_word_tokenizer(text_corpus):\n",
        "  common_prefix = [ 'pre', 'un', 'under', 'over', 'post', 'ir', 'in', 'sub']\n",
        "  common_suffix = ['ing','ly','ed', 'n\\'t','er','est', 'es', 'ful']\n",
        "\n",
        "  word_seq = text_corpus.split()\n",
        "  results = []\n",
        "  for word in word_seq:\n",
        "    filtered_prefix = list(filter(word.startswith, common_prefix))\n",
        "    filtered_suffix = list(filter(word.endswith, common_suffix))\n",
        "    if len(filtered_prefix)>0:\n",
        "      results.extend(re.split(rf\"^({filtered_prefix[0]})\",word))\n",
        "    if len(filtered_suffix)>0:\n",
        "      results.extend(re.split(rf\"({filtered_suffix[0]})$\",word))\n",
        "    if len(filtered_prefix)==0 and len(filtered_suffix)==0:\n",
        "      results.append(word)\n",
        "  \n",
        "  subword_tokens = list(filter(None, results))\n",
        "  return subword_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qEdHsrOeHvw"
      },
      "source": [
        "# Your code here\n",
        "def tokenizer_3(text_corpus):\n",
        "  subword_tokens = sub_word_tokenizer(text_corpus)\n",
        "  count = []\n",
        "  count.extend(Counter(subword_tokens).most_common())\n",
        "\n",
        "  dictionary={}\n",
        "  for i, tup in enumerate(count):\n",
        "      dictionary[tup[0]] = i\n",
        "      \n",
        "  # Make a new list of tokens associated with words    \n",
        "  data = []\n",
        "  for word in text_corpus:\n",
        "      if word in dictionary:\n",
        "          token = dictionary[word]\n",
        "      else:\n",
        "          token = 0  \n",
        "          \n",
        "      # Append token to data \n",
        "      data.append(token)\n",
        "      \n",
        "  dictionary['UNK']=len(text_corpus.split())\n",
        "  reversed_dictionary = {v:k for k,v in dictionary.items()}\n",
        "\n",
        "  return data, count, dictionary, reversed_dictionary\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUDI4DhLeHvw"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **1.3 [6 points] CONSTRUCTING BIGRAMS**\n",
        "<br />\n",
        "\n",
        "**1.3.1** - Using each of the tokenizer functions you created, split each sentence into tokens in their numerical representation. \n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY8lbSsAtMX8"
      },
      "source": [
        "def get_tok_sequence(text, vocab_dict):\n",
        "  seq=[]\n",
        "  for word in text.split():\n",
        "    if word in vocab_dict.keys():\n",
        "      seq.append(vocab_dict[word])\n",
        "    else:\n",
        "      seq.append(vocab_dict['UNK'])\n",
        "  return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAd57jUIsLxD"
      },
      "source": [
        "text_corpus = \"\"\n",
        "for text in  df['message'].values:\n",
        "  text_corpus = text_corpus + \" \"+ text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hAtFbwxs87-"
      },
      "source": [
        "data, count, dictionary, reversed_dictionary  = tokenizer_1(text_corpus,1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcBQAjWQuV_n"
      },
      "source": [
        "df_tok1 = pd.DataFrame(df['message'].apply(get_tok_sequence, vocab_dict=dictionary), columns=['message'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdHgFBBiwTnN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d03bb3e2-1952-433b-aadf-fdc7399f42d4"
      },
      "source": [
        "df_tok1.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[2, 187, 0, 8, 437, 960, 23, 0, 16, 130, 551, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[2, 11, 1, 133, 80, 55, 11, 29, 36, 80, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[2, 512, 336, 979, 6, 67, 0, 6, 1, 84, 248, 4,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[2, 76, 60, 1, 165, 0, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[2, 138, 26, 0, 980, 6, 0, 3]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message\n",
              "0  [2, 187, 0, 8, 437, 960, 23, 0, 16, 130, 551, ...\n",
              "1         [2, 11, 1, 133, 80, 55, 11, 29, 36, 80, 3]\n",
              "2  [2, 512, 336, 979, 6, 67, 0, 6, 1, 84, 248, 4,...\n",
              "3                          [2, 76, 60, 1, 165, 0, 3]\n",
              "4                      [2, 138, 26, 0, 980, 6, 0, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiBhLryJz_6K"
      },
      "source": [
        "data_2, count_2, dictionary_2, reversed_dictionary_2  = tokenizer_2(text_corpus)\n",
        "df_tok2 = pd.DataFrame(df['message'].apply(get_tok_sequence, vocab_dict=dictionary_2), columns=['message'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-prQDSb0PiT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b4a09783-d586-4cd1-c2eb-7ef7bf5760f9"
      },
      "source": [
        "df_tok2.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 186, 1959, 7, 436, 959, 22, 2661, 15, 129,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 10, 0, 132, 79, 54, 10, 28, 35, 79, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 511, 335, 978, 5, 66, 1327, 5, 0, 83, 247,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 75, 59, 0, 164, 3300, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 137, 25, 15281, 979, 5, 1647, 2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message\n",
              "0  [1, 186, 1959, 7, 436, 959, 22, 2661, 15, 129,...\n",
              "1         [1, 10, 0, 132, 79, 54, 10, 28, 35, 79, 2]\n",
              "2  [1, 511, 335, 978, 5, 66, 1327, 5, 0, 83, 247,...\n",
              "3                       [1, 75, 59, 0, 164, 3300, 2]\n",
              "4               [1, 137, 25, 15281, 979, 5, 1647, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYLWM3LklRQ3"
      },
      "source": [
        "def get_subword_tok_sequence(text, vocab_dict):\n",
        "  subword_tokens = sub_word_tokenizer(text)\n",
        "  seq=[]\n",
        "  for word in subword_tokens:\n",
        "    if word in vocab_dict.keys():\n",
        "      seq.append(vocab_dict[word])\n",
        "    else:\n",
        "      seq.append(vocab_dict['UNK'])\n",
        "  return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4GxBhhw1ZaZ"
      },
      "source": [
        "data_3, count_3, dictionary_3, reversed_dictionary_3 = tokenizer_3(text_corpus)\n",
        "df_tok3 = pd.DataFrame(df['message'].apply(get_subword_tok_sequence, vocab_dict=dictionary_3), columns=['message'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTlmonM8_yFi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cd80c7c7-693d-4ee8-84b1-60a16833978f"
      },
      "source": [
        "df_tok3.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 213, 10, 1955, 11, 435, 1001, 29, 2606, 20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 15, 0, 151, 67, 91, 10, 62, 15, 36, 35, 67...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 561, 236, 54, 502, 9, 75, 149, 54, 9, 0, 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 3, 86, 68, 0, 183, 3186, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 156, 31, 13824, 4, 1039, 9, 1659, 2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message\n",
              "0  [1, 213, 10, 1955, 11, 435, 1001, 29, 2606, 20...\n",
              "1  [1, 15, 0, 151, 67, 91, 10, 62, 15, 36, 35, 67...\n",
              "2  [1, 561, 236, 54, 502, 9, 75, 149, 54, 9, 0, 9...\n",
              "3                    [1, 3, 86, 68, 0, 183, 3186, 2]\n",
              "4           [1, 156, 31, 13824, 4, 1039, 9, 1659, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-eaLNsNeHvw"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.3.2** - Count the bigrams in the dataset for each tokenizer and divide them by the total number of bigrams. This will give you the probability of each bigram.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc83X8dVeHvx"
      },
      "source": [
        "# Your code here\n",
        "def get_bigrams(df):\n",
        "  bigrams = []\n",
        "  for seq in df['message'].values:\n",
        "    bigrams.extend([(seq[i],seq[i+1]) for i in range(1,len(seq[1:-1]))])\n",
        "  bigrams_count_dict = Counter(bigrams)\n",
        "  return bigrams_count_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggfq882eyyFf"
      },
      "source": [
        "def get_bigram_prob(bigrams_count_dict):\n",
        "  first_gram_count = {}\n",
        "  for k, v in bigrams_count_dict.items():\n",
        "    if k[0] in first_gram_count.keys():\n",
        "      first_gram_count[k[0]] += v\n",
        "    else:\n",
        "      first_gram_count[k[0]] = v\n",
        "\n",
        "  bigram_prob_dict = {k: v / first_gram_count[k[0]] for k, v in bigrams_count_dict.items()}\n",
        "  return bigram_prob_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfUKg9sq0Wtr"
      },
      "source": [
        "# Bigram probability for tokenizer1  \n",
        "bigrams_count_dict_tok1 = get_bigrams(df_tok1)\n",
        "bigram_prob_dict_tok1 = get_bigram_prob(bigrams_count_dict_tok1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW6Toj2zwz7G",
        "outputId": "b0bacd8d-0f09-4405-a89f-d120a31807b1"
      },
      "source": [
        "max(bigrams_count_dict_tok1, key=bigrams_count_dict_tok1.get)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqtH1rV_02FO"
      },
      "source": [
        "# Bigram probability for tokenizer2  \n",
        "bigrams_count_dict_tok2 = get_bigrams(df_tok2)\n",
        "bigram_prob_dict_tok2 = get_bigram_prob(bigrams_count_dict_tok2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2aZztZ1xJEt",
        "outputId": "ab9983e4-3017-47d8-f6ff-90834d7c8640"
      },
      "source": [
        "max(bigrams_count_dict_tok2, key=bigrams_count_dict_tok2.get)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpwUfxmY1h2l"
      },
      "source": [
        "# Bigram probability for tokenizer3  \n",
        "bigrams_count_dict_tok3 = get_bigrams(df_tok3)\n",
        "bigram_prob_dict_tok3 = get_bigram_prob(bigrams_count_dict_tok3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-0Eh7k6eHvx"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **1.4 [8 points] PREDICTING THE NEXT WORDS**\n",
        "<br />\n",
        "\n",
        "**1.4.1** - Simulate the incomplete messages dividing each sentence of the **test** set into two. For this, split each sentence in a 3:1 ratio. The first $75\\%$ of a sentence will represent the correct message, and the last $25\\%$ will convey the missing information. You will not give this 25% to your model, it is kept hidden. This 25% will only be used to evaluate the predictions of your language model.\n",
        "    \n",
        "For example in the sentence: *\"I will go out on a vacation, now that my semester ended.\"*\n",
        "\n",
        "The first 75% will be *\"I will go out on a vacation, now\"*\n",
        "\n",
        "The last 25% will be *\"that my semeter ended\"*\n",
        "\n",
        "Your aim is to predict the last part by giving your model the first \"part\" of the sentence.\n",
        "\n",
        "\n",
        "Note that in an n-gram language model, only the last $n-1$ words are used to make a prediction. For example, for the above sentence, if you are using bigrams, the input to your model would only be \"now\" and you are expected to predict \"that\".\n",
        "    \n",
        "<br /><br />   \n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-id0s97jeHvx"
      },
      "source": [
        "# Your code here\n",
        "def split_sent(word_seq):\n",
        "  msg_len = round(0.75*len(word_seq))\n",
        "  msg = word_seq[:msg_len]\n",
        "  missing_info = word_seq[msg_len:]\n",
        "  return msg, missing_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F27PVLL1eHvx"
      },
      "source": [
        "# get word sequence based on tokenizer_1 for test set \n",
        "df_val_tok1 = pd.DataFrame(df_val['message'].apply(get_tok_sequence, vocab_dict=dictionary), columns=['message'])\n",
        "\n",
        "# get word sequence based on tokenizer_2 for test set \n",
        "df_val_tok2 = pd.DataFrame(df_val['message'].apply(get_tok_sequence, vocab_dict=dictionary_2), columns=['message'])\n",
        "\n",
        "# get word sequence based on tokenizer_3 for test set \n",
        "df_val_tok3 = pd.DataFrame(df_val['message'].apply(get_subword_tok_sequence, vocab_dict=dictionary_3), columns=['message'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grVd-hLxSEtP"
      },
      "source": [
        "# split the msg in 3:1 ratio\n",
        "def split_sent_df(df):\n",
        "  message, missing_info = [],[]\n",
        "  for text_seq in df['message'].values:\n",
        "    x, y = split_sent(text_seq)\n",
        "    message.append(x)\n",
        "    missing_info.append(y)\n",
        "\n",
        "  df_split = pd.DataFrame({'message':message, 'missing_info':missing_info})\n",
        "  return df_split\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibq0sVLsSzod"
      },
      "source": [
        "df_val_tok1_split = split_sent_df(df_val_tok1)\n",
        "df_val_tok2_split = split_sent_df(df_val_tok2)\n",
        "df_val_tok3_split = split_sent_df(df_val_tok3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Lf4C33kkL8_M",
        "outputId": "73f2c68e-6f65-437e-9175-d81e194c242f"
      },
      "source": [
        "df_val_tok2_split.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "      <th>missing_info</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 148, 0, 4949, 941, 1663, 22, 25936, 41, 35...</td>\n",
              "      <td>[0, 742, 4, 573, 5904, 55, 1823, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 60, 698, 35, 18697, 172, 316, 34]</td>\n",
              "      <td>[15, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 60, 86, 1130, 9, 7, 179, 48, 1158, 29]</td>\n",
              "      <td>[8, 256, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 211, 4, 382, 2201, 64, 53, 84, 1092, 19, 1...</td>\n",
              "      <td>[31930, 31931, 31932, 3, 31933, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 251, 476, 674, 26, 1040, 13348, 3292, 5, 8...</td>\n",
              "      <td>[13349, 694, 5565, 15, 453, 26009, 5156, 5498,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message                                       missing_info\n",
              "0  [1, 148, 0, 4949, 941, 1663, 22, 25936, 41, 35...                [0, 742, 4, 573, 5904, 55, 1823, 2]\n",
              "1              [1, 60, 698, 35, 18697, 172, 316, 34]                                            [15, 2]\n",
              "2         [1, 60, 86, 1130, 9, 7, 179, 48, 1158, 29]                                        [8, 256, 2]\n",
              "3  [1, 211, 4, 382, 2201, 64, 53, 84, 1092, 19, 1...                 [31930, 31931, 31932, 3, 31933, 2]\n",
              "4  [1, 251, 476, 674, 26, 1040, 13348, 3292, 5, 8...  [13349, 694, 5565, 15, 453, 26009, 5156, 5498,..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "8fWeXNsvWXb7",
        "outputId": "daea2670-df63-4032-8e47-97b37888c345"
      },
      "source": [
        "df_val_tok3_split.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "      <th>missing_info</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 172, 0, 1748, 993, 1679, 29, 23905, 51, 35...</td>\n",
              "      <td>[0, 714, 8, 7, 621, 10, 3997, 63, 1841, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 69, 749, 35, 22, 17039, 195, 339]</td>\n",
              "      <td>[41, 20, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 69, 99, 550, 5, 13, 11, 202, 56, 1199]</td>\n",
              "      <td>[37, 12, 155, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 230, 5, 7, 423, 616, 8, 73, 61, 97, 1145, ...</td>\n",
              "      <td>[6, 29337, 29338, 29339, 6, 29340, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 283, 508, 740, 33, 796, 4, 12068, 3178, 9,...</td>\n",
              "      <td>[892, 12069, 518, 5166, 20, 3, 500, 23972, 478...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message                                       missing_info\n",
              "0  [1, 172, 0, 1748, 993, 1679, 29, 23905, 51, 35...         [0, 714, 8, 7, 621, 10, 3997, 63, 1841, 2]\n",
              "1              [1, 69, 749, 35, 22, 17039, 195, 339]                                        [41, 20, 2]\n",
              "2         [1, 69, 99, 550, 5, 13, 11, 202, 56, 1199]                                   [37, 12, 155, 2]\n",
              "3  [1, 230, 5, 7, 423, 616, 8, 73, 61, 97, 1145, ...              [6, 29337, 29338, 29339, 6, 29340, 2]\n",
              "4  [1, 283, 508, 740, 33, 796, 4, 12068, 3178, 9,...  [892, 12069, 518, 5166, 20, 3, 500, 23972, 478..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdckQrYgeHvx"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "\n",
        "**1.4.2** - Given 5 sentences from the previous question (test set), predict the next word. \n",
        "Append this predicted word to the input sequence and predict the next one. Repeat this process until you reach the 10th token or the end of a sentence. Compare your results qualitatively with the original sentences. Do the results make sense wrt the context and semantics?\n",
        "\n",
        "Repeat this for all the models built using different tokenization techniques.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODaMZUSPxmJb"
      },
      "source": [
        "def predict_missing_word_freq_based(msg, missing_info, dictionary, reversed_dictionary, count_dict):\n",
        "  missing_text_actual = \"\"\n",
        "  for seq in missing_info[:min(10, len(missing_info))]:\n",
        "    missing_text_actual += reversed_dictionary[seq] + \" \"\n",
        "\n",
        "  missing_text_predicted=''\n",
        "  message = msg.copy()\n",
        "  for i in range(min(10, len(missing_info))):\n",
        "    last_word = message[-1]\n",
        "    keyword_dict = {k[1]:v for k,v in count_dict.items() if k[0] == last_word}\n",
        "\n",
        "    next_word = max(keyword_dict, key=keyword_dict.get)\n",
        "    message.append(next_word)\n",
        "    missing_text_predicted += reversed_dictionary[next_word] + \" \"\n",
        "\n",
        "  return missing_text_actual, missing_text_predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QOcBjmKag8g"
      },
      "source": [
        "def print_predicted_seq(df, dictionary, reversed_dictionary, count_dict):\n",
        "  for i in range(5):\n",
        "    actual, pred = predict_missing_word_freq_based(df['message'][i], df['missing_info'][i], dictionary, reversed_dictionary, count_dict)\n",
        "    print(\"==== Sentence {} ====\".format(i+1))\n",
        "    incomplete_message= \" \".join([reversed_dictionary[seq] for seq in df['message'][i]])\n",
        "    print(\"Input Message :\", incomplete_message)\n",
        "    print(\"Actual: \",actual)\n",
        "    print(\"Predicted: \",pred)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t87ngLI8hdrl",
        "outputId": "b036dfe4-b116-4136-a66a-2acfd7d480e1"
      },
      "source": [
        "print_predicted_seq(df_val_tok1_split, dictionary, reversed_dictionary, bigrams_count_dict_tok1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== Sentence 1 ====\n",
            "Input Message : <s> while the UNK line UNK from UNK was not UNK some UNK were made to the UNK to UNK for more UNK UNK of\n",
            "Actual:  the lines to better UNK their UNK </s> \n",
            "Predicted:  UNK UNK UNK UNK UNK UNK UNK UNK \n",
            "\n",
            "==== Sentence 2 ====\n",
            "Input Message : <s> if u not UNK just tell me\n",
            "Actual:  that </s> \n",
            "Predicted:  UNK UNK \n",
            "\n",
            "==== Sentence 3 ====\n",
            "Input Message : <s> if im UNK for a job which UNK can\n",
            "Actual:  i call </s> \n",
            "Predicted:  i am a \n",
            "\n",
            "==== Sentence 4 ====\n",
            "Input Message : <s> according to wfp UNK no more than UNK people have UNK in three temporary UNK camps near UNK and\n",
            "Actual:  UNK UNK UNK and UNK </s> \n",
            "Predicted:  UNK UNK UNK UNK UNK UNK \n",
            "\n",
            "==== Sentence 5 ====\n",
            "Input Message : <s> pakistan air force has UNK UNK UNK of relief UNK including tents blankets dry UNK medicines in the disaster UNK UNK and UNK areas during last six UNK relief activities was UNK by various\n",
            "Actual:  UNK transport UNK that include UNK UNK UNK and UNK \n",
            "Predicted:  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Es43Rmh5Lx",
        "outputId": "2bf4f7de-df4a-49a4-9a72-3f64503407de"
      },
      "source": [
        "print_predicted_seq(df_val_tok2_split, dictionary_2, reversed_dictionary_2, bigrams_count_dict_tok2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== Sentence 1 ====\n",
            "Input Message : <s> while the actual line data from gaul was not modified some adjustments were made to the data to allow for more finegrained styling of\n",
            "Actual:  the lines to better reflect their status </s> \n",
            "Predicted:  the earthquake in the earthquake in the earthquake \n",
            "\n",
            "==== Sentence 2 ====\n",
            "Input Message : <s> if u not underst just tell me\n",
            "Actual:  that </s> \n",
            "Predicted:  to the \n",
            "\n",
            "==== Sentence 3 ====\n",
            "Input Message : <s> if im looking for a job which ngo can\n",
            "Actual:  i call </s> \n",
            "Predicted:  i am a \n",
            "\n",
            "==== Sentence 4 ====\n",
            "Input Message : <s> according to wfp estimates no more than 10000 people have remained in three temporary accomodation camps near gisenyi and\n",
            "Actual:  ruhengeri nkamira mkwero and mudende </s> \n",
            "Predicted:  the earthquake in the earthquake in \n",
            "\n",
            "==== Sentence 5 ====\n",
            "Input Message : <s> pakistan air force has delivered 163000 kg of relief cargo including tents blankets dry ration medicines in the disaster stricken muzaffarabad and adjacent areas during last six daysthe relief activities was undertaken by various\n",
            "Actual:  paf transport aircrafts that include y12 c130 casa and mi17 \n",
            "Predicted:  locations for the earthquake in the earthquake in the earthquake \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6966-Deh-XS",
        "outputId": "1e93e9ab-fa05-4909-b820-d8294c00b263"
      },
      "source": [
        "print_predicted_seq(df_val_tok3_split, dictionary_3, reversed_dictionary_3, bigrams_count_dict_tok3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== Sentence 1 ====\n",
            "Input Message : <s> while the actual line data from gaul was not modifi ed some adjustments were made to the data to allow for more finegrain ed styl ing of\n",
            "Actual:  the lin es to bett er reflect their status </s> \n",
            "Predicted:  the in the in the in the in the in \n",
            "\n",
            "==== Sentence 2 ====\n",
            "Input Message : <s> if u not un derst just tell\n",
            "Actual:  me that </s> \n",
            "Predicted:  me to the \n",
            "\n",
            "==== Sentence 3 ====\n",
            "Input Message : <s> if im look ing for a job which ngo\n",
            "Actual:  can i call </s> \n",
            "Predicted:  that the in the \n",
            "\n",
            "==== Sentence 4 ====\n",
            "Input Message : <s> accord ing to wfp estimat es no more than 10000 people have remain ed in three temporary accomodation camps near gisenyi\n",
            "Actual:  and ruhengeri nkamira mkwero and mudende </s> \n",
            "Predicted:  and the in the in the in \n",
            "\n",
            "==== Sentence 5 ====\n",
            "Input Message : <s> pakistan air force has deliver ed 163000 kg of relief cargo in cluding includ ing tents blankets dry ration medicin es in the disast er stricken muzaffarabad and adjacent areas dur ing last six daysthe relief activiti es was un dertaken by\n",
            "Actual:  various paf transport aircrafts that in clude y12 c130 casa \n",
            "Predicted:  the in the in the in the in the in \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr2aiM6xnnyy"
      },
      "source": [
        "**We can clearly see that the perdicted word doesn't make sense wrt the context and semantics, since its always predicting the most frequent word.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVSW0IpFeHvx"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.4.3** - Repeat the same exercise, for all 3 models, but this time, the next token will be sampled from a distribution given by the bigram frequency. Compare and comment on the results?\n",
        "\n",
        "\n",
        "*Hint:* In a model of two bigrams with frequencies 0.7 and 0.3, a deterministic prediction will only predict the first bigram. Sampling from a distribution, will enable the model to predict the second bigram with a probability of 0.3. In this way we can still predict infrequent tokens. \n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y864JVr3eHvy"
      },
      "source": [
        "# Your code here\n",
        "def predict_next_word(msg, missing_info, dictionary, reversed_dictionary, probability_dict):\n",
        "  missing_text_actual = \"\"\n",
        "  for seq in missing_info[:min(10, len(missing_info))]:\n",
        "    missing_text_actual += reversed_dictionary[seq] + \" \"\n",
        "\n",
        "  missing_text_predicted=''\n",
        "  message = msg.copy()\n",
        "  for i in range(min(10, len(missing_info))):\n",
        "    last_word = message[-1]\n",
        "    probability_list = [v for k,v in probability_dict.items() if k[0] == last_word]\n",
        "    next_word_list = [k[1] for k,v in probability_dict.items() if k[0] == last_word]\n",
        "\n",
        "    next_word = np.random.choice(next_word_list, p=probability_list)\n",
        "    message.append(next_word)\n",
        "    missing_text_predicted += reversed_dictionary[next_word] + \" \"\n",
        "\n",
        "  return missing_text_actual, missing_text_predicted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWB36AV1umc3"
      },
      "source": [
        "def print_predicted_seq(df, dictionary, reversed_dictionary, probability_dict):\n",
        "  for i in range(5):\n",
        "    actual, pred = predict_next_word(df['message'][i], df['missing_info'][i], dictionary, reversed_dictionary, probability_dict)\n",
        "    print(\"==== Sentence {} ====\".format(i+1))\n",
        "    incomplete_message= \" \".join([reversed_dictionary[seq] for seq in df['message'][i]])\n",
        "    print(\"Input Message :\", incomplete_message)\n",
        "    print(\"Actual: \",actual)\n",
        "    print(\"Predicted: \",pred)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOr3zRJEAVhQ",
        "outputId": "4578bde3-dad4-4f95-c8d8-f989abf604e0"
      },
      "source": [
        "print_predicted_seq(df_val_tok1_split, dictionary, reversed_dictionary, bigram_prob_dict_tok1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== Sentence 1 ====\n",
            "Input Message : <s> while the UNK line UNK from UNK was not UNK some UNK were made to the UNK to UNK for more UNK UNK of\n",
            "Actual:  the lines to better UNK their UNK </s> \n",
            "Predicted:  UNK good morning a person UNK UNK a \n",
            "\n",
            "==== Sentence 2 ====\n",
            "Input Message : <s> if u not UNK just tell me\n",
            "Actual:  that </s> \n",
            "Predicted:  are in \n",
            "\n",
            "==== Sentence 3 ====\n",
            "Input Message : <s> if im UNK for a job which UNK can\n",
            "Actual:  i call </s> \n",
            "Predicted:  send to UNK \n",
            "\n",
            "==== Sentence 4 ====\n",
            "Input Message : <s> according to wfp UNK no more than UNK people have UNK in three temporary UNK camps near UNK and\n",
            "Actual:  UNK UNK UNK and UNK </s> \n",
            "Predicted:  roads used to improve UNK also \n",
            "\n",
            "==== Sentence 5 ====\n",
            "Input Message : <s> pakistan air force has UNK UNK UNK of relief UNK including tents blankets dry UNK medicines in the disaster UNK UNK and UNK areas during last six UNK relief activities was UNK by various\n",
            "Actual:  UNK transport UNK that include UNK UNK UNK and UNK \n",
            "Predicted:  communities to pass over a large parts of his UNK \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhBF_-8FEuw_",
        "outputId": "1d295019-796d-432b-b843-997f01f8a8c1"
      },
      "source": [
        "print_predicted_seq(df_val_tok2_split, dictionary_2, reversed_dictionary_2, bigram_prob_dict_tok2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== Sentence 1 ====\n",
            "Input Message : <s> while the actual line data from gaul was not modified some adjustments were made to the data to allow for more finegrained styling of\n",
            "Actual:  the lines to better reflect their status </s> \n",
            "Predicted:  cyclones and hundreds of tools that we were \n",
            "\n",
            "==== Sentence 2 ====\n",
            "Input Message : <s> if u not underst just tell me\n",
            "Actual:  that </s> \n",
            "Predicted:  where unhcr \n",
            "\n",
            "==== Sentence 3 ====\n",
            "Input Message : <s> if im looking for a job which ngo can\n",
            "Actual:  i call </s> \n",
            "Predicted:  go through nutrition \n",
            "\n",
            "==== Sentence 4 ====\n",
            "Input Message : <s> according to wfp estimates no more than 10000 people have remained in three temporary accomodation camps near gisenyi and\n",
            "Actual:  ruhengeri nkamira mkwero and mudende </s> \n",
            "Predicted:  little thing which it is job \n",
            "\n",
            "==== Sentence 5 ====\n",
            "Input Message : <s> pakistan air force has delivered 163000 kg of relief cargo including tents blankets dry ration medicines in the disaster stricken muzaffarabad and adjacent areas during last six daysthe relief activities was undertaken by various\n",
            "Actual:  paf transport aircrafts that include y12 c130 casa and mi17 \n",
            "Predicted:  camps daily paper del valle nevado ski resort to ensure \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX7kKjUER-wI",
        "outputId": "00760023-fad5-4378-b470-61d2fdf1e47d"
      },
      "source": [
        "print_predicted_seq(df_val_tok3_split, dictionary_3, reversed_dictionary_3, bigram_prob_dict_tok3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== Sentence 1 ====\n",
            "Input Message : <s> while the actual line data from gaul was not modifi ed some adjustments were made to the data to allow for more finegrain ed styl ing of\n",
            "Actual:  the lin es to bett er reflect their status </s> \n",
            "Predicted:  shops just to l ed fishery sector in haiti what \n",
            "\n",
            "==== Sentence 2 ====\n",
            "Input Message : <s> if u not un derst just tell\n",
            "Actual:  me that </s> \n",
            "Predicted:  me the hudson \n",
            "\n",
            "==== Sentence 3 ====\n",
            "Input Message : <s> if im look ing for a job which ngo\n",
            "Actual:  can i call </s> \n",
            "Predicted:  are sufer ing can \n",
            "\n",
            "==== Sentence 4 ====\n",
            "Input Message : <s> accord ing to wfp estimat es no more than 10000 people have remain ed in three temporary accomodation camps near gisenyi\n",
            "Actual:  and ruhengeri nkamira mkwero and mudende </s> \n",
            "Predicted:  and countryspecific programs work ing of rue \n",
            "\n",
            "==== Sentence 5 ====\n",
            "Input Message : <s> pakistan air force has deliver ed 163000 kg of relief cargo in cluding includ ing tents blankets dry ration medicin es in the disast er stricken muzaffarabad and adjacent areas dur ing last six daysthe relief activiti es was un dertaken by\n",
            "Actual:  various paf transport aircrafts that in clude y12 c130 casa \n",
            "Predicted:  burn ing this messageim an emergency relief suppli es 4636 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCHmIDeWeHvy"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **1.5 [5 points] EVALUATE THE LANGUAGE MODELS**\n",
        "<br />\n",
        "\n",
        "    \n",
        "**1.5.1** - For each of the models built using different tokenization techniques, compute the average perplexity in the test set (part 1.1.3). Perform smoothing on the bigram models. Based on the perplexity, which model is better?\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "5EqDp6yCeI54",
        "outputId": "19ab8da2-11cf-455b-e3a2-da43e56640b7"
      },
      "source": [
        "df_val_tok1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12640</th>\n",
              "      <td>[2, 149, 1, 0, 942, 0, 23, 0, 42, 36, 0, 48, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6978</th>\n",
              "      <td>[2, 61, 699, 36, 0, 173, 317, 35, 16, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5754</th>\n",
              "      <td>[2, 61, 87, 0, 10, 8, 180, 49, 0, 30, 9, 257, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17980</th>\n",
              "      <td>[2, 212, 5, 383, 0, 65, 54, 85, 0, 20, 13, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12699</th>\n",
              "      <td>[2, 252, 477, 675, 27, 0, 0, 0, 6, 83, 0, 124,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4779</th>\n",
              "      <td>[2, 14, 12, 540, 71, 6, 96, 104, 0, 41, 36, 65...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20460</th>\n",
              "      <td>[2, 0, 0, 607, 0, 908, 54, 85, 0, 494, 79, 8, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4727</th>\n",
              "      <td>[2, 317, 1, 534, 5, 115, 1, 869, 80, 5, 250, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10616</th>\n",
              "      <td>[2, 48, 0, 86, 13, 56, 195, 0, 5, 1, 371, 0, 5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20852</th>\n",
              "      <td>[2, 0, 694, 0, 12, 62, 373, 5, 33, 872, 140, 5...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1053 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 message\n",
              "12640  [2, 149, 1, 0, 942, 0, 23, 0, 42, 36, 0, 48, 0...\n",
              "6978            [2, 61, 699, 36, 0, 173, 317, 35, 16, 3]\n",
              "5754    [2, 61, 87, 0, 10, 8, 180, 49, 0, 30, 9, 257, 3]\n",
              "17980  [2, 212, 5, 383, 0, 65, 54, 85, 0, 20, 13, 0, ...\n",
              "12699  [2, 252, 477, 675, 27, 0, 0, 0, 6, 83, 0, 124,...\n",
              "...                                                  ...\n",
              "4779   [2, 14, 12, 540, 71, 6, 96, 104, 0, 41, 36, 65...\n",
              "20460  [2, 0, 0, 607, 0, 908, 54, 85, 0, 494, 79, 8, ...\n",
              "4727     [2, 317, 1, 534, 5, 115, 1, 869, 80, 5, 250, 3]\n",
              "10616  [2, 48, 0, 86, 13, 56, 195, 0, 5, 1, 371, 0, 5...\n",
              "20852  [2, 0, 694, 0, 12, 62, 373, 5, 33, 872, 140, 5...\n",
              "\n",
              "[1053 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPUMX1_3cWvo",
        "outputId": "32719673-d07c-46af-9204-ae43813086f7"
      },
      "source": [
        "functools.reduce(lambda x,y: x+y,[len(x) for x in df_val_tok1.message.values])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26713"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knv9CpymeHvy"
      },
      "source": [
        "import functools\n",
        "\n",
        "# Your code here\n",
        "def perplexity_calculation(prob_dict, X_test):\n",
        "  total_count = functools.reduce(lambda x,y: x+y,[len(x) for x in X_test])\n",
        "  perplexity=0\n",
        "  for sentence in X_test:\n",
        "    prob_list = [prob_dict[(sentence[i], sentence[i+1])]+1 if (sentence[i], sentence[i+1]) in prob_dict.keys() else 1 for i in range(len(sentence)-1)]\n",
        "    prob_prod = functools.reduce((lambda x,y: x*y),prob_list)\n",
        "    perplexity += np.power((1/prob_prod),1/total_count)\n",
        "  return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42fFw7z0Eoc3",
        "outputId": "fcf5ee7c-e1cf-4659-da19-92053b2beb89"
      },
      "source": [
        "print(\"Average Perplexity of LM with tokenizer1 = {:.2f}\".format(perplexity_calculation(bigram_prob_dict_tok1, df_val_tok1.message.values)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Perplexity of LM with tokenizer1 = 1052.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DwF8Hf4FXwH",
        "outputId": "3ff3b249-d4a2-4931-8d94-3f2b3aadf3fc"
      },
      "source": [
        "print(\"Average Perplexity of LM with tokenizer2 = {:.2f}\".format(perplexity_calculation(bigram_prob_dict_tok2, df_val_tok2.message.values)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Perplexity of LM with tokenizer2 = 1052.90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5oQlQcEFZSG",
        "outputId": "cc40234c-87a6-4d9d-bbb7-5393c000dcb6"
      },
      "source": [
        "print(\"Average Perplexity of LM with tokenizer3 = {:.2f}\".format(perplexity_calculation(bigram_prob_dict_tok3, df_val_tok3.message.values)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Perplexity of LM with tokenizer3 = 1052.87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8izi_pjTeHvy"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.5.2** - Given the perplexities, which model do you think is better? Why do you think so? Does this reflect the quality of the prediction as seen in part 1.4? \n",
        "\n",
        "What is the effect of UNK words?\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onbPzcy2eHvy"
      },
      "source": [
        "#### Definietly the LM built with tokenizer 3 has the lowest perplexity, because here we're essentially teaching the model to learn the base word, therefore the same baseword with different prefix and suffix would be treated similarly, unlike the other tokenizers which fail to recognize this and end up assigning 'UNK' or prob=0 to the words with different prefix or suffix.\n",
        "\n",
        "#### Clearly, lower the perplexity, more confident our model is in its prediction. Therefore the quality of prediction is good.\n",
        "\n",
        "#### In tokenizer 1, that assigns 'UNK' to unkown words, the perplexity is also relatively lower than tokenizer 2, because it is assigning 'UNK' tag to all the new and less frequent words, because of which during prediction as well, it is mostly predicting 'UNK' which is correct according to the dictionary that is built, but the quality of the prediction isn't good at all, as it isn't conveying any meaningful info. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MYWqq-7eHvy"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **1.6 [3 points] HOMEWORK QUIZ**\n",
        "<br />\n",
        "After attempting this part of the homework, answer the questions on edStem. All the questions depend on this part of the homework and you will not be able to answer it without attempting it.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC0qDXmNeHvy"
      },
      "source": [
        "#### Answer the questions on EdStem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1k22ZTSeHvy"
      },
      "source": [
        "___\n",
        "___\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAXiWcigeHvz"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "<h1>PART 2 [50 pts]:Word2Vec from scratch</h1>\n",
        "<br /><br />\n",
        "[Return to contents](#contents)\n",
        "<br /><br />\n",
        "\n",
        "<a id=\"part2intro\"></a>\n",
        "<h2> Problem Statement </h2>\n",
        "<br /><br />\n",
        "[Return to contents](#contents)\n",
        "<br /><br />\n",
        "\n",
        "Word2Vec architecture allows us to get *contextual* representations of word tokens.     \n",
        "<br /><br />\n",
        "There are several methods to build a word embedding. We will focus on the SGNS architecture. \n",
        "![](https://i.ibb.co/FW8Sr54/Screen-Shot-2021-04-27-at-3-27-16-PM.png)    \n",
        "<br /><br />\n",
        "In this problem, you are asked to build and analyze a Word2Vec architecture trained on wikipedia articles.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns9A1eXTeHvz"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "## **PART 2 [50 pts]: Word2Vec from scratch** \n",
        "<br />\n",
        "\n",
        "Word2Vec architecture allows us to get *contextual* representations of word tokens.     \n",
        "<br /><br />\n",
        "There are several methods to build a word embedding. We will focus on the SGNS architecture. \n",
        "<br/>\n",
        "\n",
        "![](https://i.ibb.co/FW8Sr54/Screen-Shot-2021-04-27-at-3-27-16-PM.png)    \n",
        "<br />\n",
        "In this problem, you are asked to build and analyze a Word2Vec architecture trained on wikipedia articles.\n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CruesQYZeHvz"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "## **PART 2 Questions**\n",
        "<br />\n",
        "    \n",
        "### **2.1 [5 points] MODEL PROCESSING**\n",
        "<br />\n",
        "    \n",
        "**2.1.1 Get the data:**    \n",
        "\n",
        "- Get the data from the `text8.zip` file.\n",
        "    `text8.zip` is a small, *cleaned* subset of a large corpus of data scraped from wikipedia pages. More details can be found [here](https://paperswithcode.com/sota/language-modelling-on-text8).\n",
        "    It is usually used to quickly train, or test language models.\n",
        "    [Read here](http://mattmahoney.net/dc/textdata#:~:text=The%20purpose%20of%20the%20smaller,on%20the%20larger%20data%20set.&text=The%20two%20files%20have%20the,108%20bytes%20of%20fil9.) for more information.\n",
        "- Split the data by whitespace and print the first 10 words to check if has been correctly loaded.\n",
        "\n",
        "    **NOTE:** For this part of the homework, all words will be in their lowercase for simplicity of analysis.\n",
        "<br />    \n",
        "\n",
        "**2.1.2 Build the dataset**  \n",
        "\n",
        "- Write a function that takes the `vocabulary_size` and `corpus` as input, and outputs:\n",
        "    - Tokenized data\n",
        "    - Count of each token\n",
        "    - A dictionary that maps words to tokens\n",
        "    - A dictionary that maps tokens to words.\n",
        "    You can use the same function used in **Lab 3**, or else you can use [`tf.keras.Tokenizer`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) to write a similar function.\n",
        "- Print the first 10 tokens and reverse them to words to confirm a match to the initial print above.\n",
        "     \n",
        "  \n",
        "Example:  \n",
        "\n",
        " `corpus[:10] = ['this','is,'an','example',...]`\n",
        "\n",
        "`data[:10] = [44,26,24,16,...]`\n",
        "    \n",
        "`reversed_data =['this','is,'an','example',...]`\n",
        "\n",
        "**NOTE**: Choose a sufficiently large vocabulary size. i.e `vocab_size>= 1000`    \n",
        "<br />\n",
        "    \n",
        "**2.1.3 Build skipgrams with negative samples:**  \n",
        "- Use the `tf.keras.preprocessing.sequence.skipgrams` function to build positive and negative samples for word2vec training. Follow the documentation on how to make the pairs, or see Lab 3 for an example.\n",
        "- You are free to choose your own `window_size`, but we recommend a value of 3.\n",
        "- Print 10 pairs of *center* and *context* words with their associated labels.    \n",
        "    \n",
        "### Skip-gram Sampling table\n",
        "A large dataset means larger vocabulary with higher number of more frequent words such as stopwords. Training examples obtained from sampling commonly occurring words (such as *the*, *is*, *on*) don't add much useful information for the model to learn from. [Mikolov et al.](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) suggest subsampling of frequent words as a helpful practice to improve embedding quality.\n",
        "\n",
        "The `tf.keras.preprocessing.sequence.skipgrams` function accepts a sampling table argument to encode probabilities of sampling any token. You can use the `tf.keras.preprocessing.sequence.make_sampling_table` to generate a word-frequency rank based probabilistic sampling table and pass it to skipgrams function.    \n",
        "<br />\n",
        "    \n",
        "**2.1.4 Conceptual question** \n",
        "    \n",
        "What is the difference between using a sampling table and not using a sampling table while building the dataset for skipgrams?\n",
        "(Answer in less than 200 words)\n",
        "<br /><br />\n",
        "    \n",
        "### **2.2 [8 points] BUILDING A WORD2VEC MODEL** \n",
        "<br />\n",
        "    \n",
        "Build a word2vec model architecture based on the schematic below.\n",
        "<center>\n",
        "    <img src=\"https://i.ibb.co/1LyGb0J/Screen-Shot-2021-04-23-at-10-50-52-AM.png\" alt=\"centered image\" width=\"200\"/>\n",
        "</center>    \n",
        "    \n",
        "- To do so, you will need:\n",
        "    - `tf.keras.layers.Embedding` layer\n",
        "    - `tf.keras.layers.Dot()`\n",
        "    - `tf.keras.Model()` which is the functional API\n",
        "- You can choose an appropriate embedding dimension\n",
        "- Compile the model using `binary_crossentropy()` function and an appropriate optimizer. \n",
        "- Sufficiently train the model.    \n",
        "It is generally a good practise to save the model weights. To save the model weights using the `model.save_weights()` for analysis of **2.3**. (More information on how to save your weights can be found [here](https://www.tensorflow.org/tutorials/keras/save_and_load))\n",
        "<br />\n",
        "\n",
        "\n",
        "### **2.3 [7 points] POST-TRAINING ANALYSIS**\n",
        "\n",
        "    \n",
        "This segment involves some simple analysis of your trained embeddings.\n",
        "<br /><br />\n",
        "    \n",
        "**2.3.1 Vector Algebra on Embeddings**\n",
        "\n",
        "- Assuming you have chosen a sufficiently large `vocab_size`, find the embeddings for:\n",
        "    \n",
        "1. King\n",
        "2. Male\n",
        "3. Female\n",
        "4. Queen\n",
        "    \n",
        "- Find the vector `v = King - Male + Female` and find it's `cosine_similarity()` with the embedding for 'Queen'.\n",
        "You can use the `cosine_similarity()` function defind in the exercise from lecture.\n",
        "\n",
        "**NOTE**: The `cosine_similarity()` value, must be greater than `0.9`; If it is not, this implies that your word2vec embeddings are not well-trained.\n",
        "\n",
        "- Write a function `most_similar()`, which finds the top-n words most similar to the given word.\n",
        "- Use this function to find the words most similar to `king`.\n",
        "    \n",
        "- **Conceptual Question** Why can't we use `cosine_similarity()` as a `loss_function`?\n",
        "(Answer in less than 200 words) \n",
        "    \n",
        "<br />\n",
        "    \n",
        "**2.3.2 Visualizing Embeddings**\n",
        "\n",
        "- Find the embeddings for the words:\n",
        "1. 'Six'\n",
        "2. 'Seven'\n",
        "3. 'Eight'\n",
        "4. 'Nine'\n",
        "    \n",
        "- Find the `cosine_similarity()` of 'six' with each of 'seven`,'eight','nine'.\n",
        "    \n",
        "- Reset your network (make sure your trained weights are saved), and again compute the `cosine_similarity()` values. The values should be small (because the embeddings are random).\n",
        "    \n",
        "- Use a demonstrative plot to show the `before & after training` the 4 embeddings. Here are some suggestions: \n",
        "    1. PCA/TSNE for dimensionality reduction\n",
        "    2. Radar plot to show all embedding dimensions\n",
        "    \n",
        "Bonus points for using creative means to demonstrate how the embeddings change after training.\n",
        "\n",
        "Here is a [video](https://youtu.be/VDl_iA8m8u0) of a sample demonstration. We used a custom callback to get embeddings during training.  \n",
        "        \n",
        "\n",
        "<br />\n",
        "    \n",
        "**2.3.3 Embedding and Context Matrix**\n",
        "    \n",
        "<br />\n",
        "    \n",
        "Investigate the relation between the Embedding & Context matrix. Again use the `cosine_similarity()` function to find the average value across all the words in the embedding and context matrix, i.e:\n",
        "\n",
        "- For a word 'dog', find the embedding value, and context value.\n",
        "- Calculate the `cosine_similarity()` between the two\n",
        "- Repeat the same for every word in the vocabulary and calculate the average value of the `cosine_similarity()\n",
        " \n",
        "<br /><br />\n",
        "    \n",
        "\n",
        "### **2.4 [5 points] LEARNING PHRASES**\n",
        "    \n",
        "As per the original paper by [Mikolov et al]() many phrases have a meaning that is not a simple composition of the meanings of its individual words. \n",
        "For eg. `new york` is one entity, however, as per our analysis above, we have two separate entities `new` & `york` which can have different meanings independently.    \n",
        "To learn vector representation for phrases, we first find words that\n",
        "appear frequently together, and infrequently in other contexts.\n",
        "    \n",
        "As per the analysis in the paper, we can use a formula to rank commonly used word pairs, and take the first 100 commonly occuring pairs.\n",
        "$$\\operatorname{score}\\left(w_{i}, w_{j}\\right)=\\frac{\\operatorname{count}\\left(w_{i} w_{j}\\right)-\\delta}{\\operatorname{count}\\left(w_{i}\\right) \\times \\operatorname{count}\\left(w_{j}\\right)}$$\n",
        "\n",
        "**NOTE:** For simplicity of analysis, we take the discounting factor $\\delta$ as 0, and take bi-gram combinations. You can experiment with tri-grams for word pairs such as `New_York_Times`.     \n",
        "<br /><br />\n",
        "\n",
        "    \n",
        "**2.4.1 Find 100 most common bi-grams**\n",
        "\n",
        "- From the tokenized data above, find the count for each bigram pair.\n",
        "    \n",
        "- For each such pair, find the score associated with each token pair using the formula above.\n",
        "    \n",
        "- Pick the top 100 pairs based on the score (higher is better). To understand the `score()` function we suggest you read the paper mentioned above.\n",
        "    \n",
        "- Replace the original `text8` file with the pairs as one entity. For e.g., if `prime,minister` is a commonly occuring pair, replace `... prime minister ...' in the original corpus to a single entity `prime_minister`. Do this for all 100 pairs.\n",
        "<br /><br />\n",
        "    \n",
        "**2.4.2 Retrain word2vec**    \n",
        "- With the new corpus generated as above, build the dataset, use skipgrams and retrain your word2vec with a sufficiently large vocabulary.\n",
        "    \n",
        "- Use the `most_similar()` function defiend above to find the entities most similar to `united_kingdom`.\n",
        "    \n",
        "- Compare the above with separate tokens for `united` & `kingdom` and the sum of the vectors (to get this, you may need a sufficiently large vocabulary (>2000).\n",
        "<br /> <br />\n",
        "\n",
        "    \n",
        "### **2.5 [5 points] HOMEWORK QUIZ**\n",
        "<br />\n",
        "After attempting this part of the homework, answer the questions on edStem. All the questions depend on this part of the homework and you will not be able to answer them without attempting this part.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0dGMQ2iK-50"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Q2OJZ-eHvz"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "## **PART 2: Solutions**\n",
        "<br />\n",
        "    \n",
        "### **2.1 [5 points] MODEL PROCESSING**\n",
        "<br />\n",
        "    \n",
        "**2.1.1 Get the data:**    \n",
        "\n",
        "- Get the data from the `text8.zip` file.\n",
        "    `text8.zip` is a small, *cleaned* subset of a large corpus of data scraped from wikipedia pages.\n",
        "    It is usually used to quickly train, or test language models.\n",
        "    [Read here](http://mattmahoney.net/dc/textdata#:~:text=The%20purpose%20of%20the%20smaller,on%20the%20larger%20data%20set.&text=The%20two%20files%20have%20the,108%20bytes%20of%20fil9.) for more information\n",
        "- Split the data by whitespace print the first 10 words to check if has been correctly loaded.\n",
        "    \n",
        "**NOTE** : For this part of the homework, all words will be in their lowercase for simplicity of analysis\n",
        "<br />    <br />    \n",
        "\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iU--QnTeHv0"
      },
      "source": [
        "#### Helper function to read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lMX_3qYeHv0"
      },
      "source": [
        "# Helper code to read the data\n",
        "\n",
        "filename = '/content/drive/MyDrive/UnivAI/Univ AI 3/text8.zip'\n",
        "with zipfile.ZipFile(filename) as f:\n",
        "# Read the data into a list of strings.\n",
        "    vocabulary = tf.compat.as_str(f.read(f.namelist()[0])).split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzXogMFXeHv0"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.1.2 Build the dataset**  \n",
        "\n",
        "- Write a function that takes the `vocabulary_size` and `corpus` as input, and outputs:\n",
        "    - Tokenized data\n",
        "    - count of each token\n",
        "    - A dictionary that maps words to tokens\n",
        "    - A dictionary that maps tokens to words\n",
        "    You can use the same function used in **Lab 3**, or else you can use [`tf.keras.Tokenizer`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) to write a similar function.\n",
        "- Print the first 10 tokens and reverse them to words to confirm a match to the initial print above.\n",
        "     \n",
        "  \n",
        "Eg. `corpus[:10] = ['this','is,'an','example',...]`\n",
        "\n",
        "`data[:10] = [44,26,24,16,...]`\n",
        "    \n",
        "`reversed_data =['this','is,'an','example',...]`\n",
        "\n",
        "**NOTE**: Choose a sufficiently large vocabulary size. i.e `vocab_size>= 1000`    \n",
        "<br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jVkW-3VeHv0"
      },
      "source": [
        "def build_dataset(words, n_words):\n",
        "    \"\"\"Process raw inputs into a dataset.\"\"\" \n",
        "    # Fill in to complete this function \n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(Counter(words).most_common(n_words-1))\n",
        "    \n",
        "    dictionary={}\n",
        "    for i, tup in enumerate(count):\n",
        "        dictionary[tup[0]] = i\n",
        "           \n",
        "    data = []\n",
        "    unk_count = 0\n",
        "    \n",
        "    for word in words:\n",
        "        if word in dictionary:\n",
        "            token = dictionary[word]\n",
        "        else:\n",
        "            token = 0  \n",
        "            unk_count += 1\n",
        "            \n",
        "        data.append(token)\n",
        "        \n",
        "    count[0][1] = unk_count\n",
        "    \n",
        "    reversed_dictionary = {v:k for k,v in dictionary.items()}\n",
        "    \n",
        "    return data, count, dictionary, reversed_dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q2pa5Pv8AWl"
      },
      "source": [
        "def get_data_sequence(text, vocab_dict):\n",
        "  seq=[]\n",
        "  for word in text:\n",
        "    if word in vocab_dict.keys():\n",
        "      seq.append(vocab_dict[word])\n",
        "    else:\n",
        "      seq.append(vocab_dict['UNK'])\n",
        "  return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKyqEj6peHv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2453c4d-cf1b-462f-c047-b96ff40094b6"
      },
      "source": [
        "vocab_size = 3000\n",
        "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
        "                                                                vocab_size)\n",
        "\n",
        "data_sequence = get_data_sequence(vocabulary[0:10], dictionary)\n",
        "print(f'Original sentence is: \"{\" \".join(vocabulary[0:10])}\"')\n",
        "print(f'Tokenized sentence is: \"{data_sequence[0:10]}\"')\n",
        "print(f'Tokenized sentence reversed is: {\" \".join([reverse_dictionary[i] for i in data_sequence])}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original sentence is: \"anarchism originated as a term of abuse first used against\"\n",
            "Tokenized sentence is: \"[0, 0, 12, 6, 195, 2, 0, 46, 59, 156]\"\n",
            "Tokenized sentence reversed is: UNK UNK as a term of UNK first used against\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG26-aeq7C1A"
      },
      "source": [
        "#del vocabulary  # Hint to reduce memory."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm_kU0A2eHv0"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.1.3 Build skipgrams with negative samples:**  \n",
        "- Use the `tf.keras.preprocessing.sequence.skipgrams` function to build positive and negative samples \\\n",
        "    for word2vec training. Follow the documentation on how to make the pairs, or see Lab 3 for an example.\n",
        "- You are free to choose your own `window_size`, but we recommend a value of 3.\n",
        "- Print 10 pairs of *center* and *context* words with their associated labels.    \n",
        "    \n",
        "#### Skip-gram Sampling table\n",
        "A large dataset means larger vocabulary with higher number of more frequent words such as stopwords. Training examples obtained from sampling commonly occurring words (such as the, is, on) don't add much useful information for the model to learn from. [Mikolov et al.](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) suggest subsampling of frequent words as a helpful practice to improve embedding quality.\n",
        "\n",
        "The `tf.keras.preprocessing.sequence.skipgrams` function accepts a sampling table argument to encode probabilities of sampling any token. You can use the `tf.keras.preprocessing.sequence.make_sampling_table` to generate a word-frequency rank based probabilistic sampling table and pass it to skipgrams function.    \n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOA28WUPeHv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25d33f9-9b5b-45e6-fd31-4d602b9abc4d"
      },
      "source": [
        "# Your code here\n",
        "window_size = 3\n",
        "couples, labels = skipgrams(data,vocab_size, window_size=window_size, sampling_table=make_sampling_table(vocab_size))\n",
        "\n",
        "# Separate the target,context pairs as word_target, word_context \n",
        "word_center, word_context = zip(*couples)\n",
        "print(couples[:10], labels[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[455, 1045], [467, 939], [2110, 936], [2169, 1], [94, 1315], [687, 5], [1568, 46], [1909, 684], [259, 10], [829, 2347]] [1, 0, 0, 1, 0, 1, 1, 1, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TqcUZFReHv1"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.1.4 Conceptual question** \n",
        "    \n",
        "What is the difference between using a sampling table and not using a sampling table while building the dataset for skipgrams?\n",
        "(Answer in less than 200 words)\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOvNIpsYeHv1"
      },
      "source": [
        "#### Type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsqeGIxOeHv1"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.2 [15 points]** **Building a word2vec model:** \n",
        "\n",
        "Build a word2vec model architecture based on the schematic below.\n",
        "<center>\n",
        "    <img src=\"https://i.ibb.co/1LyGb0J/Screen-Shot-2021-04-23-at-10-50-52-AM.png\" alt=\"centered image\" width=\"200\"/>\n",
        "</center>    \n",
        "    \n",
        "- To do so, you will need:\n",
        "    - `tf.keras.layers.Embedding` layer\n",
        "    - `tf.keras.layers.Dot()`\n",
        "    - `tf.keras.Model()` which is the functional API\n",
        "- You can choose an appropriate embedding dimension\n",
        "- Compile the model using `binary_crossentropy()` function and an appropriate optimizer. \n",
        "- Sufficiently train the model.\n",
        "    - Your model will be sufficiently trained when? (Check with Ignacio)    \n",
        "    \n",
        "It is generally a good practise to save the model weights. To save the model weights using the `model.save_weights()` for analysis of **2.3**. (More information on how to save your weights can be found [here](https://www.tensorflow.org/tutorials/keras/save_and_load))\n",
        "\n",
        "\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buKn58NbeHv1"
      },
      "source": [
        "# Your code here\n",
        "def create_model():\n",
        "    \n",
        "  embedding_dim = 300\n",
        "\n",
        "  word_model = Sequential()\n",
        "  word_model.add(Embedding(vocab_size, embedding_dim, input_length=1, name=\"Embedding_1\"))\n",
        "  word_model.add(Reshape((embedding_dim, ))) \n",
        "\n",
        "  context_model = Sequential()\n",
        "  context_model.add(Embedding(vocab_size, embedding_dim, input_length=1, name=\"Embedding_2\"))\n",
        "  context_model.add(Reshape((embedding_dim, ))) \n",
        "\n",
        "  dot_product = dot([word_model.output, context_model.output],axes=1,\n",
        "                    normalize=False,name='dotproduct')\n",
        "\n",
        "  sigmoid_dot_product = Dense(1, activation=\"sigmoid\", name=\"Dense_Layer\")(dot_product)\n",
        "\n",
        "  model = Model(inputs=[word_model.input, context_model.input], outputs=sigmoid_dot_product, name=\"Model\")\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaIQ-IKi2dNz",
        "outputId": "837fe21d-216a-43bc-a885-c999015ef677"
      },
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Embedding_1_input (InputLayer)  [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding_2_input (InputLayer)  [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding_1 (Embedding)         (None, 1, 300)       900000      Embedding_1_input[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding_2 (Embedding)         (None, 1, 300)       900000      Embedding_2_input[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 300)          0           Embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 300)          0           Embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dotproduct (Dot)                (None, 1)            0           reshape[0][0]                    \n",
            "                                                                 reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Dense_Layer (Dense)             (None, 1)            2           dotproduct[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 1,800,002\n",
            "Trainable params: 1,800,002\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CB4RhwZB2Q3"
      },
      "source": [
        "model.compile(loss='bce',optimizer=optimizers.Adam(0.01), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS0ydLmxhBMg"
      },
      "source": [
        "import random\n",
        "\n",
        "def generate_data(word_center, word_context, labels):\n",
        "  batch_size=128\n",
        "  for i in range(0,len(labels),batch_size):\n",
        "    yield ([np.array(word_center[i:i+batch_size]), np.array(word_context[i:i+batch_size])], np.array(labels[i:i+batch_size]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxSoe49fmF6K"
      },
      "source": [
        "temp = list(zip(list(word_center), list(word_context), list(labels)))\n",
        "random.shuffle(temp)\n",
        "word_center_shuffled, word_context_shuffled, labels_shuffled = zip(*temp)\n",
        "generator = generate_data(word_center_shuffled, word_context_shuffled, labels_shuffled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD0825jPCFpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5ca3b4-5660-4713-d7a2-e1ff02c2c11b"
      },
      "source": [
        "#model.fit_generator(generator, epochs=10, steps_per_epoch=len(labels)//128, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "109248/109248 [==============================] - 2085s 19ms/step - loss: 0.4165 - accuracy: 0.8153\n",
            "Epoch 2/10\n",
            "     1/109248 [..............................] - ETA: 32:27 - loss: 0.1707 - accuracy: 0.9286WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1092480 batches). You may need to use the repeat() function when building your dataset.\n",
            "109248/109248 [==============================] - 0s 0us/step - loss: 0.1707 - accuracy: 0.9286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc8cb4f93d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68RF1_g5Y_3_"
      },
      "source": [
        "#model.save_weights('/content/drive/MyDrive/UnivAI/Univ AI 3/word2vec_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTEJIshIQSWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6962cdbd-7f85-4e00-924e-d688a2d118b6"
      },
      "source": [
        "# Create a new model instance\n",
        "model = create_model()\n",
        "\n",
        "# Restore the weights\n",
        "model.load_weights('/content/drive/MyDrive/UnivAI/Univ AI 3/word2vec_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f81f8b1f990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0yyMVPCeHv1"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **2.3 [7 points] POST-TRAINING ANALYSIS**\n",
        "<br />\n",
        "    \n",
        "This segment involves some simple analysis of your trained embeddings.\n",
        "<br />\n",
        "    \n",
        "**2.3.1Vector Algebra on Embeddings**\n",
        "\n",
        "Assuming you have chosen a sufficiently large `vocab_size`, find the embeddings for:\n",
        "    \n",
        "1. King\n",
        "2. Male\n",
        "3. Female\n",
        "4. Queen\n",
        "    \n",
        "Find the vector `v = King - Male + Female` and find it's `cosine_similarity()` with the embedding for 'Queen'.\n",
        "You can use the `cosine_similarity()` function defind in session 3 exercise.\n",
        "\n",
        "**NOTE**:The `cosine_similarity()` value, must be greater than `0.9`; If it is not, this implies that your word2vec embeddings are not well-trained.\n",
        "\n",
        "Write a function `most_similar()`, which finds the top-n words most similar to the given word.\n",
        "    - Use this function to find the words most similar to `king`.\n",
        "    \n",
        "**Conceptual Question** Why can't we use `cosine_similarity()` as a `loss_function`?\n",
        "(Answer in less than 200 words) \n",
        "    \n",
        "<br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fa_rv-oeHv2"
      },
      "source": [
        "# Your code here\n",
        "def cosine_similarity(vectorA,vectorB):\n",
        "    return np.dot(vectorA,vectorB.T)/(np.linalg.norm(vectorA)*np.linalg.norm(vectorB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfQodJOW3eaY"
      },
      "source": [
        "embeddings = model.get_layer('Embedding_1').get_weights()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqX8n-XP4ck6"
      },
      "source": [
        "king_vector = embeddings[dictionary['king']]\n",
        "male_vector = embeddings[dictionary['male']]\n",
        "female_vector = embeddings[dictionary['female']]\n",
        "queen_vector = embeddings[dictionary['queen']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6TmXcLn6N6o",
        "outputId": "4ccec60d-7c94-4313-f03f-2c17a5a55cdc"
      },
      "source": [
        "v = king_vector - male_vector + female_vector\n",
        "similarity_v_queen = cosine_similarity(v, queen_vector)\n",
        "similarity_v_queen"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.39938477"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1xnZpRv61-U"
      },
      "source": [
        "def most_similar(word):\n",
        "  word_vector = embeddings[dictionary[word]]\n",
        "  cosine_values = {k:cosine_similarity(embeddings[v],word_vector) for k,v in dictionary.items() if k not in [word]}\n",
        "  closest = max(cosine_values,key =lambda x: cosine_values[x])\n",
        "  return closest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EHWg9Bjg8nVN",
        "outputId": "c9e3a152-2e9f-4b2c-969f-736832fd6b22"
      },
      "source": [
        "most_similar('king')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'son'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtfmUBsFeHv2"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "\n",
        "**2.3.2 Visualizing Embeddings**\n",
        "\n",
        "Find the embeddings for the words:\n",
        "1. 'Six'\n",
        "2. 'Seven'\n",
        "3. 'Eight'\n",
        "4. 'Nine'\n",
        "    \n",
        "Find the `cosine_similarity()` of 'six' with each of 'seven`,'eight','nine' (which should be high values).\n",
        "    \n",
        "Reset your network (make sure your trained weights are saved), and again compute the `cosine_similarity()` values. The values should be small (because the embeddings are random).\n",
        "    \n",
        "Use a demonstrative plot to show the `before & after training` of the 4 embeddings. Here are some suggestions: \n",
        "    1. PCA/TSNE for dimensionality reduction\n",
        "    2. Radar plot to show all embedding dimensions\n",
        "    \n",
        "Bonus points for using creative means to demonstrate how the embeddings change after training.\n",
        "\n",
        "        \n",
        "\n",
        "<br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3623j7W19a5m",
        "outputId": "328d5ff5-7d8a-4eef-94ff-dca5d8554189"
      },
      "source": [
        "six_vector= embeddings[dictionary['six']]\n",
        "seven_vector= embeddings[dictionary['seven']]\n",
        "eight_vector= embeddings[dictionary['eight']]\n",
        "nine_vector= embeddings[dictionary['nine']]\n",
        "\n",
        "cos_sim_six_seven = cosine_similarity(seven_vector, six_vector)\n",
        "cos_sim_six_eight = cosine_similarity(eight_vector, six_vector)\n",
        "cos_sim_six_nine = cosine_similarity(nine_vector, six_vector)\n",
        "\n",
        "cos_sim_six_seven, cos_sim_six_eight, cos_sim_six_nine"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7515314, 0.73684996, 0.67870075)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q74NNmM_HAk"
      },
      "source": [
        "new_model = create_model()\n",
        "new_model.compile(loss='bce',optimizer='adam', metrics=['accuracy'])\n",
        "initial_embeddings = new_model.get_layer('Embedding_1').get_weights()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc1VegylADIu"
      },
      "source": [
        "### TODO: Radar or TSNE plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cApSXEheHv2"
      },
      "source": [
        "#### Here is a [video](https://youtu.be/VDl_iA8m8u0) of a sample demonstration. We used a custom callback to get embeddings during training.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1irhva4AeHv2"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.3.3 Embedding and Context Matrix**\n",
        "    \n",
        "<br />\n",
        "    \n",
        "Investigate the relation between the Embedding & Context matrix. Again use the `cosine_similarity()` function to find the average value across all the words in the embedding and context matrix, i.e:\n",
        "    - For a word 'dog', find the embedding value, and context value.\n",
        "    - Calculate the `cosine_similarity()` between the two\n",
        "    - Repeat the same for every word in the vocabulary and calculate the average value of the `cosine_similarity()`\n",
        "\n",
        "<br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-87UALh0eHv2"
      },
      "source": [
        "# Your code here\n",
        "context_matrix = model.get_layer('Embedding_2').get_weights()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgjHAsZZCbGR",
        "outputId": "34361b13-7683-4407-cf41-d64a46be5a25"
      },
      "source": [
        "cos_sum = 0\n",
        "for _,v in dictionary.items():\n",
        "  idx = v\n",
        "  embedding_vector = embeddings[idx]\n",
        "  context_vector = context_matrix[idx]\n",
        "  cos_sum =  cos_sum + float(cosine_similarity(embedding_vector, context_vector))\n",
        "\n",
        "avg_value = cos_sum/vocab_size\n",
        "avg_value"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.1888070660606342"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhBrIaL7eHv3"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **2.4 [5 points] LEARNING PHRASES**\n",
        "    \n",
        "As per the original paper by [Mikolov et al]() many phrases have a meaning that is not a simple composition of the meanings of its individual words. \n",
        "For eg. `new york` is one entity, however, as per our analysis above, we have two separate entities `new` & `york` which can have different meanings independently.    \n",
        "To learn vector representation for phrases, we first find words that\n",
        "appear frequently together, and infrequently in other contexts.\n",
        "    \n",
        "As per the analysis in the paper, we can use a formula to rank commonly used word pairs, and take the first 100 commonly occuring pairs.\n",
        "$$\\operatorname{score}\\left(w_{i}, w_{j}\\right)=\\frac{\\operatorname{count}\\left(w_{i} w_{j}\\right)-\\delta}{\\operatorname{count}\\left(w_{i}\\right) \\times \\operatorname{count}\\left(w_{j}\\right)}$$\n",
        "\n",
        "**NOTE:** For simplicity of analysis, we take the discounting factor $\\delta$ as 0, and take bi-gram combinations. You can experiment with tri-grams for word pairs such as `New_York_Times`.     \n",
        "<br /><br />\n",
        "\n",
        "    \n",
        "**2.4.1 Find 100 most common bi-grams**\n",
        "\n",
        "From the tokenized data above, find the count for each bigram pair.\n",
        "    \n",
        "For each such pair, find the score associated with each token pair using the formula above.\n",
        "    \n",
        " Pick the top 100 pairs based on the score. (Higher the better). To understand the `score()` function we suggest you read the paper mentioned above.\n",
        "    \n",
        "Replace the original `text8` file with the pairs as one entity. For e.g., if `prime,minister` is a commonly occuring pair, replace `... prime minister ...' in the original corpus to a single entity `prime_minister`. Do this for all 100 pairs.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o0UGlHNeHv3"
      },
      "source": [
        "# Uncomment this cell to download the dataset directly onto colab\n",
        "# !gdown https://drive.google.com/uc?id=1yW-rQc8It9Ro0DAu4jp3XhVH3JQOdNKk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sObDbIQYeHv3"
      },
      "source": [
        "# Your code here\n",
        "# Your code here\n",
        "def get_bigrams(data):\n",
        "  bigrams = []\n",
        "  bigrams.extend([(data[i], data[i+1]) for i, seq in enumerate(data) if i<len(data)-1])\n",
        "  bigrams_count_dict = Counter(bigrams)\n",
        "  return bigrams_count_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9tZELhPQFVe"
      },
      "source": [
        "def bigram_score(bigrams_count_dict, count, reverse_dictionary):\n",
        "  scores={}\n",
        "  count_dict = dict(count)\n",
        "  for k,v in bigrams_count_dict.items():\n",
        "    count_k = v\n",
        "    count_k_0 = count_dict[reverse_dictionary[k[0]]]\n",
        "    count_k_1 = count_dict[reverse_dictionary[k[1]]]\n",
        "    score = count_k / (count_k_0 * count_k_1)\n",
        "    scores[k] = score\n",
        "\n",
        "  return scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsgZ6MSy58V5"
      },
      "source": [
        "bigrams_count_dict = get_bigrams(data)\n",
        "bigram_scores = bigram_score(bigrams_count_dict, count,reverse_dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ileFapcaHE2Z"
      },
      "source": [
        "import more_itertools as mi\n",
        "a = {k: v for k, v in sorted(bigram_scores.items(), key=lambda item: item[1], reverse=True)}\n",
        "n_items = mi.take(100, a.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uMogXlyeHv3"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.4.2 Retrain word2vec**    \n",
        "With the new corpus generated as above, build the dataset, use skipgrams and retrain your word2vec with a sufficiently large vocabulary.\n",
        "    \n",
        "Use the `most_similar()` function defiend above to find the entities most similar to `united_kingdom`\n",
        "    \n",
        "Compare the above with separate tokens for `united` & `kingdom` and the sum of the vectors (to get this, you may need a sufficiently large vocabulary (>2000).\n",
        "<br /> <br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFbEX2_1eHv3"
      },
      "source": [
        "# Your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtdjuYgIeHv3"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **2.5 [5 points] HOMEWORK QUIZ**\n",
        "<br />\n",
        "After attempting this part of the homework, answer the questions on edStem. All the questions depend on this part of the homework and you will not be able to answer them without attempting this part.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTHaqVymeHv3"
      },
      "source": [
        "#### Answer the questions on edStem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_ktf1AweHv4"
      },
      "source": [
        "___\n",
        "___\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUtUZIgCeHv4"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "## **PART 3 [35 points] : Language Modelling using RNNs**\n",
        "<br />    \n",
        "\n",
        "In the last part of the homework, you are expected to build and train a language model. For this, we will be using Pavlos famous texts which end with `...` for prediction. Here, you will preprocess a data corpus and train your simple RNN network with it. With this network you will try to predict what he meant when he typed `...`. This can be to some extent a form of transfer learning.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBnVsppEeHv4"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "## **PART 3: Questions**\n",
        "<br />\n",
        "\n",
        "### **3.1 [2 points] PREPROCESS THE DATASET**\n",
        "<br />\n",
        "\n",
        "**3.1.1** - Read in the dataset `imdb.csv`. Create a new dataframe by splitting each review into individual sentences. The sentences can be delimited by different characters such as period and question mark (eroteme). Call this column as `text` in the new dataframe.\n",
        "<br /><br />\n",
        "\n",
        "**3.1.2** - Define a function `clean_data` that takes the new dataframe as input and removes all html tags and non-alphabetic characters from the dataframe. Additionally, convert all characters to lower case. Remove all the sentences where the number of words is less than 10 and higher than 30. Finally, add the start token `<s>` and the end token `</s>` to every sentence (row) in the dataframe. Return the processed the dataframe. \n",
        "<br /><br />\n",
        "\n",
        "### **3.2 [2 points] TOKENIZE THE DATASET**\n",
        "<br />\n",
        "\n",
        "**3.2.1** - Tokenize the dataset using `tensorflow.keras.preprocessing.text` with a vocabulary size of 5000. Do **not** add an additional token for unknown words (out of vocabulary words) `<UNK>`.\n",
        "<br /><br />\n",
        "\n",
        "**3.2.2** - Fit the tokenizer on the dataset and get the sequence representation of each sentence.\n",
        "<br /><br />\n",
        "\n",
        "### **3.3 [10 points] MODELLING THE DATA**\n",
        "<br />\n",
        "\n",
        "**3.3.1** - The first step is to define the input and output of the model. The input to the model is all the words of the sentences except the last one. \n",
        "The output of the model is all words of the sentences except the first one. Using `tf.keras.preprocessing.sequence.pad_sequences` Post-pad all the sentences of the input and output to a length of 30.\n",
        "<br /><br />\n",
        "\n",
        "**3.3.2** - Define a simple RNN model that has an embedding layer with an embedding dimension of 300. You can define any number of RNN layers. The output of the RNN model will be a dense layer with size of the vocabulary and softmax activation. This model is that one you will be using to train the network. Use functional API for this to reuse it later on.\n",
        "<br /><br />\n",
        "\n",
        "**3.3.3** - Train the model with the input and output data formed above and a validation split of 0.2. The number epochs and batch size is left as a choice you have to make.\n",
        "<br /><br />\n",
        "\n",
        "**3.3.4** - Plot the train and validation loss. \n",
        "<br /><br />\n",
        "\n",
        "### **3.4 [9 points] PREDICTING THE NEXT WORD**\n",
        "<br />\n",
        "\n",
        "**3.4.1** - Read the dataset `pp_text.csv`. Add the start and end tokens to each line and tokenize it. Convert each sentence to a sequence vector and post-pad to a length of 30. This will be the input for the prediction phase.\n",
        "<br /><br />\n",
        "\n",
        "**3.4.2** - For predicting the next word, use the trained RNN model from above. \n",
        "\n",
        "NOTE - Based on your implementation, the output of the RNN model might have to be different from that of your trained network. You can make use of Keras function API for this.\n",
        "<br /><br />\n",
        "\n",
        "**3.4.3** - Choose any sentence from the list of Pavlos' texts to predict the next word. Input this to the RNN model built for prediction and print the predicted word. Try this out with multiple sentences.\n",
        "<br /><br />\n",
        "\n",
        "**3.4.4** - Do you notice any pattern in the predicted words? Do they seem approriate to the context of the texts as you understand it? What do you attribute this discrepency to? How can you resolve it?\n",
        "\n",
        "Answer in less than 150 words.\n",
        "<br /><br />\n",
        "\n",
        "### **3.5 [6 points] TRAINING AND PREDICTING WITH A DIFFERENT DATASET**\n",
        "<br />\n",
        "\n",
        "**3.5.1** - Read the dataset `cleaned_sarcasm.csv`. This dataset has been preprocessed for you, all you need to do is tokenize, convert to sequence and pad it, similar to 3.2.1, 3.2.2 and 3.3.1.\n",
        "<br /><br />\n",
        "\n",
        "**3.5.2** - Train your RNN model with this data and plot the train and validation trace plot. This part is similar to 3.3.2, 3.3.3 and 3.3.4.\n",
        "<br /><br />\n",
        "\n",
        "**3.5.3** - Repeat 3.4.1, 3.4.2 and 3.4.3 with the RNN model trained using the new dataset.\n",
        "<br /><br />\n",
        "\n",
        "**3.5.4** - How do the results with the new dataset compare to the previous ones? Why do you think so? \n",
        "\n",
        "Answer in less than 100 words.\n",
        "<br /><br />\n",
        "    \n",
        "### **3.6 [3 points] COMPLETING THE SENTENCE**\n",
        "<br />\n",
        "\n",
        "**3.6.1** Until now we have predicted a single word for a given sentence. However, what if he meant more than one word when he typed in `...`\n",
        "\n",
        "We will now predict multiple words for each input sentence. To do this we will first predict one word, append this word to the input text and then predict one more with the updated input. Continue doing this for 5 words or until the end token `</s>` (whichever comes first). \n",
        "<br /><br />\n",
        "\n",
        "### **3.7 [3 points] HOMEWORK QUIZ**\n",
        "<br />\n",
        "After attempting this part of the homework, answer the questions on edStem. All the questions depend on this part of the homework and you will not be able to answer them without attempting this part.\n",
        "\n",
        "<br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVT4R2UieHv5"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "## **PART 3: Solutions**\n",
        "\n",
        "<br />\n",
        "\n",
        "### **3.1 [2 points] PREPROCESS THE DATASET**\n",
        "<br />    \n",
        "\n",
        "**3.1.1** - Read in the dataset `imdb.csv`. Create a new dataframe by splitting each review into individual sentences. The sentences can be delimited by different characters such as period and question mark (eroteme). Call this column as `text` in the new dataframe.\n",
        "    \n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyFz6C4oeHv5"
      },
      "source": [
        "*If you are using colab, the code in the next cell will help download the dataset directly onto your workspace.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPhpPLCieHv5"
      },
      "source": [
        "# Uncomment if you are using colab\n",
        "# !gdown --id \"1YmxaDY-VhGItJ5uRZKtHoEqpqAR1L2IY\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "358m9yIheHv5"
      },
      "source": [
        "# Your code here\n",
        "imdb_data_path = \"/content/drive/MyDrive/UnivAI/Univ AI 3/imdb.csv\"\n",
        "imdb_data = pd.read_csv(imdb_data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "-_C56JXFwhZW",
        "outputId": "ade7dc9d-c54b-448e-b8b8-aff548d09b6d"
      },
      "source": [
        "imdb_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>first think another Disney movie, might good, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>big fan Stephen King's work, film made even gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  polarity\n",
              "0  first think another Disney movie, might good, ...         1\n",
              "1  Put aside Dr. House repeat missed, Desperate H...         0\n",
              "2  big fan Stephen King's work, film made even gr...         1\n",
              "3  watched horrid thing TV. Needless say one movi...         0\n",
              "4  truly enjoyed film. acting terrific plot. Jeff...         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p_fpCzDw37i"
      },
      "source": [
        "def split_text(df_text):\n",
        "  new_df = pd.DataFrame(columns=['text'])\n",
        "  for text in df_text:\n",
        "    sent_df = pd.DataFrame(re.split(r'[.\\?!;]\\s',text), columns=['text'])\n",
        "    new_df = new_df.append(sent_df, ignore_index=True)\n",
        "  return new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekZLv3QzJ5dM"
      },
      "source": [
        "new_df = split_text(imdb_data.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xabq_uQNKBgt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c30651be-0e2e-4d89-c0a6-51dd5dc30fb1"
      },
      "source": [
        "new_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>first think another Disney movie, might good, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>watch it, can't help enjoy it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ages love movie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>first saw movie 10 8 years later still love it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Danny Glover superb could play part better</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  first think another Disney movie, might good, ...\n",
              "1                      watch it, can't help enjoy it\n",
              "2                                    ages love movie\n",
              "3     first saw movie 10 8 years later still love it\n",
              "4         Danny Glover superb could play part better"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMRqTd4UeHv5"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.1.2** - Define a function `clean_data` that takes the new dataframe as input and removes all html tags and non-alphabetic characters from the dataframe. Additionally, convert all characters to lower case. Remove all the sentences where the number of words is less than 10 and higher than 30. Finally, add the start token `<s>` and the end token `</s>` to every sentence (row) in the dataframe. Return the processed the dataframe. \n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYUWrmSreHv5"
      },
      "source": [
        "# Your code here\n",
        "def clean_data(df):\n",
        "  clean_df = pd.DataFrame(columns=['text'])\n",
        "  for text in df.text:\n",
        "    clean_text = text.lower()\n",
        "    # remove special characters - basically anything that is not a letter or a space\n",
        "    clean_text = re.sub(r'[^a-z\\s]+','', clean_text)\n",
        "    clean_text = pd.DataFrame(['<s> '+clean_text+' </s>'], columns=['text'])\n",
        "    clean_df = clean_df.append(clean_text, ignore_index=True)\n",
        "  return clean_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znJR6eKGM44v"
      },
      "source": [
        "clean_df = clean_data(new_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzj2P1oANBP4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c43b4e39-f7f0-4db9-e2cf-6ab922e2023f"
      },
      "source": [
        "clean_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;s&gt; first think another disney movie might goo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;s&gt; watch it cant help enjoy it &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;s&gt; ages love movie &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;s&gt; first saw movie   years later still love i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;s&gt; danny glover superb could play part better...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  <s> first think another disney movie might goo...\n",
              "1               <s> watch it cant help enjoy it </s>\n",
              "2                           <s> ages love movie </s>\n",
              "3  <s> first saw movie   years later still love i...\n",
              "4  <s> danny glover superb could play part better..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH5IE0zMeHv6"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "### **3.2 [2 points] TOKENIZE THE DATASET**\n",
        "    \n",
        "<br />\n",
        "\n",
        "**3.2.1** - Tokenize the dataset using `tensorflow.keras.preprocessing.text` with a vocabulary size of 5000. Do **not** add an additional token for unknown words (out of vocabulary words) `<UNK>`.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4HBCSUoeHv6"
      },
      "source": [
        "tok = Tokenizer(num_words=5000, filters='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQZZ3qlv4Jak"
      },
      "source": [
        "**3.2.2** - Fit the tokenizer on the dataset and get the sequence representation of each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qB06Vfj31I7"
      },
      "source": [
        "# Your code here\n",
        "seq_df = pd.DataFrame()\n",
        "tok.fit_on_texts(clean_df.text)\n",
        "seq_df['text'] =tok.texts_to_sequences(clean_df.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "bMnflDXN3-lg",
        "outputId": "e85dc3be-63e3-42d3-e6aa-e599f9e63540"
      },
      "source": [
        "seq_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 25, 33, 75, 823, 4, 139, 9, 8, 266, 4, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 35, 15, 89, 239, 259, 15, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 1851, 45, 4, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 25, 113, 4, 63, 193, 50, 45, 15, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 1616, 3407, 830, 30, 207, 85, 51, 2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            text\n",
              "0  [1, 25, 33, 75, 823, 4, 139, 9, 8, 266, 4, 2]\n",
              "1               [1, 35, 15, 89, 239, 259, 15, 2]\n",
              "2                            [1, 1851, 45, 4, 2]\n",
              "3        [1, 25, 113, 4, 63, 193, 50, 45, 15, 2]\n",
              "4       [1, 1616, 3407, 830, 30, 207, 85, 51, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts5vGMYL4NPj"
      },
      "source": [
        "### **3.3 [10 points] MODELLING THE DATA** **3.3.1** - The first step is to define the input and output of the model. The input to the model is all the words of the sentences except the last one. The output of the model is all words of the sentences except the first one. Using `tf.keras.preprocessing.sequence.pad_sequences` Post-pad all the sentences of the input and output to a length of 30."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVtYDp6j5V_3"
      },
      "source": [
        "def remove_a_word(text_seq, first=False):\n",
        "  if first:\n",
        "    text_seq=text_seq[2:]\n",
        "  else:\n",
        "    text_seq=text_seq[:-2]\n",
        "  return text_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXrNyFX25-XQ"
      },
      "source": [
        "X_train = seq_df.text.apply(remove_a_word)\n",
        "y_train = seq_df.text.apply(remove_a_word, first=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqOkNG6L6SYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38513963-3c56-495b-a5b8-117489bf0b66"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [1, 25, 33, 75, 823, 4, 139, 9, 8, 266]\n",
              "1                  [1, 35, 15, 89, 239, 259]\n",
              "2                              [1, 1851, 45]\n",
              "3           [1, 25, 113, 4, 63, 193, 50, 45]\n",
              "4          [1, 1616, 3407, 830, 30, 207, 85]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzL3AJB_6ZaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c0e210-a23d-48c4-ec9f-0cca4ac13754"
      },
      "source": [
        "y_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [33, 75, 823, 4, 139, 9, 8, 266, 4, 2]\n",
              "1                 [15, 89, 239, 259, 15, 2]\n",
              "2                                [45, 4, 2]\n",
              "3          [113, 4, 63, 193, 50, 45, 15, 2]\n",
              "4           [3407, 830, 30, 207, 85, 51, 2]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAaydGZksCYp"
      },
      "source": [
        "X_train = X_train.values\n",
        "y_train = y_train.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN4ihSWVeHv6"
      },
      "source": [
        "# Your code here\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=30, padding='post')\n",
        "y_train = sequence.pad_sequences(y_train, maxlen=30, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ac0fUsweHv6"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "    \n",
        "**3.3.2** - Define a simple RNN model that has an embedding layer with an embedding dimension of 300. You can define any number of RNN layers. The output of the RNN model will be a dense layer with size of the vocabulary and softmax activation. This model is that one you will be using to train the network. Use functional API for this to reuse it later on.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIwhdbqFHHPe",
        "outputId": "bc445e2b-0d35-467d-fb98-b4efdc66b4db"
      },
      "source": [
        "X_train[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,  35,  15,  89, 239, 259,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76_vGQMfHDjn",
        "outputId": "f352c4bc-fef1-48bc-a20a-75f7807789a9"
      },
      "source": [
        "y_train[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 15,  89, 239, 259,  15,   2,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg1SCDaEJIn0",
        "outputId": "5cfa5290-4ed6-4d20-b624-b8f3083b2ede"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(112889, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyaGFC2KJTjw",
        "outputId": "315f1c0f-7a8f-4622-8d65-85177dc336cf"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(112889, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjGaVj7zK0Gj"
      },
      "source": [
        "embedding_size=300\n",
        "vocabulary_size = 5000\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(vocabulary_size, embedding_size, mask_zero=True, name='embedding'))\n",
        "model.add(SimpleRNN(128, return_sequences=True, name='RNN_1'))\n",
        "model.add(SimpleRNN(64, return_sequences=True, name='RNN_2'))\n",
        "model.add(Dense(vocabulary_size, activation='softmax', name='dense'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnpVL8Cfb5ug"
      },
      "source": [
        "model.compile(optimizer=\"Adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1VdCugNcIQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "385900f3-fb96-4d7e-b631-eef908dfa461"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 300)         1500000   \n",
            "_________________________________________________________________\n",
            "RNN_1 (SimpleRNN)            (None, None, 128)         54912     \n",
            "_________________________________________________________________\n",
            "RNN_2 (SimpleRNN)            (None, None, 64)          12352     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 5000)        325000    \n",
            "=================================================================\n",
            "Total params: 1,892,264\n",
            "Trainable params: 1,892,264\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H9LiwlzeHv7"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.3.3** - Train the model with the input and output data formed above and a validation split of 0.2. The number epochs and batch size is left as a choice you have to make.\n",
        "<br /><br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpGhdkjPeHv7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc2279f-51b7-4ead-be41-a939778128c8"
      },
      "source": [
        "# Your code here\n",
        "epochs = 10\n",
        "history = model.fit(X_train,y_train, epochs=epochs, validation_split=0.2, batch_size=64,verbose=1);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1412/1412 [==============================] - 135s 95ms/step - loss: 2.1945 - accuracy: 0.1054 - val_loss: 2.1079 - val_accuracy: 0.1081\n",
            "Epoch 2/10\n",
            "1412/1412 [==============================] - 133s 94ms/step - loss: 2.1268 - accuracy: 0.1083 - val_loss: 2.0991 - val_accuracy: 0.1089\n",
            "Epoch 3/10\n",
            "1412/1412 [==============================] - 134s 95ms/step - loss: 2.1003 - accuracy: 0.1094 - val_loss: 2.0898 - val_accuracy: 0.1094\n",
            "Epoch 4/10\n",
            "1412/1412 [==============================] - 134s 95ms/step - loss: 2.0948 - accuracy: 0.1098 - val_loss: 2.0869 - val_accuracy: 0.1093\n",
            "Epoch 5/10\n",
            "1412/1412 [==============================] - 133s 94ms/step - loss: 2.0732 - accuracy: 0.1103 - val_loss: 2.0876 - val_accuracy: 0.1090\n",
            "Epoch 6/10\n",
            "1412/1412 [==============================] - 135s 96ms/step - loss: 2.0590 - accuracy: 0.1102 - val_loss: 2.0907 - val_accuracy: 0.1092\n",
            "Epoch 7/10\n",
            "1412/1412 [==============================] - 134s 95ms/step - loss: 2.0499 - accuracy: 0.1103 - val_loss: 2.0962 - val_accuracy: 0.1091\n",
            "Epoch 8/10\n",
            "1412/1412 [==============================] - 134s 95ms/step - loss: 2.0226 - accuracy: 0.1111 - val_loss: 2.1019 - val_accuracy: 0.1092\n",
            "Epoch 9/10\n",
            "1412/1412 [==============================] - 134s 95ms/step - loss: 2.0211 - accuracy: 0.1107 - val_loss: 2.1098 - val_accuracy: 0.1088\n",
            "Epoch 10/10\n",
            "1412/1412 [==============================] - 133s 94ms/step - loss: 2.0056 - accuracy: 0.1111 - val_loss: 2.1186 - val_accuracy: 0.1079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPp20qinL-yM",
        "outputId": "58d3f1ab-3c71-4a06-87c8-a20a14236fcb"
      },
      "source": [
        "model.save('/content/drive/MyDrive/UnivAI/Univ AI 3/models/rnn_10epochs_fixed')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/UnivAI/Univ AI 3/models/rnn_10epochs_fixed/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCGSGbNceHv7"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.3.4** - Plot the train and validation loss. \n",
        "\n",
        "<br /><br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BHWuvQbzeHv7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "2fef11a4-7124-40f2-ed71-62a12fc5c1ab"
      },
      "source": [
        "# Your code here\n",
        "def plot_history(history, name):\n",
        "    fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
        "    for i, metric in enumerate(['loss', 'accuracy']): \n",
        "        ax[i].plot(history.history[metric], label='Train')\n",
        "        ax[i].plot(history.history['val_'+metric], label='Validation')\n",
        "        if metric == 'accuracy': ax[i].axhline(0.5, c='r', ls='--', label='Trivial accuracy')\n",
        "        ax[i].set_xlabel('Epoch')\n",
        "        ax[i].set_ylabel(metric)\n",
        "    plt.suptitle(f'{name} Training', y=1.05)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "plot_history(history, 'RNN')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlizWM9CeHv7"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **3.4 [9 points] PREDICTING THE NEXT WORD**\n",
        "    \n",
        "<br />\n",
        "    \n",
        "**3.4.1** - Read the dataset `pp_text.csv`. Add the start and end tokens to each line and tokenize it. Convert each sentence to a sequence vector and post-pad to a length of 30. This will be the input for the prediction phase.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EdGafL1eHv7"
      },
      "source": [
        "*If you are using colab, the code in the next cell will help download the dataset directly onto your workspace.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIE1oIaBeHv7"
      },
      "source": [
        "# Uncomment if you are using colab\n",
        "# !gdown --id \"1xeQ4w0iYJimzth0e3t3dQJbeuCsFlxVa\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGDI42Rfy6f7"
      },
      "source": [
        "model = load_model('/content/drive/MyDrive/UnivAI/Univ AI 3/models/rnn_20epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBGWgunAeHv8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d0a78aa5-88a4-4541-e60d-5109df657482"
      },
      "source": [
        "# Your code here\n",
        "pp_text_df = pd.read_csv(\"/content/drive/MyDrive/UnivAI/Univ AI 3/pp_text.csv\")\n",
        "pp_text_df.columns=['text']\n",
        "pp_text_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When you make changes please always check if e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I found them very similar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Give some overview of what the exercises are a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Honestly I do not remember</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Can you check the video</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  When you make changes please always check if e...\n",
              "1                          I found them very similar\n",
              "2  Give some overview of what the exercises are a...\n",
              "3                         Honestly I do not remember\n",
              "4                            Can you check the video"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "GyES7IllOl62",
        "outputId": "6267d187-1856-4f24-b121-6adbf4a90eff"
      },
      "source": [
        "pp_text_clean = clean_data(pp_text_df)\n",
        "pp_text_clean.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;s&gt; when you make changes please always check ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;s&gt; i found them very similar &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;s&gt; give some overview of what the exercises a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;s&gt; honestly i do not remember &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;s&gt; can you check the video &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  <s> when you make changes please always check ...\n",
              "1                 <s> i found them very similar </s>\n",
              "2  <s> give some overview of what the exercises a...\n",
              "3                <s> honestly i do not remember </s>\n",
              "4                   <s> can you check the video </s>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HUy-KkfOxXc"
      },
      "source": [
        "pp_text_seq_df=pd.DataFrame()\n",
        "pp_text_seq_df['text'] = tok.texts_to_sequences(pp_text_clean.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "wuy7vOP7PUR8",
        "outputId": "e4d01c27-f5b5-4137-eea8-215585fcb36d"
      },
      "source": [
        "pp_text_seq_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 1041, 232, 29, 1229, 511, 115, 774, 374, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 53, 159, 191, 1503, 695, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 102, 1267, 597, 434, 10, 872, 95, 648, 148...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 1175, 53, 466, 302, 280, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 1913, 232, 774, 10, 331, 2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  [1, 1041, 232, 29, 1229, 511, 115, 774, 374, 1...\n",
              "1                    [1, 53, 159, 191, 1503, 695, 2]\n",
              "2  [1, 102, 1267, 597, 434, 10, 872, 95, 648, 148...\n",
              "3                    [1, 1175, 53, 466, 302, 280, 2]\n",
              "4                    [1, 1913, 232, 774, 10, 331, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br5MBr-aeHv8"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "\n",
        "**3.4.2** - For predicting the next word, use the trained RNN model from above. \n",
        "\n",
        "NOTE - Based on your implementation, the output of the RNN model might have to be different from that of your trained network. You can make use of Keras function API for this.\n",
        "<br /><br />\n",
        "\n",
        "\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyHJgB1MeHv8"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.4.3** - Choose any sentence from the list of Pavlos' texts to predict the next word. Input this to the RNN model built for prediction and print the predicted word. Try this out with multiple sentences.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdDrOTlIZbHJ",
        "outputId": "f8258be0-bc77-441c-9468-0eb1a154b654"
      },
      "source": [
        "idx=3\n",
        "print(\"Actual sentence: \",pp_text_df.text[idx])\n",
        "print(\"Sentence sequence: \", pp_text_seq_df.text[idx])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual sentence:  Honestly I do not remember\n",
            "Sentence sequence:  [1, 1175, 53, 466, 302, 280, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJFFxCaIeHv8"
      },
      "source": [
        "# Your code here\n",
        "y_test_pred = model.predict(pp_text_seq_df.text[idx][:-2], batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsI3VUXUH9ts",
        "outputId": "a3007a0d-b569-4a1c-96b9-b9a6a300ee63"
      },
      "source": [
        "y_test_pred.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 1, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjKYBFYParBr"
      },
      "source": [
        "last_word_seq = np.argmax(y_test_pred,axis=2)[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ5laeMoUjY8",
        "outputId": "6f100994-cb76-4c09-ef4b-403cc77453f3"
      },
      "source": [
        "predicted_word = tok.sequences_to_texts([last_word_seq])[0]\n",
        "print(\"predicted last word = \", predicted_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted last word =  film\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaFRIBhDeHv8"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.4.4** - Do you notice any pattern in the predicted words? Do they seem approriate to the context of the texts as you understand it? What do you attribute this discrepency to? How can you resolve it?\n",
        "\n",
        "Answer in less than 150 words.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACjJfO-TeHv8"
      },
      "source": [
        "#### The results are not exactly in the context of the pp_text, but it is because the last words in pp_text have occured in altogether a different context while training the model with imdb data, hence the predictions are made accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuZ-laWieHv9"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "\n",
        "### **3.5 [6 points] TRAINING AND PREDICTING WITH A DIFFERENT DATASET**\n",
        "<br />\n",
        "    \n",
        "**3.5.1** - Read the dataset `cleaned_sarcasm.csv`. This dataset has been preprocessed for you, all you need to do is tokenize, convert to sequence and pad it, similar to 3.2.1, 3.2.2 and 3.3.1.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t90ibxTueHv9"
      },
      "source": [
        "*If you are using colab, the code in the next cell will help download the dataset directly onto your workspace.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj5peWaMeHv9"
      },
      "source": [
        "# Uncomment if you are using colab\n",
        "# !gdown --id \"1pMUuhoKsZVnktosQRuqAutfd3J4sFUsa\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaLH26CieHv9"
      },
      "source": [
        "# Your code here\n",
        "sarcasm_data_path = \"/content/drive/MyDrive/UnivAI/Univ AI 3/cleaned_sarcasm.csv\"\n",
        "sarcasm_data = pd.read_csv(sarcasm_data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "OmGWzIgPUONd",
        "outputId": "b69fcf47-a83d-4cd5-97ac-535fadf5a150"
      },
      "source": [
        "sarcasm_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>you do know west teams play against west teams...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>they were underdogs earlier today but since gr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this meme isnt funny none of the new york nigg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i dont pay attention to her but as long as she...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>you dont have to you have a good build buy gam...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  you do know west teams play against west teams...\n",
              "1  they were underdogs earlier today but since gr...\n",
              "2  this meme isnt funny none of the new york nigg...\n",
              "3  i dont pay attention to her but as long as she...\n",
              "4  you dont have to you have a good build buy gam..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTKc4PHkeHv9"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.5.2** - Train your RNN model with this data and plot the train and validation trace plot. This part is similar to 3.3.2, 3.3.3 and 3.3.4.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MItF0zFKeHv9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "52f375e1-00eb-4fca-d128-1e148df239dd"
      },
      "source": [
        "# Your code here\n",
        "sarcasm_clean_df = clean_data(new_df)\n",
        "sarcasm_clean_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;s&gt; first think another disney movie might goo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;s&gt; watch it cant help enjoy it &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;s&gt; ages love movie &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;s&gt; first saw movie   years later still love i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;s&gt; danny glover superb could play part better...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  <s> first think another disney movie might goo...\n",
              "1               <s> watch it cant help enjoy it </s>\n",
              "2                           <s> ages love movie </s>\n",
              "3  <s> first saw movie   years later still love i...\n",
              "4  <s> danny glover superb could play part better..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGeBZfKCVS2O"
      },
      "source": [
        "sarcasm_tok = Tokenizer(num_words=5000, filters='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHfC3cMsVW1d"
      },
      "source": [
        "# Your code here\n",
        "sarcasm_seq_df = pd.DataFrame()\n",
        "sarcasm_tok.fit_on_texts(sarcasm_clean_df.text)\n",
        "sarcasm_seq_df['text'] =sarcasm_tok.texts_to_sequences(sarcasm_clean_df.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "R_riNtP7VfjI",
        "outputId": "9a8e809d-10df-458f-90c3-d000ca8cc940"
      },
      "source": [
        "sarcasm_seq_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 25, 33, 75, 823, 4, 139, 9, 8, 266, 4, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 35, 15, 89, 239, 259, 15, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 1851, 45, 4, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 25, 113, 4, 63, 193, 50, 45, 15, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 1616, 3407, 830, 30, 207, 85, 51, 2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            text\n",
              "0  [1, 25, 33, 75, 823, 4, 139, 9, 8, 266, 4, 2]\n",
              "1               [1, 35, 15, 89, 239, 259, 15, 2]\n",
              "2                            [1, 1851, 45, 4, 2]\n",
              "3        [1, 25, 113, 4, 63, 193, 50, 45, 15, 2]\n",
              "4       [1, 1616, 3407, 830, 30, 207, 85, 51, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeO7SGI-WPCZ"
      },
      "source": [
        "X_train_sarcasm = sarcasm_seq_df.text.apply(remove_a_word)\n",
        "y_train_sarcasm = sarcasm_seq_df.text.apply(remove_a_word, first=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKfbbuk7WYWc"
      },
      "source": [
        "# Your code here\n",
        "X_train_sarcasm = sequence.pad_sequences(X_train_sarcasm.values, maxlen=30, padding='post')\n",
        "y_train_sarcasm = sequence.pad_sequences(y_train_sarcasm.values, maxlen=30, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc3rjTbcWhAb"
      },
      "source": [
        "embedding_size=300\n",
        "vocabulary_size = 5000\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Embedding(vocabulary_size, embedding_size, mask_zero=True, name='embedding'))\n",
        "model2.add(SimpleRNN(128, return_sequences=True, name='RNN_1'))\n",
        "model2.add(SimpleRNN(64, return_sequences=True, name='RNN_2'))\n",
        "model2.add(Dense(vocabulary_size, activation='softmax', name='dense'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPeBws5FWkLb",
        "outputId": "071ffbf8-bea9-454e-ca7d-7d97df5b0568"
      },
      "source": [
        "model2.compile(optimizer=\"Adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 300)         1500000   \n",
            "_________________________________________________________________\n",
            "RNN_1 (SimpleRNN)            (None, None, 128)         54912     \n",
            "_________________________________________________________________\n",
            "RNN_2 (SimpleRNN)            (None, None, 64)          12352     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 5000)        325000    \n",
            "=================================================================\n",
            "Total params: 1,892,264\n",
            "Trainable params: 1,892,264\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-G0CTlVWqwV",
        "outputId": "2cb14e2a-4715-4c0d-ca1f-5cd9b805ef28"
      },
      "source": [
        "history2 = model2.fit(X_train_sarcasm,y_train_sarcasm, epochs=10, validation_split=0.2, batch_size=64,verbose=1);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1412/1412 [==============================] - 138s 96ms/step - loss: 2.1904 - accuracy: 0.1056 - val_loss: 2.1099 - val_accuracy: 0.1081\n",
            "Epoch 2/10\n",
            "1412/1412 [==============================] - 135s 95ms/step - loss: 2.1291 - accuracy: 0.1082 - val_loss: 2.1001 - val_accuracy: 0.1085\n",
            "Epoch 3/10\n",
            "1412/1412 [==============================] - 136s 96ms/step - loss: 2.1123 - accuracy: 0.1086 - val_loss: 2.0914 - val_accuracy: 0.1091\n",
            "Epoch 4/10\n",
            "1412/1412 [==============================] - 137s 97ms/step - loss: 2.0937 - accuracy: 0.1096 - val_loss: 2.0878 - val_accuracy: 0.1095\n",
            "Epoch 5/10\n",
            "1412/1412 [==============================] - 136s 96ms/step - loss: 2.0742 - accuracy: 0.1101 - val_loss: 2.0880 - val_accuracy: 0.1091\n",
            "Epoch 6/10\n",
            "1412/1412 [==============================] - 134s 95ms/step - loss: 2.0638 - accuracy: 0.1102 - val_loss: 2.0911 - val_accuracy: 0.1091\n",
            "Epoch 7/10\n",
            "1412/1412 [==============================] - 137s 97ms/step - loss: 2.0502 - accuracy: 0.1102 - val_loss: 2.0966 - val_accuracy: 0.1090\n",
            "Epoch 8/10\n",
            "1412/1412 [==============================] - 138s 98ms/step - loss: 2.0332 - accuracy: 0.1104 - val_loss: 2.1043 - val_accuracy: 0.1087\n",
            "Epoch 9/10\n",
            "1412/1412 [==============================] - 137s 97ms/step - loss: 2.0055 - accuracy: 0.1113 - val_loss: 2.1141 - val_accuracy: 0.1069\n",
            "Epoch 10/10\n",
            "1412/1412 [==============================] - 136s 96ms/step - loss: 1.9982 - accuracy: 0.1114 - val_loss: 2.1247 - val_accuracy: 0.1060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk5DvqW1foam",
        "outputId": "90d69261-1ebb-4737-ff8f-89498a7d7b87"
      },
      "source": [
        "model2.save('/content/drive/MyDrive/UnivAI/Univ AI 3/models/model2_rnn_10epochs')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/UnivAI/Univ AI 3/models/model2_rnn_10epochs/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "P6SvDzARW6gD",
        "outputId": "1fd37620-e323-441c-a1c0-3718eb6df6f8"
      },
      "source": [
        "# Your code here\n",
        "def plot_history(history, name):\n",
        "    fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
        "    for i, metric in enumerate(['loss', 'accuracy']): \n",
        "        ax[i].plot(history.history[metric], label='Train')\n",
        "        ax[i].plot(history.history['val_'+metric], label='Validation')\n",
        "        if metric == 'accuracy': ax[i].axhline(0.5, c='r', ls='--', label='Trivial accuracy')\n",
        "        ax[i].set_xlabel('Epoch')\n",
        "        ax[i].set_ylabel(metric)\n",
        "    plt.suptitle(f'{name} Training', y=1.05)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "plot_history(history2, 'RNN')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Lp_yRleHv9"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.5.3** - Repeat 3.4.1, 3.4.2 and 3.4.3 with the RNN model trained using the new dataset.\n",
        "<br /><br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NINDskVeHv9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bf5978b0-7cfa-4443-abf4-f3d5cb4c2ba0"
      },
      "source": [
        "# Your code here\n",
        "pp_text_df = pd.read_csv(\"/content/drive/MyDrive/UnivAI/Univ AI 3/pp_text.csv\")\n",
        "pp_text_df.columns=['text']\n",
        "pp_text_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When you make changes please always check if e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I found them very similar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Give some overview of what the exercises are a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Honestly I do not remember</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Can you check the video</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  When you make changes please always check if e...\n",
              "1                          I found them very similar\n",
              "2  Give some overview of what the exercises are a...\n",
              "3                         Honestly I do not remember\n",
              "4                            Can you check the video"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "MzI9_GGyYQOK",
        "outputId": "479c21ed-6ede-4349-f213-7027d6657c1b"
      },
      "source": [
        "pp_text_clean = clean_data(pp_text_df)\n",
        "pp_text_seq_df=pd.DataFrame()\n",
        "pp_text_seq_df['text'] = tok.texts_to_sequences(pp_text_clean.text)\n",
        "pp_text_seq_df.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 1041, 232, 29, 1229, 511, 115, 774, 374, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 53, 159, 191, 1503, 695, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 102, 1267, 597, 434, 10, 872, 95, 648, 148...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 1175, 53, 466, 302, 280, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 1913, 232, 774, 10, 331, 2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  [1, 1041, 232, 29, 1229, 511, 115, 774, 374, 1...\n",
              "1                    [1, 53, 159, 191, 1503, 695, 2]\n",
              "2  [1, 102, 1267, 597, 434, 10, 872, 95, 648, 148...\n",
              "3                    [1, 1175, 53, 466, 302, 280, 2]\n",
              "4                    [1, 1913, 232, 774, 10, 331, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-fV_kBggNRN",
        "outputId": "259912a8-c434-45ed-9785-2738834dbf2f"
      },
      "source": [
        "idx=3\n",
        "print(\"Actual sentence: \",pp_text_df.text[idx])\n",
        "print(\"Sentence sequence: \", pp_text_seq_df.text[idx])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual sentence:  Honestly I do not remember\n",
            "Sentence sequence:  [1, 1175, 53, 466, 302, 280, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou--lzT5fQDJ"
      },
      "source": [
        "# Your code here\n",
        "y_test_pred = model2.predict(pp_text_seq_df.text[idx][:-2], batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeWoeAxygBoc"
      },
      "source": [
        "last_word_seq = np.argmax(y_test_pred,axis=2)[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEOD1sGZfl1J",
        "outputId": "4593d8ed-d74b-4fe6-ef0a-207a6c10a137"
      },
      "source": [
        "predicted_word = tok.sequences_to_texts([last_word_seq])[0]\n",
        "print(\"predicted last word = \", predicted_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted last word =  movie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H29yqyw8eHv-"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.5.4** - How do the results with the new dataset compare to the previous ones? Why do you think so? \n",
        "\n",
        "Answer in less than 100 words.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDk_guEjiVe-"
      },
      "source": [
        "Predictions are almost similar, this is because the last word in the pp_text often ends up occuring together with similar set of words in both imbd and sarcasm data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5xjXvFfeHv-"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "\n",
        "### **3.6 [3 points] COMPLETING THE SENTENCE**\n",
        "<br />\n",
        "\n",
        "**3.6.1** Until now we have predicted a single word for a given sentence. However, what if he meant more than one word when he typed in `...`\n",
        "\n",
        "We will now predict multiple words for each input sentence. To do this we will first predict one word, append this word to the input text and then predict one more with the updated input. Continue doing this for 5 words or until the end token `</s>` (whichever comes first). \n",
        "<br /><br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVwi2sztp4-Y"
      },
      "source": [
        "input_sent = pp_text_seq_df.text[idx][:-2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dn4xmO4qb0A"
      },
      "source": [
        "input_sent.append(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzI8epMXqpVX",
        "outputId": "1b160587-b855-46c3-8c62-7aa57c99b07b"
      },
      "source": [
        "input_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1175, 53, 466, 302, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 283
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdxgVI65qgFw",
        "outputId": "e011807e-cfbf-4307-c9ad-46638641783e"
      },
      "source": [
        "pp_text_seq_df.text[idx][:-2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1175, 53, 466, 302]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 277
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gte6ERQ4eHv-"
      },
      "source": [
        "# Your code here\n",
        "def predict_word_seq(estimator, sent_idx):\n",
        "  pred_sent = []\n",
        "  idx=sent_idx\n",
        "  input_sent = pp_text_seq_df.text[idx][:-2]\n",
        "  for i in range(5):\n",
        "    y_test_pred = estimator.predict(input_sent, batch_size=1)\n",
        "    last_word_seq = np.argmax(y_test_pred,axis=2)[-1]\n",
        "    input_sent.append(last_word_seq[0])\n",
        "    last_word = tok.sequences_to_texts([last_word_seq])[0]\n",
        "    pred_sent.append(last_word)\n",
        "    if last_word == '</s>':\n",
        "      break\n",
        "  return pred_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQhhP49Vrfce",
        "outputId": "a61326d3-b4e4-4c81-acc1-ae47691b47de"
      },
      "source": [
        "sent_idx = 4\n",
        "pred_sent = predict_word_seq(model, sent_idx)\n",
        "print(\"Actual sentence: \",pp_text_df.text[sent_idx])\n",
        "print(\"Predicted word sequence with model1 = \", \" \".join(pred_sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual sentence:  Can you check the video\n",
            "Predicted word sequence with model1 =  also performance </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26ZI2osGrCPp",
        "outputId": "169830be-8535-470c-8fc7-05713b85d3ea"
      },
      "source": [
        "sent_idx = 4\n",
        "pred_sent = predict_word_seq(model2, sent_idx)\n",
        "print(\"Actual sentence: \",pp_text_df.text[sent_idx])\n",
        "print(\"Predicted word sequence with model2 = \", \" \".join(pred_sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual sentence:  Can you check the video\n",
            "Predicted word sequence with model2 =  also like movies seen times\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfCgZwbMeHv-"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **3.7 [3 points] HOMEWORK QUIZ**\n",
        "<br />\n",
        "After attempting this part of the homework, answer the questions on edStem. All the questions depend on this part of the homework and you will not be able to answer them without attempting this part.\n",
        "<br /><br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSMxkkAceHv-"
      },
      "source": [
        "#### Answer the questions on edStem"
      ]
    }
  ]
}