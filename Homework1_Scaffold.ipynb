{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "Homework1_Scaffold.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manisha2297/DataAndBases/blob/master/Homework1_Scaffold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1LPcWO2eHvY"
      },
      "source": [
        "![](https://github.com/PadmajaVB/UnivAI_AI-3/blob/main/fig/univ.png?raw=1)\n",
        "\n",
        "# AI-3: Language Models\n",
        "## Homework 1: Embeddings and Language Models\n",
        "\n",
        "**AI3 Cohort 1**<br/>\n",
        "**Univ.AI**<br/>\n",
        "**Instructor**: Pavlos Protopapas<br />\n",
        "**Maximum Score**: 100\n",
        "\n",
        "<hr style=\"height:2.4pt\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h0qj0bGeHvn"
      },
      "source": [
        "#RUN THIS CELL \n",
        "import requests\n",
        "from IPython.core.display import HTML"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JejHEQwzeHvo"
      },
      "source": [
        "# Import necessary libraries\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import dot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPMYtVTzeHvo"
      },
      "source": [
        "### INSTRUCTIONS\n",
        "\n",
        "\n",
        "- This homework is a jupyter notebook. Download and work on it on your local machine.\n",
        "\n",
        "- This homework should be submitted in pairs.\n",
        "\n",
        "- Ensure you and your partner together have submitted the homework only once. Multiple submissions of the same work will be penalised and will cost you 2 points.\n",
        "\n",
        "- Please restart the kernel and run the entire notebook again before you submit.\n",
        "\n",
        "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit. \n",
        "\n",
        "- To submit the homework, either one of you upload the working notebook on edStem and click the submit button on the bottom right corner.\n",
        "\n",
        "- Submit the homework well before the given deadline. Submissions after the deadline will not be graded.\n",
        "\n",
        "- We have tried to include all the libraries you may need to do the assignment in the imports statement at the top of this notebook. We strongly suggest that you use those and not others as we may not be familiar with them.\n",
        "\n",
        "- Comment your code well. This would help the graders in case there is any issue with the notebook while running. It is important to remember that the graders will not troubleshoot your code. \n",
        "\n",
        "- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**. \n",
        "\n",
        "- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print()` function that includes a reference to the calculated value, **not hardcoded**. For example: \n",
        "```\n",
        "print(f'The R^2 is {R:.4f}')\n",
        "```\n",
        "- Your plots should include clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is not a descriptive title; \"95 % confidence interval of coefficients of polynomial degree 5\" is).\n",
        "\n",
        "- **Ensure you make appropraite plots for all the questions it is applicable to, regardless of it being explicitly asked for.**\n",
        "\n",
        "<hr style=\"height:2pt\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke0XQlxXeHvp"
      },
      "source": [
        "### Names of the people who worked on this homework together\n",
        "#### Manisha R and Padmaja V Bhagwat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqEwLDl_eHvp"
      },
      "source": [
        "### **DATASET ACCESS**\n",
        "\n",
        "**Please note that all the datasets used in this homework are available to you on edStem. You will find it in the resources tab (on the top right) next to your lessons tab. Additionally, some datasets have been provided in a form that will allow you to access it directly on google colab by uncommenting and running some cells.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iPpuILbeHvp"
      },
      "source": [
        "### **HOMEWORK QUIZ**\n",
        "\n",
        "**For each part of the homework, there is an associated quiz on edStem. You are required to attempt that after completing each section of this homework. Please note that the quiz is one attempt only.**\n",
        "\n",
        "\n",
        "![](https://github.com/PadmajaVB/UnivAI_AI-3/blob/main/fig/one_attempt.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc6P9IjIeHvp"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "## **PART 1 [35 points]: Language Modelling using ngrams**\n",
        "<br />    \n",
        "\n",
        "In the first part of the homework, you are expected to build a language model based on bigrams. You will develop your own sub-word tokenization to analyze dissaster messages from multiple natural disasters dataset. All the sentences are translated into english.\n",
        "\n",
        "\n",
        "You have been tasked to develop a language model to complete messages that for some reason arrive incomplete to a radio station. Given the delicate situation, you will have to be extra careful. Each word in the sentence convey a lot of information, and improper handling of the data can mean harm to someone. \n",
        "    \n",
        "    \n",
        "</div>\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noT0TNOleHvq"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "## **PART 1: Questions**\n",
        "<br />\n",
        "\n",
        "### **1.1 [5 points] PREPROCESS THE DATASET**\n",
        "<br />\n",
        "\n",
        "**1.1.1** - Read in the dataset `disaster_response_messages_training.csv` and select only the column \"message\".\n",
        "<br /><br />\n",
        "\n",
        "**1.1.2** - Define a function `clean_data` that takes the data frame as input, converts the characters to lower case and removes any special characters that you might consider irrelevant,  adds the start token `<s>` and the end token `</s>` to every sentence (row) in the data frame and returns the processed data frame. \n",
        "<br /><br />\n",
        "\n",
        "\n",
        "**1.1.3** - Split the dataset into train and test sets. The proportion should be 0.95 and 0.05, respectively. You will create the language model based on the train set and validate your results on the test set.\n",
        "<br /><br />    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "### **1.2 [8 points] TOKENIZE AND COUNT**\n",
        "<br />\n",
        "In this section, you will create three different tokenizers that you will build LM based on. The tokenization functions must divide the text into tokens, count their frequency and return a dictionary with a mapping of token to number.\n",
        "    \n",
        "**1.2.1** - Create your own tokenization function ('tokenizer_1') based on whitespace. Set the vocabulary size to 1000, including the `<UNK>` token for out of the vocabulary (OOV) words. \n",
        "<br /><br />\n",
        "\n",
        "**1.2.2** - Create a second tokenization function ('tokenizer_2') based on whitespace, but do not limit the vocabulary size.\n",
        "<br /><br />\n",
        "\n",
        "**1.2.3** - Create a third tokenization function ('tokenizer_3') based on sub-words. You have to define a set of common sub-words in the English language, for example, the subtokens _ing_ and _n't_.\n",
        "    \n",
        "In this example, the sentence \"_It is raining outside_\" would be tokenized as [_It_, _is_, _rain_, _ing_, _outside_ ].\n",
        "<br /><br />\n",
        "    \n",
        "    \n",
        "    \n",
        "### **1.3 [6 points] CONSTRUCTING BIGRAMS**\n",
        "<br />\n",
        "\n",
        "**1.3.1** - Using each of the tokenizer functions you created, split each sentence into tokens in their numerical representation. \n",
        "<br /><br />\n",
        "\n",
        "**1.3.2** - Count the bigrams in the dataset for each tokenizer and divide them by the total number of bigrams. This will give you the probability of each bigram.\n",
        "<br /><br />\n",
        "    \n",
        "    \n",
        "    \n",
        "### **1.4 [8 points] PREDICTING THE NEXT WORDS**\n",
        "<br />\n",
        "\n",
        "**1.4.1** - Simulate the incomplete messages dividing each sentence of the **test** set into two. For this, split each sentence in a 3:1 ratio. The first $75\\%$ of a sentence will represent the correct message, and the last $25\\%$ will convey the missing information. You will not give this 25% to your model, it is kept hidden. This 25% will only be used to evaluate the predictions of your language model.\n",
        "    \n",
        "For example in the sentence: *\"I will go out on a vacation, now that my semester ended.\"*\n",
        "\n",
        "The first 75% will be *\"I will go out on a vacation, now\"*\n",
        "\n",
        "The last 25% will be *\"that my semeter ended\"*\n",
        "\n",
        "Your aim is to predict the last part by giving your model the first \"part\" of the sentence.\n",
        "\n",
        "\n",
        "Note that in an n-gram language model, only the last $n-1$ words are used to make a prediction. For example, for the above sentence, if you are using bigrams, the input to your model would only be \"now\" and you are expected to predict \"that\".\n",
        "    \n",
        "<br /><br />    \n",
        "    \n",
        "**1.4.2** - Given 5 sentences from the previous question (test set), predict the next word. \n",
        "Append this predicted word to the input sequence and predict the next one. Repeat this process until you reach the 10th token or the end of a sentence. Compare your results qualitatively with the original sentences. Do the results make sense wrt the context and semantics?\n",
        "\n",
        "Repeat this for all the models built using different tokenization techniques.\n",
        "<br /><br />\n",
        "\n",
        "**1.4.3** - Repeat the same exercise, for all 3 models, but this time, the next token will be sampled from a distribution given by the bigram frequency. Compare and comment on the results?\n",
        "\n",
        "\n",
        "*Hint:* In a model of two bigrams with frequencies 0.7 and 0.3, a deterministic prediction will only predict the first bigram. Sampling from a distribution, will enable the model to predict the second bigram with a probability of 0.3. In this way we can still predict infrequent tokens. \n",
        "<br /><br />\n",
        "    \n",
        "\n",
        "### **1.5 [5 points] EVALUATE THE LANGUAGE MODELS**\n",
        "<br />\n",
        "\n",
        "    \n",
        "**1.5.1** - For each of the models built using different tokenization techniques, compute the average perplexity in the test set (part 1.1.3). Perform smoothing on the bigram models. Based on the perplexity, which model is better?\n",
        "<br /><br />\n",
        "\n",
        "**1.5.2** - Given the perplexities, which model do you think is better? Why do you think so? Does this reflect the quality of the prediction as seen in part 1.4? \n",
        " What is the effect of UNK words?\n",
        "\n",
        "<br /><br />\n",
        "\n",
        "### **1.6 [3 points] HOMEWORK QUIZ**\n",
        "<br />\n",
        "After attempting this part of the homework, answer the questions on edStem. All the questions depend on this part of the homework and you will not be able to answer them without attempting this part.\n",
        "\n",
        "<br /><br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncCnPL2Be6Bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b8387b-9cce-48b2-e53d-12e58b911173"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll4Mrj7feHvt"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "## **PART 1: Solutions**\n",
        "    \n",
        "### **1.1 [5 points] PREPROCESS THE DATASET**\n",
        "<br />\n",
        "\n",
        "**1.1.1** - Read in the dataset `disaster_response_messages_training.csv` and select only the column \"message\".\n",
        "<br /><br />\n",
        "\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUGkyUoTeHvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb1cedf-c24a-4568-f5d4-3ce970ddf735"
      },
      "source": [
        "# Your code here\n",
        "raw_data = pd.read_csv('/content/drive/MyDrive/UnivAI/Univ AI 3/Homework 1 - Part 1 Dataset.csv')\n",
        "raw_data = raw_data[['message']]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZUBqYDfLOc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "721b0f19-b9fd-44fd-caa7-73ff437faa51"
      },
      "source": [
        "raw_data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Weather update - a cold front from Cuba that c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Is the Hurricane over or is it not over</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>says: west side of Haiti, rest of the country ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Information about the National Palace-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Storm at sacred heart of jesus</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message\n",
              "0  Weather update - a cold front from Cuba that c...\n",
              "1            Is the Hurricane over or is it not over\n",
              "2  says: west side of Haiti, rest of the country ...\n",
              "3             Information about the National Palace-\n",
              "4                     Storm at sacred heart of jesus"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMQD-CRWeHvu"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "**1.1.2** - Define a function `clean_data` that takes the data frame as input, converts the characters to lower case and removes any special characters that you might consider irrelevant,  adds the start token `<s>` and the end token `</s>` to every sentence (row) in the data frame and returns the processed data frame. \n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va7CNTlaiQmw"
      },
      "source": [
        "def preprocess_text(text):\n",
        "  clean_text = text.lower()\n",
        "  # remove special characters - basically anything that is not a letter or a space\n",
        "  clean_text = re.sub(r'[^a-z0-9\\s]+','', clean_text)\n",
        "  clean_text = '<s> '+clean_text+' </s>'\n",
        "  return clean_text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E7jcvkOeHvu"
      },
      "source": [
        "# Your code here\n",
        "def clean_data(dataframe):\n",
        "  dataframe['message'] = dataframe['message'].apply(preprocess_text) \n",
        "  return dataframe"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-89piHxei8RQ"
      },
      "source": [
        "# clean the data\n",
        "df = clean_data(raw_data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncJbbmVRjnFI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "2eb25622-81b9-4589-f805-7ae7aaa0958b"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;s&gt; weather update  a cold front from cuba tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;s&gt; is the hurricane over or is it not over &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;s&gt; says west side of haiti rest of the countr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;s&gt; information about the national palace &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;s&gt; storm at sacred heart of jesus &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message\n",
              "0  <s> weather update  a cold front from cuba tha...\n",
              "1   <s> is the hurricane over or is it not over </s>\n",
              "2  <s> says west side of haiti rest of the countr...\n",
              "3     <s> information about the national palace </s>\n",
              "4            <s> storm at sacred heart of jesus </s>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbJsxkICeHvu"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.1.3** - Split the dataset into train and test sets. The proportion should be 0.95 and 0.05, respectively. You will create the language model based on the train set and validate your results on the test set.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7MGHw32eHvv"
      },
      "source": [
        "# Your code here\n",
        "index = np.unique(df.index)\n",
        "train_index, val_index = train_test_split(index, train_size=0.95, random_state=66)\n",
        "\n",
        "df_train = df.loc[train_index]\n",
        "df_val = df.loc[val_index]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8GQEOQNeHvv"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **1.2 [8 points] TOKENIZE AND COUNT**\n",
        "<br />\n",
        "In this section, you will create three different tokenizers and build an LM based on each one of them. The tokenization functions must divide the text into tokens, count their frequency and return a dictionary with a mapping of token to number.\n",
        "\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y44B-5AReHvv"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6rUkeBZeHvv"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.2.1** - Create your own tokenization function ('tokenizer_1') based on whitespace. Set the vocabulary size to 1000, including the `<UNK>` token for out of the vocabulary (OOV) words. \n",
        "<br /><br />\n",
        "    \n",
        "</div> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qy0BB6IzPiP"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "779oD91IeHvv"
      },
      "source": [
        "# Fill in to complete this function \n",
        "def tokenizer_1(text_corpus, vocabulary_size):\n",
        "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(Counter(text_corpus.split()).most_common(vocabulary_size-1))\n",
        "    \n",
        "    dictionary={}\n",
        "    # For all words in count, assign a token (you can use a for loop) \n",
        "    for i, tup in enumerate(count):\n",
        "        dictionary[tup[0]] = i\n",
        "        \n",
        "    # Make a new list of tokens associated with words    \n",
        "    data = []\n",
        "    # Initialize a counter for 'UNK' values \n",
        "    unk_count = 0\n",
        "    \n",
        "    # For all words in corpus, find the associated token, and append to \n",
        "    # the 'data' variable defined above\n",
        "    for word in text_corpus:\n",
        "        if word in dictionary:\n",
        "            token = dictionary[word]\n",
        "        # If word is not in dictionary, it is 'out of vocabulary'\n",
        "        # So we need to assign it the zero token and\n",
        "        # update the count of the 'UNK' token\n",
        "        else:\n",
        "            token = 0  \n",
        "            unk_count += 1\n",
        "            \n",
        "        # Append token to data \n",
        "        data.append(token)\n",
        "        \n",
        "    # We can now set the count of 'UNK' tokens in the corpus\n",
        "    count[0][1] = unk_count\n",
        "    \n",
        "    # A reverse dictionary takes you from tokens to words\n",
        "    # Eg. if dictionary['Ignacio'] == 44\n",
        "    # reverse_dictionary[44] == 'Ignacio'\n",
        "    reversed_dictionary = {v:k for k,v in dictionary.items()}\n",
        "    \n",
        "    return data, count, dictionary, reversed_dictionary"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ktr_kTBeHvw"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.2.2** - Create a second tokenization function ('tokenizer_2') based on whitespace, but do not limit the vocabulary size.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLGPaXj3eHvw"
      },
      "source": [
        "# Fill in to complete this function \n",
        "def tokenizer_2(text_corpus):\n",
        "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
        "    count = []\n",
        "    count.extend(Counter(text_corpus.split()).most_common())\n",
        "    \n",
        "    dictionary={}\n",
        "    for i, tup in enumerate(count):\n",
        "        dictionary[tup[0]] = i\n",
        "        \n",
        "    # Make a new list of tokens associated with words    \n",
        "    data = []\n",
        "    for word in text_corpus:\n",
        "        if word in dictionary:\n",
        "            token = dictionary[word]\n",
        "        else:\n",
        "            token = 0  \n",
        "            \n",
        "        # Append token to data \n",
        "        data.append(token)\n",
        "\n",
        "    dictionary['UNK']=len(text_corpus.split())\n",
        "    reversed_dictionary = {v:k for k,v in dictionary.items()}\n",
        "    \n",
        "    return data, count, dictionary, reversed_dictionary"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwE4UjhNeHvw"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.2.3** - Create a third tokenization function ('tokenizer_3') based on sub-words. You have to define a set of common sub-words in the English language, for example, the subtokens _ing_ and _n't_.\n",
        "    \n",
        "In this example, the sentence \"_It is raining outside_\" would be tokenized as [_It_, _is_, _rain_, _ing_, _outside_ ].\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKgmLPEykLWE"
      },
      "source": [
        "def sub_word_tokenizer(text_corpus):\n",
        "  common_prefix = [ 'pre', 'un', 'under', 'over', 'post', 'ir', 'in', 'sub']\n",
        "  common_suffix = ['ing','ly','ed', 'n\\'t','er','est', 'es', 'ful']\n",
        "\n",
        "  word_seq = text_corpus.split()\n",
        "  results = []\n",
        "  for word in word_seq:\n",
        "    filtered_prefix = list(filter(word.startswith, common_prefix))\n",
        "    filtered_suffix = list(filter(word.endswith, common_suffix))\n",
        "    if len(filtered_prefix)>0:\n",
        "      results.extend(re.split(rf\"^({filtered_prefix[0]})\",word))\n",
        "    if len(filtered_suffix)>0:\n",
        "      results.extend(re.split(rf\"({filtered_suffix[0]})$\",word))\n",
        "    if len(filtered_prefix)==0 and len(filtered_suffix)==0:\n",
        "      results.append(word)\n",
        "  \n",
        "  subword_tokens = list(filter(None, results))\n",
        "  return subword_tokens"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qEdHsrOeHvw"
      },
      "source": [
        "# Your code here\n",
        "def tokenizer_3(text_corpus):\n",
        "  subword_tokens = sub_word_tokenizer(text_corpus)\n",
        "  count = []\n",
        "  count.extend(Counter(subword_tokens).most_common())\n",
        "\n",
        "  dictionary={}\n",
        "  for i, tup in enumerate(count):\n",
        "      dictionary[tup[0]] = i\n",
        "      \n",
        "  # Make a new list of tokens associated with words    \n",
        "  data = []\n",
        "  for word in text_corpus:\n",
        "      if word in dictionary:\n",
        "          token = dictionary[word]\n",
        "      else:\n",
        "          token = 0  \n",
        "          \n",
        "      # Append token to data \n",
        "      data.append(token)\n",
        "      \n",
        "  dictionary['UNK']=len(text_corpus.split())\n",
        "  reversed_dictionary = {v:k for k,v in dictionary.items()}\n",
        "\n",
        "  return data, count, dictionary, reversed_dictionary\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUDI4DhLeHvw"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **1.3 [6 points] CONSTRUCTING BIGRAMS**\n",
        "<br />\n",
        "\n",
        "**1.3.1** - Using each of the tokenizer functions you created, split each sentence into tokens in their numerical representation. \n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY8lbSsAtMX8"
      },
      "source": [
        "def get_tok_sequence(text, vocab_dict):\n",
        "  seq=[]\n",
        "  for word in text.split():\n",
        "    if word in vocab_dict.keys():\n",
        "      seq.append(vocab_dict[word])\n",
        "    else:\n",
        "      seq.append(vocab_dict['UNK'])\n",
        "  return seq"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAd57jUIsLxD"
      },
      "source": [
        "text_corpus = \"\"\n",
        "for text in  df['message'].values:\n",
        "  text_corpus = text_corpus + \" \"+ text"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hAtFbwxs87-"
      },
      "source": [
        "data, count, dictionary, reversed_dictionary  = tokenizer_1(text_corpus,1000)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcBQAjWQuV_n"
      },
      "source": [
        "df_train_tok1 = pd.DataFrame(df_train['message'].apply(get_tok_sequence, vocab_dict=dictionary), columns=['message'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdHgFBBiwTnN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "f8515422-abf6-4de6-8e04-ebfc51ac83f7"
      },
      "source": [
        "df_train_tok1.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20231</th>\n",
              "      <td>[2, 0, 0, 686, 0, 346, 164, 988, 24, 0, 0, 308...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8658</th>\n",
              "      <td>[2, 65, 120, 71, 6, 77, 9, 0, 0, 0, 0, 28, 11,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8449</th>\n",
              "      <td>[2, 747, 39, 0, 0, 248, 1, 0, 84, 7, 1, 0, 147...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8551</th>\n",
              "      <td>[2, 25, 17, 442, 0, 0, 55, 24, 0, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18220</th>\n",
              "      <td>[2, 0, 589, 30, 33, 0, 6, 7, 0, 0, 273, 134, 1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 message\n",
              "20231  [2, 0, 0, 686, 0, 346, 164, 988, 24, 0, 0, 308...\n",
              "8658   [2, 65, 120, 71, 6, 77, 9, 0, 0, 0, 0, 28, 11,...\n",
              "8449   [2, 747, 39, 0, 0, 248, 1, 0, 84, 7, 1, 0, 147...\n",
              "8551                [2, 25, 17, 442, 0, 0, 55, 24, 0, 3]\n",
              "18220  [2, 0, 589, 30, 33, 0, 6, 7, 0, 0, 273, 134, 1..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOhmE0Ed0KUz",
        "outputId": "f54829fa-e3ec-445d-eab1-9da5c5abb30f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "count"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['UNK', 2190154],\n",
              " ('the', 26098),\n",
              " ('<s>', 21046),\n",
              " ('</s>', 21046),\n",
              " ('and', 14737),\n",
              " ('to', 14106),\n",
              " ('of', 13494),\n",
              " ('in', 12879),\n",
              " ('a', 7873),\n",
              " ('i', 5679),\n",
              " ('for', 5289),\n",
              " ('is', 4489),\n",
              " ('are', 3893),\n",
              " ('have', 3859),\n",
              " ('we', 3808),\n",
              " ('on', 3311),\n",
              " ('that', 3265),\n",
              " ('with', 2821),\n",
              " ('you', 2431),\n",
              " ('by', 2398),\n",
              " ('people', 2351),\n",
              " ('as', 2350),\n",
              " ('water', 2266),\n",
              " ('from', 2238),\n",
              " ('food', 2168),\n",
              " ('help', 2083),\n",
              " ('at', 2022),\n",
              " ('has', 1948),\n",
              " ('this', 1930),\n",
              " ('it', 1764),\n",
              " ('can', 1763),\n",
              " ('need', 1690),\n",
              " ('will', 1681),\n",
              " ('be', 1668),\n",
              " ('please', 1573),\n",
              " ('me', 1540),\n",
              " ('not', 1534),\n",
              " ('an', 1439),\n",
              " ('my', 1417),\n",
              " ('earthquake', 1404),\n",
              " ('been', 1375),\n",
              " ('were', 1316),\n",
              " ('was', 1302),\n",
              " ('they', 1280),\n",
              " ('us', 1221),\n",
              " ('like', 1220),\n",
              " ('would', 1214),\n",
              " ('do', 1115),\n",
              " ('some', 1090),\n",
              " ('which', 1087),\n",
              " ('what', 1078),\n",
              " ('there', 1067),\n",
              " ('said', 1059),\n",
              " ('all', 1047),\n",
              " ('more', 1046),\n",
              " ('or', 1021),\n",
              " ('their', 1020),\n",
              " ('but', 969),\n",
              " ('who', 969),\n",
              " ('its', 947),\n",
              " ('about', 922),\n",
              " ('if', 884),\n",
              " ('also', 881),\n",
              " ('know', 877),\n",
              " ('am', 855),\n",
              " ('no', 845),\n",
              " ('where', 811),\n",
              " ('haiti', 805),\n",
              " ('areas', 783),\n",
              " ('one', 763),\n",
              " ('dont', 760),\n",
              " ('because', 757),\n",
              " ('had', 744),\n",
              " ('find', 729),\n",
              " ('up', 706),\n",
              " ('government', 699),\n",
              " ('information', 684),\n",
              " ('sandy', 676),\n",
              " ('other', 666),\n",
              " ('after', 664),\n",
              " ('over', 661),\n",
              " ('when', 654),\n",
              " ('out', 640),\n",
              " ('relief', 627),\n",
              " ('country', 626),\n",
              " ('than', 617),\n",
              " ('now', 615),\n",
              " ('im', 609),\n",
              " ('aid', 598),\n",
              " ('so', 597),\n",
              " ('he', 594),\n",
              " ('how', 586),\n",
              " ('good', 582),\n",
              " ('two', 581),\n",
              " ('get', 573),\n",
              " ('health', 569),\n",
              " ('our', 559),\n",
              " ('many', 544),\n",
              " ('affected', 542),\n",
              " ('area', 540),\n",
              " ('them', 538),\n",
              " ('any', 533),\n",
              " ('thank', 530),\n",
              " ('since', 503),\n",
              " ('new', 491),\n",
              " ('children', 491),\n",
              " ('most', 487),\n",
              " ('message', 477),\n",
              " ('into', 456),\n",
              " ('work', 454),\n",
              " ('million', 447),\n",
              " ('emergency', 444),\n",
              " ('through', 443),\n",
              " ('want', 442),\n",
              " ('supplies', 434),\n",
              " ('give', 433),\n",
              " ('tents', 432),\n",
              " ('go', 431),\n",
              " ('well', 427),\n",
              " ('last', 427),\n",
              " ('power', 423),\n",
              " ('your', 419),\n",
              " ('during', 419),\n",
              " ('local', 418),\n",
              " ('including', 414),\n",
              " ('international', 412),\n",
              " ('rains', 405),\n",
              " ('house', 402),\n",
              " ('region', 402),\n",
              " ('time', 399),\n",
              " ('could', 397),\n",
              " ('still', 389),\n",
              " ('flood', 389),\n",
              " ('hurricane', 384),\n",
              " ('these', 383),\n",
              " ('support', 380),\n",
              " ('under', 379),\n",
              " ('send', 375),\n",
              " ('storm', 373),\n",
              " ('rain', 373),\n",
              " ('due', 371),\n",
              " ('heavy', 370),\n",
              " ('years', 369),\n",
              " ('such', 368),\n",
              " ('disaster', 367),\n",
              " ('city', 365),\n",
              " ('school', 363),\n",
              " ('http', 361),\n",
              " ('assistance', 357),\n",
              " ('while', 357),\n",
              " ('year', 355),\n",
              " ('only', 352),\n",
              " ('may', 352),\n",
              " ('medical', 351),\n",
              " ('thanks', 346),\n",
              " ('those', 346),\n",
              " ('santiago', 344),\n",
              " ('floods', 343),\n",
              " ('houses', 342),\n",
              " ('families', 342),\n",
              " ('family', 338),\n",
              " ('live', 337),\n",
              " ('three', 333),\n",
              " ('destroyed', 332),\n",
              " ('shelter', 331),\n",
              " ('national', 330),\n",
              " ('united', 329),\n",
              " ('make', 329),\n",
              " ('victims', 328),\n",
              " ('province', 327),\n",
              " ('take', 319),\n",
              " ('being', 319),\n",
              " ('hit', 314),\n",
              " ('just', 310),\n",
              " ('state', 309),\n",
              " ('very', 305),\n",
              " ('south', 303),\n",
              " ('red', 298),\n",
              " ('response', 297),\n",
              " ('living', 294),\n",
              " ('job', 292),\n",
              " ('around', 291),\n",
              " ('days', 291),\n",
              " ('number', 289),\n",
              " ('should', 287),\n",
              " ('day', 287),\n",
              " ('anything', 286),\n",
              " ('weather', 285),\n",
              " ('come', 284),\n",
              " ('much', 284),\n",
              " ('notes', 284),\n",
              " ('north', 278),\n",
              " ('provide', 277),\n",
              " ('needs', 272),\n",
              " ('high', 271),\n",
              " ('homes', 270),\n",
              " ('here', 267),\n",
              " ('between', 267),\n",
              " ('first', 267),\n",
              " ('going', 263),\n",
              " ('already', 263),\n",
              " ('conditions', 263),\n",
              " ('see', 261),\n",
              " ('river', 260),\n",
              " ('community', 260),\n",
              " ('flooding', 260),\n",
              " ('distribution', 259),\n",
              " ('care', 258),\n",
              " ('nations', 255),\n",
              " ('2', 254),\n",
              " ('world', 254),\n",
              " ('situation', 253),\n",
              " ('according', 253),\n",
              " ('working', 252),\n",
              " ('even', 252),\n",
              " ('villages', 252),\n",
              " ('caused', 252),\n",
              " ('reported', 251),\n",
              " ('team', 249),\n",
              " ('humanitarian', 248),\n",
              " ('damaged', 247),\n",
              " ('left', 246),\n",
              " ('hospital', 246),\n",
              " ('development', 244),\n",
              " ('received', 243),\n",
              " ('tent', 243),\n",
              " ('thousands', 243),\n",
              " ('before', 240),\n",
              " ('off', 239),\n",
              " ('northern', 239),\n",
              " ('major', 239),\n",
              " ('lot', 238),\n",
              " ('security', 238),\n",
              " ('district', 238),\n",
              " ('street', 237),\n",
              " ('countries', 237),\n",
              " ('efforts', 237),\n",
              " ('rt', 236),\n",
              " ('cant', 235),\n",
              " ('back', 235),\n",
              " ('village', 233),\n",
              " ('across', 233),\n",
              " ('without', 232),\n",
              " ('down', 232),\n",
              " ('rice', 232),\n",
              " ('near', 231),\n",
              " ('home', 230),\n",
              " ('place', 230),\n",
              " ('today', 229),\n",
              " ('quake', 229),\n",
              " ('god', 227),\n",
              " ('made', 227),\n",
              " ('pakistan', 227),\n",
              " ('cross', 226),\n",
              " ('morning', 226),\n",
              " ('southern', 226),\n",
              " ('his', 225),\n",
              " ('call', 225),\n",
              " ('important', 224),\n",
              " ('week', 224),\n",
              " ('against', 224),\n",
              " ('women', 223),\n",
              " ('least', 221),\n",
              " ('road', 221),\n",
              " ('use', 220),\n",
              " ('un', 220),\n",
              " ('damage', 220),\n",
              " ('several', 219),\n",
              " ('lost', 219),\n",
              " ('killed', 219),\n",
              " ('system', 216),\n",
              " ('tsunami', 215),\n",
              " ('another', 214),\n",
              " ('provided', 213),\n",
              " ('january', 213),\n",
              " ('way', 213),\n",
              " ('crops', 213),\n",
              " ('news', 212),\n",
              " ('authorities', 212),\n",
              " ('never', 212),\n",
              " ('roads', 212),\n",
              " ('drought', 210),\n",
              " ('hello', 209),\n",
              " ('needed', 209),\n",
              " ('disease', 209),\n",
              " ('nothing', 207),\n",
              " ('3', 207),\n",
              " ('hygiene', 207),\n",
              " ('central', 206),\n",
              " ('diseases', 205),\n",
              " ('severe', 205),\n",
              " ('rainfall', 205),\n",
              " ('did', 204),\n",
              " ('public', 203),\n",
              " ('following', 203),\n",
              " ('parts', 202),\n",
              " ('along', 201),\n",
              " ('10', 199),\n",
              " ('officials', 199),\n",
              " ('1', 198),\n",
              " ('say', 196),\n",
              " ('12', 195),\n",
              " ('part', 195),\n",
              " ('land', 195),\n",
              " ('chile', 195),\n",
              " ('blankets', 194),\n",
              " ('must', 194),\n",
              " ('population', 193),\n",
              " ('rescue', 190),\n",
              " ('risk', 190),\n",
              " ('s', 187),\n",
              " ('items', 187),\n",
              " ('communities', 187),\n",
              " ('used', 186),\n",
              " ('providing', 186),\n",
              " ('small', 185),\n",
              " ('project', 185),\n",
              " ('tell', 184),\n",
              " ('cyclone', 184),\n",
              " ('districts', 184),\n",
              " ('every', 183),\n",
              " ('away', 183),\n",
              " ('season', 183),\n",
              " ('facilities', 183),\n",
              " ('evening', 182),\n",
              " ('person', 182),\n",
              " ('drinking', 182),\n",
              " ('military', 182),\n",
              " ('next', 181),\n",
              " ('she', 181),\n",
              " ('bitly', 181),\n",
              " ('recent', 180),\n",
              " ('dead', 179),\n",
              " ('winter', 179),\n",
              " ('electricity', 178),\n",
              " ('large', 178),\n",
              " ('west', 176),\n",
              " ('possible', 176),\n",
              " ('management', 176),\n",
              " ('found', 175),\n",
              " ('problems', 175),\n",
              " ('however', 175),\n",
              " ('farmers', 175),\n",
              " ('died', 174),\n",
              " ('capital', 174),\n",
              " ('coming', 174),\n",
              " ('equipment', 174),\n",
              " ('asking', 173),\n",
              " ('workers', 173),\n",
              " ('service', 172),\n",
              " ('la', 171),\n",
              " ('services', 171),\n",
              " ('little', 171),\n",
              " ('states', 171),\n",
              " ('kits', 171),\n",
              " ('systems', 171),\n",
              " ('few', 170),\n",
              " ('schools', 170),\n",
              " ('each', 170),\n",
              " ('early', 170),\n",
              " ('hunger', 169),\n",
              " ('20', 169),\n",
              " ('months', 168),\n",
              " ('continue', 168),\n",
              " ('crop', 168),\n",
              " ('4', 167),\n",
              " ('four', 167),\n",
              " ('residents', 167),\n",
              " ('sanitation', 167),\n",
              " ('delmas', 166),\n",
              " ('open', 166),\n",
              " ('town', 166),\n",
              " ('problem', 165),\n",
              " ('expected', 165),\n",
              " ('night', 164),\n",
              " ('island', 164),\n",
              " ('5', 163),\n",
              " ('yet', 163),\n",
              " ('displaced', 163),\n",
              " ('supply', 162),\n",
              " ('police', 162),\n",
              " ('sent', 162),\n",
              " ('life', 162),\n",
              " ('wfp', 162),\n",
              " ('provinces', 161),\n",
              " ('ask', 161),\n",
              " ('camps', 161),\n",
              " ('further', 160),\n",
              " ('card', 160),\n",
              " ('receive', 159),\n",
              " ('zone', 159),\n",
              " ('id', 159),\n",
              " ('de', 158),\n",
              " ('too', 156),\n",
              " ('five', 156),\n",
              " ('reports', 156),\n",
              " ('production', 156),\n",
              " ('got', 155),\n",
              " ('main', 155),\n",
              " ('again', 155),\n",
              " ('both', 154),\n",
              " ('survivors', 153),\n",
              " ('100', 153),\n",
              " ('human', 153),\n",
              " ('safe', 153),\n",
              " ('programme', 152),\n",
              " ('baby', 151),\n",
              " ('think', 151),\n",
              " ('coast', 151),\n",
              " ('30', 151),\n",
              " ('agricultural', 151),\n",
              " ('buildings', 150),\n",
              " ('groups', 150),\n",
              " ('eat', 149),\n",
              " ('temporary', 149),\n",
              " ('hungry', 149),\n",
              " ('bring', 149),\n",
              " ('sleep', 149),\n",
              " ('activities', 149),\n",
              " ('sleeping', 148),\n",
              " ('money', 148),\n",
              " ('tuesday', 148),\n",
              " ('month', 148),\n",
              " ('long', 148),\n",
              " ('africa', 148),\n",
              " ('poor', 147),\n",
              " ('june', 147),\n",
              " ('others', 147),\n",
              " ('refugees', 146),\n",
              " ('past', 146),\n",
              " ('carrefour', 145),\n",
              " ('portauprince', 145),\n",
              " ('something', 145),\n",
              " ('per', 145),\n",
              " ('building', 145),\n",
              " ('access', 145),\n",
              " ('why', 145),\n",
              " ('cold', 144),\n",
              " ('let', 144),\n",
              " ('etc', 144),\n",
              " ('put', 144),\n",
              " ('port', 143),\n",
              " ('clothing', 143),\n",
              " ('result', 143),\n",
              " ('china', 143),\n",
              " ('phone', 142),\n",
              " ('lives', 142),\n",
              " ('office', 142),\n",
              " ('agriculture', 142),\n",
              " ('waiting', 141),\n",
              " ('answer', 141),\n",
              " ('able', 141),\n",
              " ('ground', 141),\n",
              " ('control', 141),\n",
              " ('include', 141),\n",
              " ('july', 141),\n",
              " ('group', 140),\n",
              " ('until', 140),\n",
              " ('among', 140),\n",
              " ('percent', 140),\n",
              " ('monsoon', 140),\n",
              " ('staff', 139),\n",
              " ('camp', 138),\n",
              " ('hundreds', 138),\n",
              " ('cases', 138),\n",
              " ('afghanistan', 138),\n",
              " ('told', 137),\n",
              " ('unicef', 137),\n",
              " ('program', 137),\n",
              " ('regions', 137),\n",
              " ('political', 136),\n",
              " ('tons', 136),\n",
              " ('15', 135),\n",
              " ('ministry', 135),\n",
              " ('death', 134),\n",
              " ('natural', 134),\n",
              " ('clean', 134),\n",
              " ('air', 134),\n",
              " ('minister', 134),\n",
              " ('prevent', 134),\n",
              " ('winds', 134),\n",
              " ('eastern', 134),\n",
              " ('fire', 133),\n",
              " ('members', 133),\n",
              " ('shelters', 133),\n",
              " ('organization', 132),\n",
              " ('center', 132),\n",
              " ('love', 132),\n",
              " ('strong', 132),\n",
              " ('east', 132),\n",
              " ('far', 131),\n",
              " ('clothes', 130),\n",
              " ('havent', 130),\n",
              " ('cannot', 130),\n",
              " ('weeks', 130),\n",
              " ('infrastructure', 130),\n",
              " ('six', 130),\n",
              " ('addition', 130),\n",
              " ('general', 129),\n",
              " ('really', 129),\n",
              " ('then', 129),\n",
              " ('march', 129),\n",
              " ('training', 129),\n",
              " ('temperatures', 129),\n",
              " ('someone', 128),\n",
              " ('available', 128),\n",
              " ('october', 128),\n",
              " ('india', 128),\n",
              " ('distributed', 127),\n",
              " ('department', 127),\n",
              " ('end', 127),\n",
              " ('operations', 127),\n",
              " ('says', 126),\n",
              " ('teams', 126),\n",
              " ('right', 126),\n",
              " ('8', 126),\n",
              " ('address', 126),\n",
              " ('army', 126),\n",
              " ('hours', 126),\n",
              " ('coastal', 126),\n",
              " ('increased', 126),\n",
              " ('4636', 125),\n",
              " ('order', 125),\n",
              " ('haitian', 125),\n",
              " ('cause', 125),\n",
              " ('worst', 125),\n",
              " ('agency', 125),\n",
              " ('massive', 125),\n",
              " ('aceh', 125),\n",
              " ('set', 123),\n",
              " ('report', 123),\n",
              " ('snow', 123),\n",
              " ('rivers', 123),\n",
              " ('levels', 123),\n",
              " ('president', 122),\n",
              " ('treatment', 122),\n",
              " ('resources', 122),\n",
              " ('especially', 121),\n",
              " ('agencies', 121),\n",
              " ('dying', 119),\n",
              " ('suffering', 119),\n",
              " ('given', 119),\n",
              " ('construction', 119),\n",
              " ('keep', 119),\n",
              " ('centre', 119),\n",
              " ('organizations', 119),\n",
              " ('total', 119),\n",
              " ('crisis', 119),\n",
              " ('economic', 119),\n",
              " ('called', 118),\n",
              " ('rural', 118),\n",
              " ('pass', 117),\n",
              " ('victim', 117),\n",
              " ('monday', 117),\n",
              " ('official', 117),\n",
              " ('afp', 117),\n",
              " ('wells', 117),\n",
              " ('50', 116),\n",
              " ('regional', 116),\n",
              " ('level', 115),\n",
              " ('vulnerable', 115),\n",
              " ('livestock', 115),\n",
              " ('collapsed', 114),\n",
              " ('currently', 114),\n",
              " ('70', 114),\n",
              " ('increase', 114),\n",
              " ('operation', 114),\n",
              " ('projects', 114),\n",
              " ('lack', 113),\n",
              " ('change', 113),\n",
              " ('same', 113),\n",
              " ('patients', 113),\n",
              " ('her', 113),\n",
              " ('6', 113),\n",
              " ('better', 113),\n",
              " ('education', 113),\n",
              " ('almost', 112),\n",
              " ('hope', 112),\n",
              " ('fever', 112),\n",
              " ('products', 112),\n",
              " ('thursday', 112),\n",
              " ('capacity', 112),\n",
              " ('nearly', 112),\n",
              " ('particularly', 112),\n",
              " ('disasters', 112),\n",
              " ('radio', 111),\n",
              " ('killing', 111),\n",
              " ('malaria', 111),\n",
              " ('reconstruction', 111),\n",
              " ('materials', 111),\n",
              " ('ebola', 111),\n",
              " ('big', 110),\n",
              " ('getting', 110),\n",
              " ('april', 110),\n",
              " ('located', 109),\n",
              " ('everything', 109),\n",
              " ('bad', 109),\n",
              " ('child', 109),\n",
              " ('basic', 109),\n",
              " ('fund', 109),\n",
              " ('recovery', 109),\n",
              " ('outside', 108),\n",
              " ('came', 108),\n",
              " ('things', 108),\n",
              " ('plastic', 108),\n",
              " ('25', 108),\n",
              " ('sunday', 108),\n",
              " ('august', 108),\n",
              " ('climate', 108),\n",
              " ('ensure', 108),\n",
              " ('does', 107),\n",
              " ('urgent', 107),\n",
              " ('start', 107),\n",
              " ('sea', 107),\n",
              " ('rue', 106),\n",
              " ('24', 106),\n",
              " ('within', 106),\n",
              " ('foreign', 106),\n",
              " ('continued', 106),\n",
              " ('landslides', 106),\n",
              " ('funds', 106),\n",
              " ('wednesday', 105),\n",
              " ('serious', 105),\n",
              " ('friday', 105),\n",
              " ('global', 104),\n",
              " ('injured', 104),\n",
              " ('countrys', 104),\n",
              " ('immediate', 104),\n",
              " ('aftershocks', 103),\n",
              " ('african', 102),\n",
              " ('february', 102),\n",
              " ('using', 101),\n",
              " ('leogane', 101),\n",
              " ('au', 101),\n",
              " ('oil', 101),\n",
              " ('11', 101),\n",
              " ('maize', 101),\n",
              " ('mission', 100),\n",
              " ('site', 100),\n",
              " ('helicopters', 100),\n",
              " ('14', 100),\n",
              " ('centres', 100),\n",
              " ('additional', 100),\n",
              " ('indian', 100),\n",
              " ('streets', 99),\n",
              " ('housing', 99),\n",
              " ('making', 99),\n",
              " ('medicine', 98),\n",
              " ('american', 98),\n",
              " ('2000', 98),\n",
              " ('plant', 98),\n",
              " ('donations', 98),\n",
              " ('outbreak', 98),\n",
              " ('tomorrow', 97),\n",
              " ('done', 97),\n",
              " ('everyone', 97),\n",
              " ('stay', 97),\n",
              " ('enough', 97),\n",
              " ('km', 97),\n",
              " ('plan', 97),\n",
              " ('return', 97),\n",
              " ('struck', 97),\n",
              " ('started', 97),\n",
              " ('donate', 97),\n",
              " ('spread', 97),\n",
              " ('december', 97),\n",
              " ('nyc', 97),\n",
              " ('november', 97),\n",
              " ('tropical', 97),\n",
              " ('violence', 96),\n",
              " ('helping', 96),\n",
              " ('missing', 96),\n",
              " ('info', 96),\n",
              " ('forces', 96),\n",
              " ('informations', 96),\n",
              " ('force', 96),\n",
              " ('border', 96),\n",
              " ('improve', 96),\n",
              " ('hurricanesandy', 96),\n",
              " ('concepcion', 96),\n",
              " ('indonesia', 96),\n",
              " ('giving', 95),\n",
              " ('difficult', 95),\n",
              " ('homeless', 95),\n",
              " ('second', 95),\n",
              " ('earthquakes', 95),\n",
              " ('carrying', 95),\n",
              " ('sms', 94),\n",
              " ('true', 94),\n",
              " ('hi', 94),\n",
              " ('waters', 94),\n",
              " ('committee', 94),\n",
              " ('200', 94),\n",
              " ('protection', 94),\n",
              " ('wheat', 94),\n",
              " ('transport', 94),\n",
              " ('widespread', 94),\n",
              " ('sri', 94),\n",
              " ('warning', 93),\n",
              " ('u', 93),\n",
              " ('reach', 93),\n",
              " ('council', 93),\n",
              " ('civil', 92),\n",
              " ('build', 92),\n",
              " ('persons', 92),\n",
              " ('causing', 92),\n",
              " ('rights', 92),\n",
              " ('virus', 92),\n",
              " ('torrential', 92),\n",
              " ('prince', 91),\n",
              " ('taking', 91),\n",
              " ('friends', 91),\n",
              " ('goods', 91),\n",
              " ('refugee', 91),\n",
              " ('partners', 91),\n",
              " ('governments', 91),\n",
              " ('arrived', 90),\n",
              " ('magnitude', 90),\n",
              " ('scale', 90),\n",
              " ('remain', 90),\n",
              " ('irrigation', 90),\n",
              " ('digicel', 89),\n",
              " ('save', 89),\n",
              " ('old', 89),\n",
              " ('great', 89),\n",
              " ('debris', 89),\n",
              " ('26', 89),\n",
              " ('sources', 89),\n",
              " ('trees', 89),\n",
              " ('estimated', 89),\n",
              " ('acute', 89),\n",
              " ('cut', 88),\n",
              " ('miles', 88),\n",
              " ('doing', 88),\n",
              " ('foods', 88),\n",
              " ('september', 87),\n",
              " ('medicines', 87),\n",
              " ('assessment', 87),\n",
              " ('measures', 87),\n",
              " ('wind', 87),\n",
              " ('rehabilitation', 87),\n",
              " ('understand', 86),\n",
              " ('heard', 86),\n",
              " ('lines', 86),\n",
              " ('current', 86),\n",
              " ('messages', 85),\n",
              " ('leaving', 85),\n",
              " ('huge', 85),\n",
              " ('concern', 85),\n",
              " ('fires', 85),\n",
              " ('late', 85),\n",
              " ('heat', 85),\n",
              " ('social', 84),\n",
              " ('energy', 84),\n",
              " ('young', 84),\n",
              " ('toll', 84),\n",
              " ('below', 84),\n",
              " ('added', 84),\n",
              " ('impact', 84),\n",
              " ('continues', 84),\n",
              " ('happy', 83),\n",
              " ('close', 83),\n",
              " ('action', 83),\n",
              " ('7', 83),\n",
              " ('name', 83),\n",
              " ('free', 83),\n",
              " ('built', 83),\n",
              " ('soil', 83),\n",
              " ('likely', 83),\n",
              " ('volunteers', 83),\n",
              " ('taken', 83),\n",
              " ('contaminated', 83),\n",
              " ('mali', 83),\n",
              " ('together', 82),\n",
              " ('stop', 82),\n",
              " ('gas', 82),\n",
              " ('leave', 82),\n",
              " ('men', 82),\n",
              " ('means', 82),\n",
              " ('places', 82),\n",
              " ('via', 82),\n",
              " ('fuel', 82),\n",
              " ('future', 82),\n",
              " ('throughout', 82),\n",
              " ('triggered', 82),\n",
              " ('bangladesh', 82),\n",
              " ('somalia', 82),\n",
              " ('des', 81),\n",
              " ('sheets', 81),\n",
              " ('13', 81),\n",
              " ('western', 81),\n",
              " ('saturday', 81),\n",
              " ('armed', 81),\n",
              " ('1000', 81),\n",
              " ('dry', 81),\n",
              " ('assist', 81),\n",
              " ('harvest', 81),\n",
              " ('cholera', 81),\n",
              " ('500', 80),\n",
              " ('airport', 80),\n",
              " ('mobile', 80),\n",
              " ('case', 80),\n",
              " ('full', 80),\n",
              " ('40', 80),\n",
              " ('despite', 80),\n",
              " ('times', 80),\n",
              " ('process', 80),\n",
              " ('hand', 80),\n",
              " ('cooking', 80),\n",
              " ('ago', 80),\n",
              " ('normal', 80),\n",
              " ('elections', 80),\n",
              " ('didnt', 79),\n",
              " ('bodies', 79),\n",
              " ('move', 79),\n",
              " ('18', 79),\n",
              " ('2010', 79),\n",
              " ('forced', 79),\n",
              " ('low', 79),\n",
              " ('students', 78),\n",
              " ('doctors', 78),\n",
              " ('distributing', 78),\n",
              " ('seen', 78),\n",
              " ('passport', 78),\n",
              " ('hour', 78),\n",
              " ('peace', 78),\n",
              " ('latrines', 78),\n",
              " ('essential', 78),\n",
              " ('evacuation', 78),\n",
              " ('brought', 77),\n",
              " ('location', 77),\n",
              " ('kids', 77),\n",
              " ('ok', 77),\n",
              " ('storms', 77),\n",
              " ('media', 77),\n",
              " ('led', 77),\n",
              " ('various', 77),\n",
              " ('body', 76),\n",
              " ('bless', 76),\n",
              " ('st', 76),\n",
              " ('head', 76),\n",
              " ('hot', 76),\n",
              " ('distribute', 76),\n",
              " ('cleaning', 76),\n",
              " ('private', 76),\n",
              " ('environmental', 76),\n",
              " ('forest', 76),\n",
              " ('bridges', 76),\n",
              " ('theres', 75),\n",
              " ('route', 75),\n",
              " ('half', 75),\n",
              " ('whole', 75),\n",
              " ('60', 75),\n",
              " ('cities', 75),\n",
              " ('seeds', 75),\n",
              " ('flash', 75),\n",
              " ('flooded', 75),\n",
              " ('campaign', 75),\n",
              " ('devastating', 75),\n",
              " ('deaths', 75),\n",
              " ('section', 74),\n",
              " ('visit', 74),\n",
              " ('9', 74),\n",
              " ('become', 74),\n",
              " ('market', 74),\n",
              " ('plans', 74),\n",
              " ('soldiers', 74),\n",
              " ('effort', 74),\n",
              " ('republic', 74),\n",
              " ('nation', 74),\n",
              " ('monitoring', 74),\n",
              " ('programs', 74),\n",
              " ('improved', 74),\n",
              " ('university', 73),\n",
              " ('advance', 73),\n",
              " ('repair', 73),\n",
              " ('forecast', 73),\n",
              " ('coordination', 73),\n",
              " ('cash', 73),\n",
              " ('conflict', 73),\n",
              " ('inside', 72),\n",
              " ('hard', 72),\n",
              " ('ready', 72),\n",
              " ('infections', 72),\n",
              " ('ngos', 72),\n",
              " ('recently', 72),\n",
              " ('although', 72),\n",
              " ('party', 72),\n",
              " ('reduce', 72),\n",
              " ('outbreaks', 72),\n",
              " ('earlier', 72),\n",
              " ('fresh', 72),\n",
              " ('frankenstorm', 72),\n",
              " ('potential', 72),\n",
              " ('significant', 72),\n",
              " ('appeal', 72),\n",
              " ('lanka', 72),\n",
              " ('malnutrition', 72),\n",
              " ('units', 72),\n",
              " ('field', 71),\n",
              " ('suffered', 71),\n",
              " ('ocean', 71),\n",
              " ('unit', 71),\n",
              " ('tanks', 71),\n",
              " ('often', 71),\n",
              " ('remote', 71),\n",
              " ('farming', 71),\n",
              " ('sector', 71),\n",
              " ('irin', 71),\n",
              " ('having', 70),\n",
              " ('die', 70),\n",
              " ('16', 70),\n",
              " ('higher', 70),\n",
              " ('necessary', 70),\n",
              " ('experts', 70),\n",
              " ('mosquito', 70),\n",
              " ('household', 70),\n",
              " ('asia', 70),\n",
              " ('27', 69),\n",
              " ('request', 69),\n",
              " ('took', 69),\n",
              " ('17', 69),\n",
              " ('feel', 69),\n",
              " ('troops', 69),\n",
              " ('feeding', 69),\n",
              " ('county', 69),\n",
              " ('diarrhoea', 69),\n",
              " ('ongoing', 69),\n",
              " ('tonnes', 69),\n",
              " ('m', 68),\n",
              " ('network', 68),\n",
              " ('planning', 68),\n",
              " ('21', 68),\n",
              " ('destruction', 68),\n",
              " ('seven', 68),\n",
              " ('waves', 68),\n",
              " ('reduced', 68),\n",
              " ('kenya', 68),\n",
              " ('station', 67),\n",
              " ('doesnt', 67),\n",
              " ('broken', 67),\n",
              " ('best', 67),\n",
              " ('line', 67),\n",
              " ('rainy', 67),\n",
              " ('largest', 67),\n",
              " ('2005', 67),\n",
              " ('unhcr', 67),\n",
              " ('japan', 67),\n",
              " ('contact', 66),\n",
              " ('anymore', 66),\n",
              " ('went', 66),\n",
              " ('based', 66),\n",
              " ('law', 66),\n",
              " ('clear', 66),\n",
              " ('occurred', 66),\n",
              " ('daily', 66),\n",
              " ('meteorological', 66),\n",
              " ('warned', 66),\n",
              " ('washed', 66),\n",
              " ('provision', 66),\n",
              " ('front', 65),\n",
              " ('b', 65),\n",
              " ('protect', 65),\n",
              " ('23', 65),\n",
              " ('car', 65),\n",
              " ('student', 65),\n",
              " ('reached', 65),\n",
              " ('22', 65),\n",
              " ('always', 65),\n",
              " ('known', 65),\n",
              " ('special', 65),\n",
              " ('severely', 65),\n",
              " ('saw', 65),\n",
              " ('began', 65),\n",
              " ('waste', 65),\n",
              " ('fish', 65),\n",
              " ('key', 65),\n",
              " ('swept', 65),\n",
              " ('sumatra', 65),\n",
              " ('side', 64),\n",
              " ('heart', 64),\n",
              " ('mother', 64),\n",
              " ('pay', 64),\n",
              " ('mountain', 64),\n",
              " ('wait', 64),\n",
              " ('kind', 64),\n",
              " ('n', 64),\n",
              " ('flights', 64),\n",
              " ('material', 64),\n",
              " ('affecting', 64),\n",
              " ('approximately', 64),\n",
              " ('federation', 64),\n",
              " ('light', 64),\n",
              " ('election', 64),\n",
              " ('livelihoods', 64),\n",
              " ('jacmel', 63),\n",
              " ('look', 63),\n",
              " ('him', 63),\n",
              " ('critical', 63),\n",
              " ('post', 63)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiBhLryJz_6K"
      },
      "source": [
        "data_2, count_2, dictionary_2, reversed_dictionary_2  = tokenizer_2(text_corpus)\n",
        "df_train_tok2 = pd.DataFrame(df_train['message'].apply(get_tok_sequence, vocab_dict=dictionary_2), columns=['message'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-prQDSb0PiT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "917409ef-6aff-4900-d3ef-335d28383a12"
      },
      "source": [
        "df_train_tok2.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20231</th>\n",
              "      <td>[1, 15252, 5112, 685, 4982, 345, 163, 987, 23,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8658</th>\n",
              "      <td>[1, 64, 119, 70, 5, 76, 8, 2231, 4631, 20250, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8449</th>\n",
              "      <td>[1, 746, 38, 6019, 7779, 247, 0, 2623, 83, 6, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8551</th>\n",
              "      <td>[1, 24, 16, 441, 1708, 1140, 54, 23, 1580, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18220</th>\n",
              "      <td>[1, 32229, 588, 29, 32, 15022, 5, 6, 10548, 12...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 message\n",
              "20231  [1, 15252, 5112, 685, 4982, 345, 163, 987, 23,...\n",
              "8658   [1, 64, 119, 70, 5, 76, 8, 2231, 4631, 20250, ...\n",
              "8449   [1, 746, 38, 6019, 7779, 247, 0, 2623, 83, 6, ...\n",
              "8551       [1, 24, 16, 441, 1708, 1140, 54, 23, 1580, 2]\n",
              "18220  [1, 32229, 588, 29, 32, 15022, 5, 6, 10548, 12..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYLWM3LklRQ3"
      },
      "source": [
        "def get_subword_tok_sequence(text, vocab_dict):\n",
        "  subword_tokens = sub_word_tokenizer(text)\n",
        "  seq=[]\n",
        "  for word in subword_tokens:\n",
        "    if word in vocab_dict.keys():\n",
        "      seq.append(vocab_dict[word])\n",
        "    else:\n",
        "      seq.append(vocab_dict['UNK'])\n",
        "  return seq"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4GxBhhw1ZaZ"
      },
      "source": [
        "data_3, count_3, dictionary_3, reversed_dictionary_3 = tokenizer_3(text_corpus)\n",
        "df_train_tok3 = pd.DataFrame(df_train['message'].apply(get_subword_tok_sequence, vocab_dict=dictionary_3), columns=['message'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTlmonM8_yFi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "308356e2-4bd6-406a-c8fd-1254268be2a4"
      },
      "source": [
        "df_train_tok3.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20231</th>\n",
              "      <td>[1, 13799, 4757, 8, 525, 5, 3597, 5, 384, 186,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8658</th>\n",
              "      <td>[1, 73, 138, 10, 80, 9, 88, 12, 1748, 14, 837,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8449</th>\n",
              "      <td>[1, 813, 46, 4051, 4, 7145, 278, 0, 368, 54, 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8551</th>\n",
              "      <td>[1, 28, 21, 218, 5, 43, 1733, 873, 5, 1183, 62...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18220</th>\n",
              "      <td>[1, 13586, 4, 643, 37, 32, 7910, 4, 9, 3, 9562...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 message\n",
              "20231  [1, 13799, 4757, 8, 525, 5, 3597, 5, 384, 186,...\n",
              "8658   [1, 73, 138, 10, 80, 9, 88, 12, 1748, 14, 837,...\n",
              "8449   [1, 813, 46, 4051, 4, 7145, 278, 0, 368, 54, 9...\n",
              "8551   [1, 28, 21, 218, 5, 43, 1733, 873, 5, 1183, 62...\n",
              "18220  [1, 13586, 4, 643, 37, 32, 7910, 4, 9, 3, 9562..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-eaLNsNeHvw"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.3.2** - Count the bigrams in the dataset for each tokenizer and divide them by the total number of bigrams. This will give you the probability of each bigram.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc83X8dVeHvx"
      },
      "source": [
        "# Your code here\n",
        "def get_bigrams(df):\n",
        "  bigrams = []\n",
        "  for seq in df['message'].values:\n",
        "    bigrams.extend([(seq[i],seq[i+1]) for i in range(1,len(seq[1:-1]))])\n",
        "  bigrams_count_dict = Counter(bigrams)\n",
        "  return bigrams_count_dict\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O13NccKLSVQz"
      },
      "source": [
        "# def get_bigram_prob(bigrams_count_dict):\n",
        "#   bigram_prob_dict = {}\n",
        "#   for k, v in bigrams_count_dict.items():\n",
        "#     total = 0\n",
        "#     for key, value in bigrams_count_dict.items():\n",
        "#       if key[0] == k[0]:\n",
        "#         total+=1\n",
        "#     bigram_prob_dict[k] = v/total\n",
        "#   return bigram_prob_dict \n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggfq882eyyFf"
      },
      "source": [
        "def get_bigram_prob(bigrams_count_dict):\n",
        "  first_gram_count = {}\n",
        "  for k, v in bigrams_count_dict.items():\n",
        "    if k[0] in first_gram_count.keys():\n",
        "      first_gram_count[k[0]] += v\n",
        "    else:\n",
        "      first_gram_count[k[0]] = v\n",
        "\n",
        "  bigram_prob_dict = {k: v / first_gram_count[k[0]] for k, v in bigrams_count_dict.items()}\n",
        "  return bigram_prob_dict"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfUKg9sq0Wtr"
      },
      "source": [
        "# Bigram probability for tokenizer1  \n",
        "bigrams_count_dict_tok1 = get_bigrams(df_train_tok1)\n",
        "bigram_prob_dict_tok1 = get_bigram_prob(bigrams_count_dict_tok1)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqtH1rV_02FO"
      },
      "source": [
        "# Bigram probability for tokenizer2  \n",
        "bigrams_count_dict_tok2 = get_bigrams(df_train_tok2)\n",
        "bigram_prob_dict_tok2 = get_bigram_prob(bigrams_count_dict_tok2)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpwUfxmY1h2l"
      },
      "source": [
        "# Bigram probability for tokenizer3  \n",
        "bigrams_count_dict_tok3 = get_bigrams(df_train_tok3)\n",
        "bigram_prob_dict_tok3 = get_bigram_prob(bigrams_count_dict_tok3)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-0Eh7k6eHvx"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **1.4 [8 points] PREDICTING THE NEXT WORDS**\n",
        "<br />\n",
        "\n",
        "**1.4.1** - Simulate the incomplete messages dividing each sentence of the **test** set into two. For this, split each sentence in a 3:1 ratio. The first $75\\%$ of a sentence will represent the correct message, and the last $25\\%$ will convey the missing information. You will not give this 25% to your model, it is kept hidden. This 25% will only be used to evaluate the predictions of your language model.\n",
        "    \n",
        "For example in the sentence: *\"I will go out on a vacation, now that my semester ended.\"*\n",
        "\n",
        "The first 75% will be *\"I will go out on a vacation, now\"*\n",
        "\n",
        "The last 25% will be *\"that my semeter ended\"*\n",
        "\n",
        "Your aim is to predict the last part by giving your model the first \"part\" of the sentence.\n",
        "\n",
        "\n",
        "Note that in an n-gram language model, only the last $n-1$ words are used to make a prediction. For example, for the above sentence, if you are using bigrams, the input to your model would only be \"now\" and you are expected to predict \"that\".\n",
        "    \n",
        "<br /><br />   \n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-id0s97jeHvx"
      },
      "source": [
        "# Your code here\n",
        "def split_sent(word_seq):\n",
        "  msg_len = round(0.75*len(word_seq))\n",
        "  msg = word_seq[:msg_len]\n",
        "  missing_info = word_seq[msg_len:]\n",
        "  return msg, missing_info"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F27PVLL1eHvx"
      },
      "source": [
        "# get word sequence based on tokenizer_1 for test set \n",
        "df_val_tok1 = pd.DataFrame(df_val['message'].apply(get_tok_sequence, vocab_dict=dictionary), columns=['message'])\n",
        "\n",
        "# get word sequence based on tokenizer_2 for test set \n",
        "df_val_tok2 = pd.DataFrame(df_val['message'].apply(get_tok_sequence, vocab_dict=dictionary_2), columns=['message'])\n",
        "\n",
        "# get word sequence based on tokenizer_3 for test set \n",
        "df_val_tok3 = pd.DataFrame(df_val['message'].apply(get_subword_tok_sequence, vocab_dict=dictionary_3), columns=['message'])"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grVd-hLxSEtP"
      },
      "source": [
        "# split the msg in 3:1 ratio\n",
        "def split_sent_df(df):\n",
        "  message, missing_info = [],[]\n",
        "  for text_seq in df['message'].values:\n",
        "    x, y = split_sent(text_seq)\n",
        "    message.append(x)\n",
        "    missing_info.append(y)\n",
        "\n",
        "  df_split = pd.DataFrame({'message':message, 'missing_info':missing_info})\n",
        "  return df_split\n",
        "  "
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibq0sVLsSzod"
      },
      "source": [
        "df_val_tok1_split = split_sent_df(df_val_tok1)\n",
        "df_val_tok2_split = split_sent_df(df_val_tok2)\n",
        "df_val_tok3_split = split_sent_df(df_val_tok3)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "8fWeXNsvWXb7",
        "outputId": "92e3eaa9-aeb1-4a43-bba7-93f05adf4119"
      },
      "source": [
        "df_val_tok3_split.head()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "      <th>missing_info</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 172, 0, 1748, 993, 1679, 29, 23905, 51, 35...</td>\n",
              "      <td>[0, 714, 8, 7, 621, 10, 3997, 63, 1841, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 69, 749, 35, 22, 17039, 195, 339]</td>\n",
              "      <td>[41, 20, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 69, 99, 550, 5, 13, 11, 202, 56, 1199]</td>\n",
              "      <td>[37, 12, 155, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 230, 5, 7, 423, 616, 8, 73, 61, 97, 1145, ...</td>\n",
              "      <td>[6, 29337, 29338, 29339, 6, 29340, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 283, 508, 740, 33, 796, 4, 12068, 3178, 9,...</td>\n",
              "      <td>[892, 12069, 518, 5166, 20, 3, 500, 23972, 478...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message                                       missing_info\n",
              "0  [1, 172, 0, 1748, 993, 1679, 29, 23905, 51, 35...         [0, 714, 8, 7, 621, 10, 3997, 63, 1841, 2]\n",
              "1              [1, 69, 749, 35, 22, 17039, 195, 339]                                        [41, 20, 2]\n",
              "2         [1, 69, 99, 550, 5, 13, 11, 202, 56, 1199]                                   [37, 12, 155, 2]\n",
              "3  [1, 230, 5, 7, 423, 616, 8, 73, 61, 97, 1145, ...              [6, 29337, 29338, 29339, 6, 29340, 2]\n",
              "4  [1, 283, 508, 740, 33, 796, 4, 12068, 3178, 9,...  [892, 12069, 518, 5166, 20, 3, 500, 23972, 478..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdckQrYgeHvx"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "\n",
        "**1.4.2** - Given 5 sentences from the previous question (test set), predict the next word. \n",
        "Append this predicted word to the input sequence and predict the next one. Repeat this process until you reach the 10th token or the end of a sentence. Compare your results qualitatively with the original sentences. Do the results make sense wrt the context and semantics?\n",
        "\n",
        "Repeat this for all the models built using different tokenization techniques.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvx4yTcTXG8V"
      },
      "source": [
        "def predict_missing_word_unigram(msg, missing_info, dictionary, reversed_dictionary):\n",
        "  missing_text_actual = \"\"\n",
        "  for seq in missing_info[:min(10, len(missing_info))]:\n",
        "    missing_text_actual += reversed_dictionary[seq] + \" \"\n",
        "\n",
        "  missing_text_predicted = \"\"\n",
        "  for j in range(min(10, len(missing_info))):\n",
        "    next_word_seq = dictionary[reversed_dictionary[0]]\n",
        "    missing_text_predicted += reversed_dictionary[next_word_seq] + \" \"\n",
        "\n",
        "  return missing_text_actual, missing_text_predicted"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QOcBjmKag8g"
      },
      "source": [
        "def print_predicted_seq(df, dictionary, reversed_dictionary):\n",
        "  for i in range(5):\n",
        "    actual, pred = predict_missing_word_unigram(df['message'][i], df['missing_info'][i], dictionary, reversed_dictionary)\n",
        "    print(\"==== Sentence {} ====\".format(i+1))\n",
        "    print(\"Actual: \",actual)\n",
        "    print(\"Predicted: \",pred)\n",
        "    print()"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t87ngLI8hdrl",
        "outputId": "82b823dc-4f80-47b4-c009-2056974f3ccd"
      },
      "source": [
        "print_predicted_seq(df_val_tok1_split, dictionary, reversed_dictionary)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== Sentence 1 ====\n",
            "Actual:  the lines to better UNK their UNK </s> \n",
            "Predicted:  UNK UNK UNK UNK UNK UNK UNK UNK \n",
            "\n",
            "==== Sentence 2 ====\n",
            "Actual:  that </s> \n",
            "Predicted:  UNK UNK \n",
            "\n",
            "==== Sentence 3 ====\n",
            "Actual:  i call </s> \n",
            "Predicted:  UNK UNK UNK \n",
            "\n",
            "==== Sentence 4 ====\n",
            "Actual:  UNK UNK UNK and UNK </s> \n",
            "Predicted:  UNK UNK UNK UNK UNK UNK \n",
            "\n",
            "==== Sentence 5 ====\n",
            "Actual:  UNK transport UNK that include UNK UNK UNK and UNK \n",
            "Predicted:  UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Es43Rmh5Lx",
        "outputId": "f4364902-50aa-4900-95ff-d93414134256"
      },
      "source": [
        "print_predicted_seq(df_val_tok2_split, dictionary_2, reversed_dictionary_2)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== Sentence 1 ====\n",
            "Actual:  the lines to better reflect their status </s> \n",
            "Predicted:  the the the the the the the the \n",
            "\n",
            "==== Sentence 2 ====\n",
            "Actual:  that </s> \n",
            "Predicted:  the the \n",
            "\n",
            "==== Sentence 3 ====\n",
            "Actual:  i call </s> \n",
            "Predicted:  the the the \n",
            "\n",
            "==== Sentence 4 ====\n",
            "Actual:  ruhengeri nkamira mkwero and mudende </s> \n",
            "Predicted:  the the the the the the \n",
            "\n",
            "==== Sentence 5 ====\n",
            "Actual:  paf transport aircrafts that include y12 c130 casa and mi17 \n",
            "Predicted:  the the the the the the the the the the \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6966-Deh-XS",
        "outputId": "23b05292-fd4b-44b3-8f66-a9cfed483d8d"
      },
      "source": [
        "print_predicted_seq(df_val_tok3_split, dictionary_3, reversed_dictionary_3)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== Sentence 1 ====\n",
            "Actual:  the lin es to bett er reflect their status </s> \n",
            "Predicted:  the the the the the the the the the the \n",
            "\n",
            "==== Sentence 2 ====\n",
            "Actual:  me that </s> \n",
            "Predicted:  the the the \n",
            "\n",
            "==== Sentence 3 ====\n",
            "Actual:  can i call </s> \n",
            "Predicted:  the the the the \n",
            "\n",
            "==== Sentence 4 ====\n",
            "Actual:  and ruhengeri nkamira mkwero and mudende </s> \n",
            "Predicted:  the the the the the the the \n",
            "\n",
            "==== Sentence 5 ====\n",
            "Actual:  various paf transport aircrafts that in clude y12 c130 casa \n",
            "Predicted:  the the the the the the the the the the \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr2aiM6xnnyy"
      },
      "source": [
        "**We can clearly see that the perdicted word doesn't make sense wrt the context and semantics, since its always predicting the most frequent word.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVSW0IpFeHvx"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.4.3** - Repeat the same exercise, for all 3 models, but this time, the next token will be sampled from a distribution given by the bigram frequency. Compare and comment on the results?\n",
        "\n",
        "\n",
        "*Hint:* In a model of two bigrams with frequencies 0.7 and 0.3, a deterministic prediction will only predict the first bigram. Sampling from a distribution, will enable the model to predict the second bigram with a probability of 0.3. In this way we can still predict infrequent tokens. \n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y864JVr3eHvy"
      },
      "source": [
        "# Your code here\n",
        "def predict_next_word(msg, missing_info, dictionary, reversed_dictionary, probability_dict):\n",
        "  missing_text_actual = \"\"\n",
        "  for seq in missing_info[:min(10, len(missing_info))]:\n",
        "    missing_text_actual += reversed_dictionary[seq] + \" \"\n",
        "\n",
        "  missing_text_predicted=''\n",
        "  message = msg\n",
        "  for i in range(min(10, len(missing_info))):\n",
        "    last_word = message[-1]\n",
        "    probability_list = [v for k,v in probability_dict if k[0] == last_word]\n",
        "    next_word_list = [k[1] for k,v in probability_dict if k[0] == last_word]\n",
        "\n",
        "    next_word = np.random.choice(next_word_list, p=probability_list)\n",
        "    message.append(next_word)\n",
        "    missing_text_predicted += reversed_dictionary[next_word] + \" \"\n",
        "\n",
        "  return missing_text_actual, missing_text_predicted\n",
        "\n",
        "\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWB36AV1umc3"
      },
      "source": [
        "def print_predicted_seq(df, dictionary, reversed_dictionary, probability_dict):\n",
        "  for i in range(5):\n",
        "    actual, pred = predict_next_word(df['message'][i], df['missing_info'][i], dictionary, reversed_dictionary, probability_dict)\n",
        "    print(\"==== Sentence {} ====\".format(i+1))\n",
        "    print(\"Actual: \",actual)\n",
        "    print(\"Predicted: \",pred)\n",
        "    print()"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOr3zRJEAVhQ",
        "outputId": "6b238755-6036-4c70-e1f3-3913054fa75c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "print_predicted_seq(df_val_tok3_split, dictionary_3, reversed_dictionary_3, bigram_prob_dict_tok3)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-e66dc4eb6482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_predicted_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_val_tok3_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreversed_dictionary_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram_prob_dict_tok3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-83-5ba581deed17>\u001b[0m in \u001b[0;36mprint_predicted_seq\u001b[0;34m(df, dictionary, reversed_dictionary, probability_dict)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_predicted_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreversed_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_next_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'missing_info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreversed_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==== Sentence {} ====\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Actual: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-110782e65725>\u001b[0m in \u001b[0;36mpredict_next_word\u001b[0;34m(msg, missing_info, dictionary, reversed_dictionary, probability_dict)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlast_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprobability_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprobability_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlast_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mnext_word_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprobability_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlast_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-110782e65725>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlast_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprobability_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprobability_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlast_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mnext_word_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprobability_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlast_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCHmIDeWeHvy"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **1.5 [5 points] EVALUATE THE LANGUAGE MODELS**\n",
        "<br />\n",
        "\n",
        "    \n",
        "**1.5.1** - For each of the models built using different tokenization techniques, compute the average perplexity in the test set (part 1.1.3). Perform smoothing on the bigram models. Based on the perplexity, which model is better?\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knv9CpymeHvy"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8izi_pjTeHvy"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.5.2** - Given the perplexities, which model do you think is better? Why do you think so? Does this reflect the quality of the prediction as seen in part 1.4? \n",
        "\n",
        "What is the effect of UNK words?\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onbPzcy2eHvy"
      },
      "source": [
        "#### Type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MYWqq-7eHvy"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **1.6 [3 points] HOMEWORK QUIZ**\n",
        "<br />\n",
        "After attempting this part of the homework, answer the questions on edStem. All the questions depend on this part of the homework and you will not be able to answer it without attempting it.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC0qDXmNeHvy"
      },
      "source": [
        "#### Answer the questions on EdStem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1k22ZTSeHvy"
      },
      "source": [
        "___\n",
        "___\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAXiWcigeHvz"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "<h1>PART 2 [50 pts]:Word2Vec from scratch</h1>\n",
        "<br /><br />\n",
        "[Return to contents](#contents)\n",
        "<br /><br />\n",
        "\n",
        "<a id=\"part2intro\"></a>\n",
        "<h2> Problem Statement </h2>\n",
        "<br /><br />\n",
        "[Return to contents](#contents)\n",
        "<br /><br />\n",
        "\n",
        "Word2Vec architecture allows us to get *contextual* representations of word tokens.     \n",
        "<br /><br />\n",
        "There are several methods to build a word embedding. We will focus on the SGNS architecture. \n",
        "![](https://i.ibb.co/FW8Sr54/Screen-Shot-2021-04-27-at-3-27-16-PM.png)    \n",
        "<br /><br />\n",
        "In this problem, you are asked to build and analyze a Word2Vec architecture trained on wikipedia articles.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns9A1eXTeHvz"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "## **PART 2 [50 pts]: Word2Vec from scratch** \n",
        "<br />\n",
        "\n",
        "Word2Vec architecture allows us to get *contextual* representations of word tokens.     \n",
        "<br /><br />\n",
        "There are several methods to build a word embedding. We will focus on the SGNS architecture. \n",
        "<br/>\n",
        "\n",
        "![](https://i.ibb.co/FW8Sr54/Screen-Shot-2021-04-27-at-3-27-16-PM.png)    \n",
        "<br />\n",
        "In this problem, you are asked to build and analyze a Word2Vec architecture trained on wikipedia articles.\n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CruesQYZeHvz"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "## **PART 2 Questions**\n",
        "<br />\n",
        "    \n",
        "### **2.1 [5 points] MODEL PROCESSING**\n",
        "<br />\n",
        "    \n",
        "**2.1.1 Get the data:**    \n",
        "\n",
        "- Get the data from the `text8.zip` file.\n",
        "    `text8.zip` is a small, *cleaned* subset of a large corpus of data scraped from wikipedia pages. More details can be found [here](https://paperswithcode.com/sota/language-modelling-on-text8).\n",
        "    It is usually used to quickly train, or test language models.\n",
        "    [Read here](http://mattmahoney.net/dc/textdata#:~:text=The%20purpose%20of%20the%20smaller,on%20the%20larger%20data%20set.&text=The%20two%20files%20have%20the,108%20bytes%20of%20fil9.) for more information.\n",
        "- Split the data by whitespace and print the first 10 words to check if has been correctly loaded.\n",
        "\n",
        "    **NOTE:** For this part of the homework, all words will be in their lowercase for simplicity of analysis.\n",
        "<br />    \n",
        "\n",
        "**2.1.2 Build the dataset**  \n",
        "\n",
        "- Write a function that takes the `vocabulary_size` and `corpus` as input, and outputs:\n",
        "    - Tokenized data\n",
        "    - Count of each token\n",
        "    - A dictionary that maps words to tokens\n",
        "    - A dictionary that maps tokens to words.\n",
        "    You can use the same function used in **Lab 3**, or else you can use [`tf.keras.Tokenizer`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) to write a similar function.\n",
        "- Print the first 10 tokens and reverse them to words to confirm a match to the initial print above.\n",
        "     \n",
        "  \n",
        "Example:  \n",
        "\n",
        " `corpus[:10] = ['this','is,'an','example',...]`\n",
        "\n",
        "`data[:10] = [44,26,24,16,...]`\n",
        "    \n",
        "`reversed_data =['this','is,'an','example',...]`\n",
        "\n",
        "**NOTE**: Choose a sufficiently large vocabulary size. i.e `vocab_size>= 1000`    \n",
        "<br />\n",
        "    \n",
        "**2.1.3 Build skipgrams with negative samples:**  \n",
        "- Use the `tf.keras.preprocessing.sequence.skipgrams` function to build positive and negative samples for word2vec training. Follow the documentation on how to make the pairs, or see Lab 3 for an example.\n",
        "- You are free to choose your own `window_size`, but we recommend a value of 3.\n",
        "- Print 10 pairs of *center* and *context* words with their associated labels.    \n",
        "    \n",
        "### Skip-gram Sampling table\n",
        "A large dataset means larger vocabulary with higher number of more frequent words such as stopwords. Training examples obtained from sampling commonly occurring words (such as *the*, *is*, *on*) don't add much useful information for the model to learn from. [Mikolov et al.](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) suggest subsampling of frequent words as a helpful practice to improve embedding quality.\n",
        "\n",
        "The `tf.keras.preprocessing.sequence.skipgrams` function accepts a sampling table argument to encode probabilities of sampling any token. You can use the `tf.keras.preprocessing.sequence.make_sampling_table` to generate a word-frequency rank based probabilistic sampling table and pass it to skipgrams function.    \n",
        "<br />\n",
        "    \n",
        "**2.1.4 Conceptual question** \n",
        "    \n",
        "What is the difference between using a sampling table and not using a sampling table while building the dataset for skipgrams?\n",
        "(Answer in less than 200 words)\n",
        "<br /><br />\n",
        "    \n",
        "### **2.2 [8 points] BUILDING A WORD2VEC MODEL** \n",
        "<br />\n",
        "    \n",
        "Build a word2vec model architecture based on the schematic below.\n",
        "<center>\n",
        "    <img src=\"https://i.ibb.co/1LyGb0J/Screen-Shot-2021-04-23-at-10-50-52-AM.png\" alt=\"centered image\" width=\"200\"/>\n",
        "</center>    \n",
        "    \n",
        "- To do so, you will need:\n",
        "    - `tf.keras.layers.Embedding` layer\n",
        "    - `tf.keras.layers.Dot()`\n",
        "    - `tf.keras.Model()` which is the functional API\n",
        "- You can choose an appropriate embedding dimension\n",
        "- Compile the model using `binary_crossentropy()` function and an appropriate optimizer. \n",
        "- Sufficiently train the model.    \n",
        "It is generally a good practise to save the model weights. To save the model weights using the `model.save_weights()` for analysis of **2.3**. (More information on how to save your weights can be found [here](https://www.tensorflow.org/tutorials/keras/save_and_load))\n",
        "<br />\n",
        "\n",
        "\n",
        "### **2.3 [7 points] POST-TRAINING ANALYSIS**\n",
        "\n",
        "    \n",
        "This segment involves some simple analysis of your trained embeddings.\n",
        "<br /><br />\n",
        "    \n",
        "**2.3.1 Vector Algebra on Embeddings**\n",
        "\n",
        "- Assuming you have chosen a sufficiently large `vocab_size`, find the embeddings for:\n",
        "    \n",
        "1. King\n",
        "2. Male\n",
        "3. Female\n",
        "4. Queen\n",
        "    \n",
        "- Find the vector `v = King - Male + Female` and find it's `cosine_similarity()` with the embedding for 'Queen'.\n",
        "You can use the `cosine_similarity()` function defind in the exercise from lecture.\n",
        "\n",
        "**NOTE**: The `cosine_similarity()` value, must be greater than `0.9`; If it is not, this implies that your word2vec embeddings are not well-trained.\n",
        "\n",
        "- Write a function `most_similar()`, which finds the top-n words most similar to the given word.\n",
        "- Use this function to find the words most similar to `king`.\n",
        "    \n",
        "- **Conceptual Question** Why can't we use `cosine_similarity()` as a `loss_function`?\n",
        "(Answer in less than 200 words) \n",
        "    \n",
        "<br />\n",
        "    \n",
        "**2.3.2 Visualizing Embeddings**\n",
        "\n",
        "- Find the embeddings for the words:\n",
        "1. 'Six'\n",
        "2. 'Seven'\n",
        "3. 'Eight'\n",
        "4. 'Nine'\n",
        "    \n",
        "- Find the `cosine_similarity()` of 'six' with each of 'seven`,'eight','nine'.\n",
        "    \n",
        "- Reset your network (make sure your trained weights are saved), and again compute the `cosine_similarity()` values. The values should be small (because the embeddings are random).\n",
        "    \n",
        "- Use a demonstrative plot to show the `before & after training` the 4 embeddings. Here are some suggestions: \n",
        "    1. PCA/TSNE for dimensionality reduction\n",
        "    2. Radar plot to show all embedding dimensions\n",
        "    \n",
        "Bonus points for using creative means to demonstrate how the embeddings change after training.\n",
        "\n",
        "Here is a [video](https://youtu.be/VDl_iA8m8u0) of a sample demonstration. We used a custom callback to get embeddings during training.  \n",
        "        \n",
        "\n",
        "<br />\n",
        "    \n",
        "**2.3.3 Embedding and Context Matrix**\n",
        "    \n",
        "<br />\n",
        "    \n",
        "Investigate the relation between the Embedding & Context matrix. Again use the `cosine_similarity()` function to find the average value across all the words in the embedding and context matrix, i.e:\n",
        "\n",
        "- For a word 'dog', find the embedding value, and context value.\n",
        "- Calculate the `cosine_similarity()` between the two\n",
        "- Repeat the same for every word in the vocabulary and calculate the average value of the `cosine_similarity()\n",
        " \n",
        "<br /><br />\n",
        "    \n",
        "\n",
        "### **2.4 [5 points] LEARNING PHRASES**\n",
        "    \n",
        "As per the original paper by [Mikolov et al]() many phrases have a meaning that is not a simple composition of the meanings of its individual words. \n",
        "For eg. `new york` is one entity, however, as per our analysis above, we have two separate entities `new` & `york` which can have different meanings independently.    \n",
        "To learn vector representation for phrases, we first find words that\n",
        "appear frequently together, and infrequently in other contexts.\n",
        "    \n",
        "As per the analysis in the paper, we can use a formula to rank commonly used word pairs, and take the first 100 commonly occuring pairs.\n",
        "$$\\operatorname{score}\\left(w_{i}, w_{j}\\right)=\\frac{\\operatorname{count}\\left(w_{i} w_{j}\\right)-\\delta}{\\operatorname{count}\\left(w_{i}\\right) \\times \\operatorname{count}\\left(w_{j}\\right)}$$\n",
        "\n",
        "**NOTE:** For simplicity of analysis, we take the discounting factor $\\delta$ as 0, and take bi-gram combinations. You can experiment with tri-grams for word pairs such as `New_York_Times`.     \n",
        "<br /><br />\n",
        "\n",
        "    \n",
        "**2.4.1 Find 100 most common bi-grams**\n",
        "\n",
        "- From the tokenized data above, find the count for each bigram pair.\n",
        "    \n",
        "- For each such pair, find the score associated with each token pair using the formula above.\n",
        "    \n",
        "- Pick the top 100 pairs based on the score (higher is better). To understand the `score()` function we suggest you read the paper mentioned above.\n",
        "    \n",
        "- Replace the original `text8` file with the pairs as one entity. For e.g., if `prime,minister` is a commonly occuring pair, replace `... prime minister ...' in the original corpus to a single entity `prime_minister`. Do this for all 100 pairs.\n",
        "<br /><br />\n",
        "    \n",
        "**2.4.2 Retrain word2vec**    \n",
        "- With the new corpus generated as above, build the dataset, use skipgrams and retrain your word2vec with a sufficiently large vocabulary.\n",
        "    \n",
        "- Use the `most_similar()` function defiend above to find the entities most similar to `united_kingdom`.\n",
        "    \n",
        "- Compare the above with separate tokens for `united` & `kingdom` and the sum of the vectors (to get this, you may need a sufficiently large vocabulary (>2000).\n",
        "<br /> <br />\n",
        "\n",
        "    \n",
        "### **2.5 [5 points] HOMEWORK QUIZ**\n",
        "<br />\n",
        "After attempting this part of the homework, answer the questions on edStem. All the questions depend on this part of the homework and you will not be able to answer them without attempting this part.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Q2OJZ-eHvz"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "## **PART 2: Solutions**\n",
        "<br />\n",
        "    \n",
        "### **2.1 [5 points] MODEL PROCESSING**\n",
        "<br />\n",
        "    \n",
        "**2.1.1 Get the data:**    \n",
        "\n",
        "- Get the data from the `text8.zip` file.\n",
        "    `text8.zip` is a small, *cleaned* subset of a large corpus of data scraped from wikipedia pages.\n",
        "    It is usually used to quickly train, or test language models.\n",
        "    [Read here](http://mattmahoney.net/dc/textdata#:~:text=The%20purpose%20of%20the%20smaller,on%20the%20larger%20data%20set.&text=The%20two%20files%20have%20the,108%20bytes%20of%20fil9.) for more information\n",
        "- Split the data by whitespace print the first 10 words to check if has been correctly loaded.\n",
        "    \n",
        "**NOTE** : For this part of the homework, all words will be in their lowercase for simplicity of analysis\n",
        "<br />    <br />    \n",
        "\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iU--QnTeHv0"
      },
      "source": [
        "#### Helper function to read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lMX_3qYeHv0"
      },
      "source": [
        "# Helper code to read the data\n",
        "\n",
        "filename = 'text8.zip'\n",
        "with zipfile.ZipFile(filename) as f:\n",
        "# Read the data into a list of strings.\n",
        "    vocabulary = tf.compat.as_str(f.read(f.namelist()[0])).split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzXogMFXeHv0"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.1.2 Build the dataset**  \n",
        "\n",
        "- Write a function that takes the `vocabulary_size` and `corpus` as input, and outputs:\n",
        "    - Tokenized data\n",
        "    - count of each token\n",
        "    - A dictionary that maps words to tokens\n",
        "    - A dictionary that maps tokens to words\n",
        "    You can use the same function used in **Lab 3**, or else you can use [`tf.keras.Tokenizer`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) to write a similar function.\n",
        "- Print the first 10 tokens and reverse them to words to confirm a match to the initial print above.\n",
        "     \n",
        "  \n",
        "Eg. `corpus[:10] = ['this','is,'an','example',...]`\n",
        "\n",
        "`data[:10] = [44,26,24,16,...]`\n",
        "    \n",
        "`reversed_data =['this','is,'an','example',...]`\n",
        "\n",
        "**NOTE**: Choose a sufficiently large vocabulary size. i.e `vocab_size>= 1000`    \n",
        "<br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jVkW-3VeHv0"
      },
      "source": [
        "def build_dataset(words, n_words):\n",
        "    \"\"\"Process raw inputs into a dataset.\"\"\" \n",
        "    # Fill in to complete this function \n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(Counter(text_corpus.split()).most_common(vocabulary_size-1))\n",
        "    \n",
        "    dictionary={}\n",
        "    for i, tup in enumerate(count):\n",
        "        dictionary[tup[0]] = i\n",
        "           \n",
        "    data = []\n",
        "    unk_count = 0\n",
        "    \n",
        "    for word in text_corpus:\n",
        "        if word in dictionary:\n",
        "            token = dictionary[word]\n",
        "        else:\n",
        "            token = 0  \n",
        "            unk_count += 1\n",
        "            \n",
        "        data.append(token)\n",
        "        \n",
        "    count[0][1] = unk_count\n",
        "    \n",
        "    reversed_dictionary = {v:k for k,v in dictionary.items()}\n",
        "    \n",
        "    return data, count, dictionary, reversed_dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q2pa5Pv8AWl"
      },
      "source": [
        "def get_data_sequence(text, vocab_dict):\n",
        "  seq=[]\n",
        "  for word in text.split():\n",
        "    if word in vocab_dict.keys():\n",
        "      seq.append(vocab_dict[word])\n",
        "    else:\n",
        "      seq.append(vocab_dict['UNK'])\n",
        "  return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKyqEj6peHv0"
      },
      "source": [
        "vocab_size = 1000\n",
        "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
        "                                                                vocab_size)\n",
        "\n",
        "data_sequence = get_data_sequence(vocabulary[0], dictionary)\n",
        "print(f'Original sentence is: \"{\" \".join(vocabulary[0])}\"')\n",
        "print(f'Tokenized sentence is: \"{data_sequence}\"')\n",
        "print(f'Tokenized sentence reversed is: {\" \".join([reverse_dictionary[i] for i in data_sequence])}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG26-aeq7C1A"
      },
      "source": [
        "del vocabulary  # Hint to reduce memory."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm_kU0A2eHv0"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.1.3 Build skipgrams with negative samples:**  \n",
        "- Use the `tf.keras.preprocessing.sequence.skipgrams` function to build positive and negative samples \\\n",
        "    for word2vec training. Follow the documentation on how to make the pairs, or see Lab 3 for an example.\n",
        "- You are free to choose your own `window_size`, but we recommend a value of 3.\n",
        "- Print 10 pairs of *center* and *context* words with their associated labels.    \n",
        "    \n",
        "#### Skip-gram Sampling table\n",
        "A large dataset means larger vocabulary with higher number of more frequent words such as stopwords. Training examples obtained from sampling commonly occurring words (such as the, is, on) don't add much useful information for the model to learn from. [Mikolov et al.](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) suggest subsampling of frequent words as a helpful practice to improve embedding quality.\n",
        "\n",
        "The `tf.keras.preprocessing.sequence.skipgrams` function accepts a sampling table argument to encode probabilities of sampling any token. You can use the `tf.keras.preprocessing.sequence.make_sampling_table` to generate a word-frequency rank based probabilistic sampling table and pass it to skipgrams function.    \n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOA28WUPeHv1"
      },
      "source": [
        "# Your code here\n",
        "window_size = 3\n",
        "couples, labels = skipgrams(data,vocab_size, window_size=window_size)\n",
        "\n",
        "# Separate the target,context pairs as word_target, word_context \n",
        "word_center, word_context = zip(*couples)\n",
        "print(couples[:10], labels[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TqcUZFReHv1"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.1.4 Conceptual question** \n",
        "    \n",
        "What is the difference between using a sampling table and not using a sampling table while building the dataset for skipgrams?\n",
        "(Answer in less than 200 words)\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOvNIpsYeHv1"
      },
      "source": [
        "#### Type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsqeGIxOeHv1"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.2 [15 points]** **Building a word2vec model:** \n",
        "\n",
        "Build a word2vec model architecture based on the schematic below.\n",
        "<center>\n",
        "    <img src=\"https://i.ibb.co/1LyGb0J/Screen-Shot-2021-04-23-at-10-50-52-AM.png\" alt=\"centered image\" width=\"200\"/>\n",
        "</center>    \n",
        "    \n",
        "- To do so, you will need:\n",
        "    - `tf.keras.layers.Embedding` layer\n",
        "    - `tf.keras.layers.Dot()`\n",
        "    - `tf.keras.Model()` which is the functional API\n",
        "- You can choose an appropriate embedding dimension\n",
        "- Compile the model using `binary_crossentropy()` function and an appropriate optimizer. \n",
        "- Sufficiently train the model.\n",
        "    - Your model will be sufficiently trained when? (Check with Ignacio)    \n",
        "    \n",
        "It is generally a good practise to save the model weights. To save the model weights using the `model.save_weights()` for analysis of **2.3**. (More information on how to save your weights can be found [here](https://www.tensorflow.org/tutorials/keras/save_and_load))\n",
        "\n",
        "\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buKn58NbeHv1"
      },
      "source": [
        "# Your code here\n",
        "embedding_dim = 300\n",
        "\n",
        "word_model = Sequential()\n",
        "word_model.add(Input(shape=(1,)))\n",
        "word_model.add(Embedding(vocab_size, embedding_dim, input_length=1, name=\"Embedding\"))\n",
        "\n",
        "context_model = Sequential()\n",
        "context_model.add(Input(shape=(1,)))\n",
        "context_model.add(Embedding(vocab_size, embedding_dim, input_length=1, name=\"Embedding\"))\n",
        "\n",
        "dot_product = dot([word_model.output, context_model.output],axes=1,\n",
        "                  normalize=False,name='dotproduct')\n",
        "\n",
        "sigmoid_dot_product = Dense(1, activation=\"signmoid\", name=\"Dense Layer\")(dot_product)\n",
        "\n",
        "model = Model(inputs=[word_model.input, context_model.input], outputs=sigmoid_dot_product, name=\"Model\")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CB4RhwZB2Q3"
      },
      "source": [
        "model.compile(loss='bce',optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD0825jPCFpu"
      },
      "source": [
        "model.fit(couples, labels, epochs=5)\n",
        "model.save_weights('/content/drive/MyDrive/UnivAI/Univ AI 3/my_checkpoint')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTEJIshIQSWY"
      },
      "source": [
        "# Create a new model instance\n",
        "model = create_model()\n",
        "\n",
        "# Restore the weights\n",
        "model.load_weights('/content/drive/MyDrive/UnivAI/Univ AI 3/my_checkpoint')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0yyMVPCeHv1"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **2.3 [7 points] POST-TRAINING ANALYSIS**\n",
        "<br />\n",
        "    \n",
        "This segment involves some simple analysis of your trained embeddings.\n",
        "<br />\n",
        "    \n",
        "**2.3.1Vector Algebra on Embeddings**\n",
        "\n",
        "Assuming you have chosen a sufficiently large `vocab_size`, find the embeddings for:\n",
        "    \n",
        "1. King\n",
        "2. Male\n",
        "3. Female\n",
        "4. Queen\n",
        "    \n",
        "Find the vector `v = King - Male + Female` and find it's `cosine_similarity()` with the embedding for 'Queen'.\n",
        "You can use the `cosine_similarity()` function defind in session 3 exercise.\n",
        "\n",
        "**NOTE**:The `cosine_similarity()` value, must be greater than `0.9`; If it is not, this implies that your word2vec embeddings are not well-trained.\n",
        "\n",
        "Write a function `most_similar()`, which finds the top-n words most similar to the given word.\n",
        "    - Use this function to find the words most similar to `king`.\n",
        "    \n",
        "**Conceptual Question** Why can't we use `cosine_similarity()` as a `loss_function`?\n",
        "(Answer in less than 200 words) \n",
        "    \n",
        "<br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fa_rv-oeHv2"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtfmUBsFeHv2"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "\n",
        "**2.3.2 Visualizing Embeddings**\n",
        "\n",
        "Find the embeddings for the words:\n",
        "1. 'Six'\n",
        "2. 'Seven'\n",
        "3. 'Eight'\n",
        "4. 'Nine'\n",
        "    \n",
        "Find the `cosine_similarity()` of 'six' with each of 'seven`,'eight','nine' (which should be high values).\n",
        "    \n",
        "Reset your network (make sure your trained weights are saved), and again compute the `cosine_similarity()` values. The values should be small (because the embeddings are random).\n",
        "    \n",
        "Use a demonstrative plot to show the `before & after training` of the 4 embeddings. Here are some suggestions: \n",
        "    1. PCA/TSNE for dimensionality reduction\n",
        "    2. Radar plot to show all embedding dimensions\n",
        "    \n",
        "Bonus points for using creative means to demonstrate how the embeddings change after training.\n",
        "\n",
        "        \n",
        "\n",
        "<br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cApSXEheHv2"
      },
      "source": [
        "#### Here is a [video](https://youtu.be/VDl_iA8m8u0) of a sample demonstration. We used a custom callback to get embeddings during training.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1irhva4AeHv2"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.3.3 Embedding and Context Matrix**\n",
        "    \n",
        "<br />\n",
        "    \n",
        "Investigate the relation between the Embedding & Context matrix. Again use the `cosine_similarity()` function to find the average value across all the words in the embedding and context matrix, i.e:\n",
        "    - For a word 'dog', find the embedding value, and context value.\n",
        "    - Calculate the `cosine_similarity()` between the two\n",
        "    - Repeat the same for every word in the vocabulary and calculate the average value of the `cosine_similarity()`\n",
        "\n",
        "<br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-87UALh0eHv2"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhBrIaL7eHv3"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **2.4 [5 points] LEARNING PHRASES**\n",
        "    \n",
        "As per the original paper by [Mikolov et al]() many phrases have a meaning that is not a simple composition of the meanings of its individual words. \n",
        "For eg. `new york` is one entity, however, as per our analysis above, we have two separate entities `new` & `york` which can have different meanings independently.    \n",
        "To learn vector representation for phrases, we first find words that\n",
        "appear frequently together, and infrequently in other contexts.\n",
        "    \n",
        "As per the analysis in the paper, we can use a formula to rank commonly used word pairs, and take the first 100 commonly occuring pairs.\n",
        "$$\\operatorname{score}\\left(w_{i}, w_{j}\\right)=\\frac{\\operatorname{count}\\left(w_{i} w_{j}\\right)-\\delta}{\\operatorname{count}\\left(w_{i}\\right) \\times \\operatorname{count}\\left(w_{j}\\right)}$$\n",
        "\n",
        "**NOTE:** For simplicity of analysis, we take the discounting factor $\\delta$ as 0, and take bi-gram combinations. You can experiment with tri-grams for word pairs such as `New_York_Times`.     \n",
        "<br /><br />\n",
        "\n",
        "    \n",
        "**2.4.1 Find 100 most common bi-grams**\n",
        "\n",
        "From the tokenized data above, find the count for each bigram pair.\n",
        "    \n",
        "For each such pair, find the score associated with each token pair using the formula above.\n",
        "    \n",
        " Pick the top 100 pairs based on the score. (Higher the better). To understand the `score()` function we suggest you read the paper mentioned above.\n",
        "    \n",
        "Replace the original `text8` file with the pairs as one entity. For e.g., if `prime,minister` is a commonly occuring pair, replace `... prime minister ...' in the original corpus to a single entity `prime_minister`. Do this for all 100 pairs.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o0UGlHNeHv3"
      },
      "source": [
        "# Uncomment this cell to download the dataset directly onto colab\n",
        "# !gdown https://drive.google.com/uc?id=1yW-rQc8It9Ro0DAu4jp3XhVH3JQOdNKk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sObDbIQYeHv3"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uMogXlyeHv3"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.4.2 Retrain word2vec**    \n",
        "With the new corpus generated as above, build the dataset, use skipgrams and retrain your word2vec with a sufficiently large vocabulary.\n",
        "    \n",
        "Use the `most_similar()` function defiend above to find the entities most similar to `united_kingdom`\n",
        "    \n",
        "Compare the above with separate tokens for `united` & `kingdom` and the sum of the vectors (to get this, you may need a sufficiently large vocabulary (>2000).\n",
        "<br /> <br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFbEX2_1eHv3"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtdjuYgIeHv3"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **2.5 [5 points] HOMEWORK QUIZ**\n",
        "<br />\n",
        "After attempting this part of the homework, answer the questions on edStem. All the questions depend on this part of the homework and you will not be able to answer them without attempting this part.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTHaqVymeHv3"
      },
      "source": [
        "#### Answer the questions on edStem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_ktf1AweHv4"
      },
      "source": [
        "___\n",
        "___\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUtUZIgCeHv4"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "## **PART 3 [35 points] : Language Modelling using RNNs**\n",
        "<br />    \n",
        "\n",
        "In the last part of the homework, you are expected to build and train a language model. For this, we will be using Pavlos famous texts which end with `...` for prediction. Here, you will preprocess a data corpus and train your simple RNN network with it. With this network you will try to predict what he meant when he typed `...`. This can be to some extent a form of transfer learning.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAJ-DaMbw9Dw"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBnVsppEeHv4"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "## **PART 3: Questions**\n",
        "<br />\n",
        "\n",
        "### **3.1 [2 points] PREPROCESS THE DATASET**\n",
        "<br />\n",
        "\n",
        "**3.1.1** - Read in the dataset `imdb.csv`. Create a new dataframe by splitting each review into individual sentences. The sentences can be delimited by different characters such as period and question mark (eroteme). Call this column as `text` in the new dataframe.\n",
        "<br /><br />\n",
        "\n",
        "**3.1.2** - Define a function `clean_data` that takes the new dataframe as input and removes all html tags and non-alphabetic characters from the dataframe. Additionally, convert all characters to lower case. Remove all the sentences where the number of words is less than 10 and higher than 30. Finally, add the start token `<s>` and the end token `</s>` to every sentence (row) in the dataframe. Return the processed the dataframe. \n",
        "<br /><br />\n",
        "\n",
        "### **3.2 [2 points] TOKENIZE THE DATASET**\n",
        "<br />\n",
        "\n",
        "**3.2.1** - Tokenize the dataset using `tensorflow.keras.preprocessing.text` with a vocabulary size of 5000. Do **not** add an additional token for unknown words (out of vocabulary words) `<UNK>`.\n",
        "<br /><br />\n",
        "\n",
        "**3.2.2** - Fit the tokenizer on the dataset and get the sequence representation of each sentence.\n",
        "<br /><br />\n",
        "\n",
        "### **3.3 [10 points] MODELLING THE DATA**\n",
        "<br />\n",
        "\n",
        "**3.3.1** - The first step is to define the input and output of the model. The input to the model is all the words of the sentences except the last one. \n",
        "The output of the model is all words of the sentences except the first one. Using `tf.keras.preprocessing.sequence.pad_sequences` Post-pad all the sentences of the input and output to a length of 30.\n",
        "<br /><br />\n",
        "\n",
        "**3.3.2** - Define a simple RNN model that has an embedding layer with an embedding dimension of 300. You can define any number of RNN layers. The output of the RNN model will be a dense layer with size of the vocabulary and softmax activation. This model is that one you will be using to train the network. Use functional API for this to reuse it later on.\n",
        "<br /><br />\n",
        "\n",
        "**3.3.3** - Train the model with the input and output data formed above and a validation split of 0.2. The number epochs and batch size is left as a choice you have to make.\n",
        "<br /><br />\n",
        "\n",
        "**3.3.4** - Plot the train and validation loss. \n",
        "<br /><br />\n",
        "\n",
        "### **3.4 [9 points] PREDICTING THE NEXT WORD**\n",
        "<br />\n",
        "\n",
        "**3.4.1** - Read the dataset `pp_text.csv`. Add the start and end tokens to each line and tokenize it. Convert each sentence to a sequence vector and post-pad to a length of 30. This will be the input for the prediction phase.\n",
        "<br /><br />\n",
        "\n",
        "**3.4.2** - For predicting the next word, use the trained RNN model from above. \n",
        "\n",
        "NOTE - Based on your implementation, the output of the RNN model might have to be different from that of your trained network. You can make use of Keras function API for this.\n",
        "<br /><br />\n",
        "\n",
        "**3.4.3** - Choose any sentence from the list of Pavlos' texts to predict the next word. Input this to the RNN model built for prediction and print the predicted word. Try this out with multiple sentences.\n",
        "<br /><br />\n",
        "\n",
        "**3.4.4** - Do you notice any pattern in the predicted words? Do they seem approriate to the context of the texts as you understand it? What do you attribute this discrepency to? How can you resolve it?\n",
        "\n",
        "Answer in less than 150 words.\n",
        "<br /><br />\n",
        "\n",
        "### **3.5 [6 points] TRAINING AND PREDICTING WITH A DIFFERENT DATASET**\n",
        "<br />\n",
        "\n",
        "**3.5.1** - Read the dataset `cleaned_sarcasm.csv`. This dataset has been preprocessed for you, all you need to do is tokenize, convert to sequence and pad it, similar to 3.2.1, 3.2.2 and 3.3.1.\n",
        "<br /><br />\n",
        "\n",
        "**3.5.2** - Train your RNN model with this data and plot the train and validation trace plot. This part is similar to 3.3.2, 3.3.3 and 3.3.4.\n",
        "<br /><br />\n",
        "\n",
        "**3.5.3** - Repeat 3.4.1, 3.4.2 and 3.4.3 with the RNN model trained using the new dataset.\n",
        "<br /><br />\n",
        "\n",
        "**3.5.4** - How do the results with the new dataset compare to the previous ones? Why do you think so? \n",
        "\n",
        "Answer in less than 100 words.\n",
        "<br /><br />\n",
        "    \n",
        "### **3.6 [3 points] COMPLETING THE SENTENCE**\n",
        "<br />\n",
        "\n",
        "**3.6.1** Until now we have predicted a single word for a given sentence. However, what if he meant more than one word when he typed in `...`\n",
        "\n",
        "We will now predict multiple words for each input sentence. To do this we will first predict one word, append this word to the input text and then predict one more with the updated input. Continue doing this for 5 words or until the end token `</s>` (whichever comes first). \n",
        "<br /><br />\n",
        "\n",
        "### **3.7 [3 points] HOMEWORK QUIZ**\n",
        "<br />\n",
        "After attempting this part of the homework, answer the questions on edStem. All the questions depend on this part of the homework and you will not be able to answer them without attempting this part.\n",
        "\n",
        "<br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVT4R2UieHv5"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "## **PART 3: Solutions**\n",
        "\n",
        "<br />\n",
        "\n",
        "### **3.1 [2 points] PREPROCESS THE DATASET**\n",
        "<br />    \n",
        "\n",
        "**3.1.1** - Read in the dataset `imdb.csv`. Create a new dataframe by splitting each review into individual sentences. The sentences can be delimited by different characters such as period and question mark (eroteme). Call this column as `text` in the new dataframe.\n",
        "    \n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyFz6C4oeHv5"
      },
      "source": [
        "*If you are using colab, the code in the next cell will help download the dataset directly onto your workspace.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPhpPLCieHv5"
      },
      "source": [
        "# Uncomment if you are using colab\n",
        "# !gdown --id \"1YmxaDY-VhGItJ5uRZKtHoEqpqAR1L2IY\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "358m9yIheHv5"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMRqTd4UeHv5"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.1.2** - Define a function `clean_data` that takes the new dataframe as input and removes all html tags and non-alphabetic characters from the dataframe. Additionally, convert all characters to lower case. Remove all the sentences where the number of words is less than 10 and higher than 30. Finally, add the start token `<s>` and the end token `</s>` to every sentence (row) in the dataframe. Return the processed the dataframe. \n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYUWrmSreHv5"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH5IE0zMeHv6"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "### **3.2 [2 points] TOKENIZE THE DATASET**\n",
        "    \n",
        "<br />\n",
        "\n",
        "**3.2.1** - Tokenize the dataset using `tensorflow.keras.preprocessing.text` with a vocabulary size of 5000. Do **not** add an additional token for unknown words (out of vocabulary words) `<UNK>`.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4HBCSUoeHv6"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aPI-UUNeHv6"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.2.2** - Fit the tokenizer on the dataset and get the sequence representation of each sentence.\n",
        "<br /><br />\n",
        "\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7hbZaEHeHv6"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vjRolcheHv6"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **3.3 [10 points] MODELLING THE DATA**\n",
        "\n",
        "**3.3.1** - The first step is to define the input and output of the model. The input to the model is all the words of the sentences except the last one. \n",
        "The output of the model is all words of the sentences except the first one. Using `tf.keras.preprocessing.sequence.pad_sequences` Post-pad all the sentences of the input and output to a length of 30.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN4ihSWVeHv6"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ac0fUsweHv6"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "    \n",
        "**3.3.2** - Define a simple RNN model that has an embedding layer with an embedding dimension of 300. You can define any number of RNN layers. The output of the RNN model will be a dense layer with size of the vocabulary and softmax activation. This model is that one you will be using to train the network. Use functional API for this to reuse it later on.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_WiwAa7eHv6"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H9LiwlzeHv7"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.3.3** - Train the model with the input and output data formed above and a validation split of 0.2. The number epochs and batch size is left as a choice you have to make.\n",
        "<br /><br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpGhdkjPeHv7"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCGSGbNceHv7"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.3.4** - Plot the train and validation loss. \n",
        "\n",
        "<br /><br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BHWuvQbzeHv7"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlizWM9CeHv7"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **3.4 [9 points] PREDICTING THE NEXT WORD**\n",
        "    \n",
        "<br />\n",
        "    \n",
        "**3.4.1** - Read the dataset `pp_text.csv`. Add the start and end tokens to each line and tokenize it. Convert each sentence to a sequence vector and post-pad to a length of 30. This will be the input for the prediction phase.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EdGafL1eHv7"
      },
      "source": [
        "*If you are using colab, the code in the next cell will help download the dataset directly onto your workspace.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIE1oIaBeHv7"
      },
      "source": [
        "# Uncomment if you are using colab\n",
        "# !gdown --id \"1xeQ4w0iYJimzth0e3t3dQJbeuCsFlxVa\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBGWgunAeHv8"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br5MBr-aeHv8"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "\n",
        "**3.4.2** - For predicting the next word, use the trained RNN model from above. \n",
        "\n",
        "NOTE - Based on your implementation, the output of the RNN model might have to be different from that of your trained network. You can make use of Keras function API for this.\n",
        "<br /><br />\n",
        "\n",
        "\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJFFxCaIeHv8"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyHJgB1MeHv8"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.4.3** - Choose any sentence from the list of Pavlos' texts to predict the next word. Input this to the RNN model built for prediction and print the predicted word. Try this out with multiple sentences.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUDbjY4qeHv8"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaFRIBhDeHv8"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.4.4** - Do you notice any pattern in the predicted words? Do they seem approriate to the context of the texts as you understand it? What do you attribute this discrepency to? How can you resolve it?\n",
        "\n",
        "Answer in less than 150 words.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACjJfO-TeHv8"
      },
      "source": [
        "#### Type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuZ-laWieHv9"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "\n",
        "### **3.5 [6 points] TRAINING AND PREDICTING WITH A DIFFERENT DATASET**\n",
        "<br />\n",
        "    \n",
        "**3.5.1** - Read the dataset `cleaned_sarcasm.csv`. This dataset has been preprocessed for you, all you need to do is tokenize, convert to sequence and pad it, similar to 3.2.1, 3.2.2 and 3.3.1.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t90ibxTueHv9"
      },
      "source": [
        "*If you are using colab, the code in the next cell will help download the dataset directly onto your workspace.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj5peWaMeHv9"
      },
      "source": [
        "# Uncomment if you are using colab\n",
        "# !gdown --id \"1pMUuhoKsZVnktosQRuqAutfd3J4sFUsa\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaLH26CieHv9"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTKc4PHkeHv9"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.5.2** - Train your RNN model with this data and plot the train and validation trace plot. This part is similar to 3.3.2, 3.3.3 and 3.3.4.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MItF0zFKeHv9"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Lp_yRleHv9"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.5.3** - Repeat 3.4.1, 3.4.2 and 3.4.3 with the RNN model trained using the new dataset.\n",
        "<br /><br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NINDskVeHv9"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H29yqyw8eHv-"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**3.5.4** - How do the results with the new dataset compare to the previous ones? Why do you think so? \n",
        "\n",
        "Answer in less than 100 words.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3ERRa-1eHv-"
      },
      "source": [
        "#### Type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5xjXvFfeHv-"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "\n",
        "### **3.6 [3 points] COMPLETING THE SENTENCE**\n",
        "<br />\n",
        "\n",
        "**3.6.1** Until now we have predicted a single word for a given sentence. However, what if he meant more than one word when he typed in `...`\n",
        "\n",
        "We will now predict multiple words for each input sentence. To do this we will first predict one word, append this word to the input text and then predict one more with the updated input. Continue doing this for 5 words or until the end token `</s>` (whichever comes first). \n",
        "<br /><br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gte6ERQ4eHv-"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfCgZwbMeHv-"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **3.7 [3 points] HOMEWORK QUIZ**\n",
        "<br />\n",
        "After attempting this part of the homework, answer the questions on edStem. All the questions depend on this part of the homework and you will not be able to answer them without attempting this part.\n",
        "<br /><br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSMxkkAceHv-"
      },
      "source": [
        "#### Answer the questions on edStem"
      ]
    }
  ]
}